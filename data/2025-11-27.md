<div id=toc></div>

# Table of Contents

- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [1] [AI4X Roadmap: Artificial Intelligence for the advancement of scientific pursuit and its future directions](https://arxiv.org/abs/2511.20976)
*Stephen G. Dale, Nikita Kazeev, Alastair J. A. Price, Victor Posligua, Stephan Roche, O. Anatole von Lilienfeld, Konstantin S. Novoselov, Xavier Bresson, Gianmarco Mengaldo, Xudong Chen, Terence J. O'Kane, Emily R. Lines, Matthew J. Allen, Amandine E. Debus, Clayton Miller, Jiayu Zhou, Hiroko H. Dodge, David Rousseau, Andrey Ustyuzhanin, Ziyun Yan, Mario Lanza, Fabio Sciarrino, Ryo Yoshida, Zhidong Leong, Teck Leong Tan, Qianxiao Li, Adil Kabylda, Igor Poltavsky, Alexandre Tkatchenko, Sherif Abdulkader Tawfik, Prathami Divakar Kamath, Theo Jaffrelot Inizan, Kristin A. Persson, Bryant Y. Li, Vir Karan, Chenru Duan, Haojun Jia, Qiyuan Zhao, Hiroyuki Hayashi, Atsuto Seko, Isao Tanaka, Omar M. Yaghi, Tim Gould, Bun Chan, Stefan Vuckovic, Tianbo Li, Min Lin, Zehcen Tang, Yang Li, Yong Xu, Amrita Joshi, Xiaonan Wang, Leonard W. T. Ng, Sergei V. Kalinin, Mahshid Ahmadi, Jiyizhe Zhang, Shuyuan Zhang, Alexei Lapkin, Ming Xiao, Zhe Wu, Kedar Hippalgaonkar, Limsoon Wong, Lorenzo Bastonero, Nicola Marzari, Dorye Luis Esteras Cordoba, Andrei Tomut, Alba Quinones Andrade, Jose-Hugo Garcia*

Main category: physics.soc-ph

TL;DR: 本文概述了人工智能（AI）在科学探索中的应用现状和未来发展方向，旨在推动AI与科学研究的深度融合，加速科学发现和创新。


<details>
  <summary>Details</summary>
Motivation: 科学研究面临着数据量爆炸式增长、复杂性不断提高的挑战，传统方法难以有效应对。人工智能技术在数据分析、模型构建、实验设计等方面的潜力巨大，有望显著提升科学研究的效率和质量。

Method: 本文通过广泛的文献调研和专家访谈，总结了AI在不同科学领域的应用案例，分析了当前面临的挑战，并提出了未来发展路线图，涵盖了机器学习、深度学习、强化学习等多种AI技术。

Result: 文章识别出AI在材料科学、化学、物理学、生物学等多个领域已经取得显著进展，并预测了AI在自动化科学发现、加速新材料研发、优化实验设计等方面的应用前景。

Conclusion: 人工智能将成为推动科学进步的关键力量，未来需要加强跨学科合作，解决AI在科学研究中应用面临的伦理、数据和算法挑战，从而实现AI与科学的深度融合，加速科学发现和创新。

Abstract: Artificial intelligence and machine learning are reshaping how we approach scientific discovery, not by replacing established methods but by extending what researchers can probe, predict, and design. In this roadmap we provide a forward-looking view of AI-enabled science across biology, chemistry, climate science, mathematics, materials science, physics, self-driving laboratories and unconventional computing. Several shared themes emerge: the need for diverse and trustworthy data, transferable electronic-structure and interatomic models, AI systems integrated into end-to-end scientific workflows that connect simulations to experiments and generative systems grounded in synthesisability rather than purely idealised phases. Across domains, we highlight how large foundation models, active learning and self-driving laboratories can close loops between prediction and validation while maintaining reproducibility and physical interpretability. Taken together, these perspectives outline where AI-enabled science stands today, identify bottlenecks in data, methods and infrastructure, and chart concrete directions for building AI systems that are not only more powerful but also more transparent and capable of accelerating discovery in complex real-world environments.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [2] [Differentiable Physics-Neural Models enable Learning of Non-Markovian Closures for Accelerated Coarse-Grained Physics Simulations](https://arxiv.org/abs/2511.21369)
*Tingkai Xue, Chin Chun Ooi, Zhengwei Ge, Fong Yew Leong, Hongying Li, Chang Wei Kang*

Main category: physics.comp-ph

TL;DR: 本文提出了一种可微分的物理-神经网络模型，通过联合学习物理模型参数化和非马尔可夫神经网络闭包模型，实现了对复杂物理现象的快速、准确和可泛化的粗粒化替代。


<details>
  <summary>Details</summary>
Motivation: 传统的数值模拟计算成本高昂，而粗粒化模拟是降低计算成本的常用方法。现有方法通常依赖于马尔可夫闭包项，忽略了历史依赖效应，且物理参数固定，难以实现准确和泛化的预测。

Method: 该模型采用混合物理-神经网络替代模型，通过端到端可微分框架，联合学习物理模型参数化（如各向同性扩散率）和非马尔可夫神经网络闭包模型，以捕捉未解析的粗粒化效应。该模型将3D问题简化为2D平面，并学习隐式地参数化领域几何形状。

Result: 该模型在标量输运模拟中，速度比3D模拟快几个数量级（从数小时到不到1分钟），仅用26个训练数据即可学习，并且在分布外场景（如移动源）中表现良好，Spearman相关系数达到0.96。

Conclusion: 该可微分的物理-神经网络框架为物理现象的快速、准确和可泛化的粗粒化替代提供了新的途径，有望加速复杂物理问题的研究和应用。

Abstract: Numerical simulations provide key insights into many physical, real-world problems. However, while these simulations are solved on a full 3D domain, most analysis only require a reduced set of metrics (e.g. plane-level concentrations). This work presents a hybrid physics-neural model that predicts scalar transport in a complex domain orders of magnitude faster than the 3D simulation (from hours to less than 1 min). This end-to-end differentiable framework jointly learns the physical model parameterization (i.e. orthotropic diffusivity) and a non-Markovian neural closure model to capture unresolved, 'coarse-grained' effects, thereby enabling stable, long time horizon rollouts. This proposed model is data-efficient (learning with 26 training data), and can be flexibly extended to an out-of-distribution scenario (with a moving source), achieving a Spearman correlation coefficient of 0.96 at the final simulation time. Overall results show that this differentiable physics-neural framework enables fast, accurate, and generalizable coarse-grained surrogates for physical phenomena.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [Physics Steering: Causal Control of Cross-Domain Concepts in a Physics Foundation Model](https://arxiv.org/abs/2511.20798)
*Rio Alexa Fear, Payel Mukhopadhyay, Michael McCabe, Alberto Bietti, Miles Cranmer*

Main category: cs.LG

TL;DR: 本研究探索了物理基础模型（如Walrus）是否像大型语言模型一样，能够学习到可解释的物理概念表示，并证明可以通过操纵这些表示来控制模型的预测。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型的可解释性研究取得了进展，但针对科学数据的物理基础模型内部表示的探索仍然不足。本研究旨在回答物理模型是否学习了与基本物理定律和原理相符的可解释表示，还是仅仅依赖于数据中的表面相关性和模式。

Method: 研究人员从Walrus模型中提取激活向量，计算不同物理条件之间的“delta”表示（概念方向），并将这些概念方向注入模型推理过程中，实现激活引导，从而评估其因果影响。

Result: 研究表明，沿着这些概念方向进行干预可以以可解释的方式引导模型预测，并且这些概念特征在不相关的物理系统中具有可传递性，表明神经网络学习了可传递的物理原理。

Conclusion: 该研究为理解和控制科学基础模型开辟了新的途径，并对AI辅助科学发现具有重要意义，证明了科学基础模型能够学习到通用的物理原理表示，而非仅仅依赖于模拟中的表面模式。

Abstract: Recent advances in mechanistic interpretability have revealed that large language models (LLMs) develop internal representations corresponding not only to concrete entities but also distinct, human-understandable abstract concepts and behaviour. Moreover, these hidden features can be directly manipulated to steer model behaviour. However, it remains an open question whether this phenomenon is unique to models trained on inherently structured data (ie. language, images) or if it is a general property of foundation models. In this work, we investigate the internal representations of a large physics-focused foundation model. Inspired by recent work identifying single directions in activation space for complex behaviours in LLMs, we extract activation vectors from the model during forward passes over simulation datasets for different physical regimes. We then compute "delta" representations between the two regimes. These delta tensors act as concept directions in activation space, encoding specific physical features. By injecting these concept directions back into the model during inference, we can steer its predictions, demonstrating causal control over physical behaviours, such as inducing or removing some particular physical feature from a simulation. These results suggest that scientific foundation models learn generalised representations of physical principles. They do not merely rely on superficial correlations and patterns in the simulations. Our findings open new avenues for understanding and controlling scientific foundation models and has implications for AI-enabled scientific discovery.

</details>
