{"id": "2601.10453", "title": "Stable Differentiable Modal Synthesis for Learning Nonlinear Dynamics", "authors": ["Victor Zheleznov", "Stefan Bilbao", "Alec Wright", "Simon King"], "summary": "Modal methods are a long-standing approach to physical modelling synthesis. Extensions to nonlinear problems are possible, including the case of a high-amplitude vibration of a string. A modal decomposition leads to a densely coupled nonlinear system of ordinary differential equations. Recent work in scalar auxiliary variable techniques has enabled construction of explicit and stable numerical solvers for such classes of nonlinear systems. On the other hand, machine learning approaches (in particular neural ordinary differential equations) have been successful in modelling nonlinear systems automatically from data. In this work, we examine how scalar auxiliary variable techniques can be combined with neural ordinary differential equations to yield a stable differentiable model capable of learning nonlinear dynamics. The proposed approach leverages the analytical solution for linear vibration of system's modes so that physical parameters of a system remain easily accessible after the training without the need for a parameter encoder in the model architecture. As a proof of concept, we generate synthetic data for the nonlinear transverse vibration of a string and show that the model can be trained to reproduce the nonlinear dynamics of the system. Sound examples are presented.", "published": "2026-01-15", "categories": ["cs.SD", "cs.LG", "eess.AS", "physics.comp-ph"], "pdf_url": "https://arxiv.org/pdf/2601.10453v1", "primary_category": "cs.SD"}
{"id": "2601.10588", "title": "Searching for Quantum Effects in the Brain: A Bell-Type Test for Nonclassical Latent Representations in Autoencoders", "authors": ["I. K. Kominis", "C. Xie", "S. Li", "M. Skotiniotis", "G. P. Tsironis"], "summary": "Whether neural information processing is entirely classical or involves quantum-mechanical elements remains an open question. Here we propose a model-agnostic, information-theoretic test of nonclassicality that bypasses microscopic assumptions and instead probes the structure of neural representations themselves. Using autoencoders as a transparent model system, we introduce a Bell-type consistency test in latent space, and ask whether decoding statistics obtained under multiple readout contexts can be jointly explained by a single positive latent-variable distribution. By shifting the search for quantum-like signatures in neural systems from microscopic dynamics to experimentally testable constraints on information processing, this work opens a new route for probing the fundamental physics of neural computation.", "published": "2026-01-15", "categories": ["quant-ph", "cs.LG", "physics.bio-ph"], "pdf_url": "https://arxiv.org/pdf/2601.10588v1", "primary_category": "quant-ph"}
{"id": "2601.10684", "title": "On the origin of neural scaling laws: from random graphs to natural language", "authors": ["Maissam Barkeshli", "Alberto Alfarano", "Andrey Gromov"], "summary": "Scaling laws have played a major role in the modern AI revolution, providing practitioners predictive power over how the model performance will improve with increasing data, compute, and number of model parameters. This has spurred an intense interest in the origin of neural scaling laws, with a common suggestion being that they arise from power law structure already present in the data. In this paper we study scaling laws for transformers trained to predict random walks (bigrams) on graphs with tunable complexity. We demonstrate that this simplified setting already gives rise to neural scaling laws even in the absence of power law structure in the data correlations. We further consider dialing down the complexity of natural language systematically, by training on sequences sampled from increasingly simplified generative language models, from 4,2,1-layer transformer language models down to language bigrams, revealing a monotonic evolution of the scaling exponents. Our results also include scaling laws obtained from training on random walks on random graphs drawn from Erdös-Renyi and scale-free Barabási-Albert ensembles. Finally, we revisit conventional scaling laws for language modeling, demonstrating that several essential results can be reproduced using 2 layer transformers with context length of 50, provide a critical analysis of various fits used in prior literature, demonstrate an alternative method for obtaining compute optimal curves as compared with current practice in published literature, and provide preliminary evidence that maximal update parameterization may be more parameter efficient than standard parameterization.", "published": "2026-01-15", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.AI", "stat.ML"], "pdf_url": "https://arxiv.org/pdf/2601.10684v1", "primary_category": "cs.LG"}
{"id": "2601.10628", "title": "Parametric RDT approach to computational gap of symmetric binary perceptron", "authors": ["Mihailo Stojnic"], "summary": "We study potential presence of statistical-computational gaps (SCG) in symmetric binary perceptrons (SBP) via a parametric utilization of \\emph{fully lifted random duality theory} (fl-RDT) [96]. A structural change from decreasingly to arbitrarily ordered $c$-sequence (a key fl-RDT parametric component) is observed on the second lifting level and associated with \\emph{satisfiability} ($α_c$) -- \\emph{algorithmic} ($α_a$) constraints density threshold change thereby suggesting a potential existence of a nonzero computational gap $SCG=α_c-α_a$. The second level estimate is shown to match the theoretical $α_c$ whereas the $r\\rightarrow \\infty$ level one is proposed to correspond to $α_a$. For example, for the canonical SBP ($κ=1$ margin) we obtain $α_c\\approx 1.8159$ on the second and $α_a\\approx 1.6021$ (with converging tendency towards $\\sim 1.59$ range) on the seventh level. Our propositions remarkably well concur with recent literature: (i) in [20] local entropy replica approach predicts $α_{LE}\\approx 1.58$ as the onset of clustering defragmentation (presumed driving force behind locally improving algorithms failures); (ii) in $α\\rightarrow 0$ regime we obtain on the third lifting level $κ\\approx 1.2385\\sqrt{\\frac{α_a}{-\\log\\left ( α_a \\right ) }}$ which qualitatively matches overlap gap property (OGP) based predictions of [43] and identically matches local entropy based predictions of [24]; (iii) $c$-sequence ordering change phenomenology mirrors the one observed in asymmetric binary perceptron (ABP) in [98] and the negative Hopfield model in [100]; and (iv) as in [98,100], we here design a CLuP based algorithm whose practical performance closely matches proposed theoretical predictions.", "published": "2026-01-15", "categories": ["stat.ML", "cond-mat.dis-nn", "cs.IT", "cs.LG", "math.PR"], "pdf_url": "https://arxiv.org/pdf/2601.10628v1", "primary_category": "stat.ML"}
