<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [cs.LG](#cs.LG) [Total: 9]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 1]
- [hep-lat](#hep-lat) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [stat.ML](#stat.ML) [Total: 3]
- [quant-ph](#quant-ph) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs](https://arxiv.org/abs/2510.01370)
*Abu Bucker Siddik, Diane Oyen, Alexander Most, Michal Kucer, Ayan Biswas*

Main category: cs.CV

TL;DR: 本文提出了SPUS，一种轻量级且参数高效的神经网络基础模型，基于残差U-Net架构，用于求解各种偏微分方程(PDEs)。它在参数效率和泛化能力方面优于现有基于Transformer的模型。


<details>
  <summary>Details</summary>
Motivation: 现有PDE基础模型(FM)通常采用大型Transformer架构，计算和参数开销高。本文旨在探索更轻量级、更高效的架构，并利用大规模PDE数据集进行预训练，以实现更好的泛化能力。

Method: SPUS采用残差U-Net架构，并使用一种简单的自回归预训练策略，该策略模仿数值求解器的行为。模型在流体动力学PDE数据集上进行预训练，并在6个具有挑战性的、未见过的下游PDE任务上进行评估。

Result: 实验结果表明，SPUS在这些下游任务上实现了最先进的泛化性能，同时参数数量显著减少，且需要更少的微调数据。

Conclusion: SPUS证明了轻量级残差U-Net架构作为PDE基础模型的可行性，并为构建参数高效、泛化能力强的PDE求解器提供了新的思路，具有解决各种PDE系统问题的潜力。

Abstract: We introduce Small PDE U-Net Solver (SPUS), a compact and efficient foundation model (FM) designed as a unified neural operator for solving a wide range of partial differential equations (PDEs). Unlike existing state-of-the-art PDE FMs-primarily based on large complex transformer architectures with high computational and parameter overhead-SPUS leverages a lightweight residual U-Net-based architecture that has been largely underexplored as a foundation model architecture in this domain. To enable effective learning in this minimalist framework, we utilize a simple yet powerful auto-regressive pretraining strategy which closely replicates the behavior of numerical solvers to learn the underlying physics. SPUS is pretrained on a diverse set of fluid dynamics PDEs and evaluated across 6 challenging unseen downstream PDEs spanning various physical systems. Experimental results demonstrate that SPUS using residual U-Net based architecture achieves state-of-the-art generalization on these downstream tasks while requiring significantly fewer parameters and minimal fine-tuning data, highlighting its potential as a highly parameter-efficient FM for solving diverse PDE systems.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [2] [GeoGraph: Geometric and Graph-based Ensemble Descriptors for Intrinsically Disordered Proteins](https://arxiv.org/abs/2510.00774)
*Eoin Quinn, Marco Carobene, Jean Quentin, Sebastien Boyer, Miguel Arbesú, Oliver Bent*

Main category: q-bio.BM

TL;DR: 本文提出了一种名为GeoGraph的新方法，通过将粗粒化分子动力学模拟转化为图描述符，直接从序列预测内源性无序蛋白(IDPs)的残基间接触图的集成统计信息，从而避免了显式建模整个蛋白构象集合的计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的基于蛋白质语言模型(PLMs)的方法缺乏物理基础，而训练用于建模完整集合的生成模型计算成本高昂。本文旨在解决IDPs的构象集合建模难题，寻找一种更有效的方法。

Method: GeoGraph将粗粒化分子动力学模拟转化为残基和序列级别的图描述符，并利用这些描述符训练一个模拟信息代理，直接从序列预测残基间接触图的集成统计信息。

Result: 实验表明，GeoGraph方法产生的表征比现有方法更能预测关键的生物物理性质。

Conclusion: GeoGraph提供了一种更有效且信息丰富的IDPs建模方法，通过关注集成统计信息而非完整构象集合，避免了高维度的计算负担，并能更好地捕捉生物物理信号。

Abstract: While deep learning has revolutionized the prediction of rigid protein structures, modelling the conformational ensembles of Intrinsically Disordered Proteins (IDPs) remains a key frontier. Current AI paradigms present a trade-off: Protein Language Models (PLMs) capture evolutionary statistics but lack explicit physical grounding, while generative models trained to model full ensembles are computationally expensive. In this work we critically assess these limits and propose a path forward. We introduce GeoGraph, a simulation-informed surrogate trained to predict ensemble-averaged statistics of residue-residue contact-map topology directly from sequence. By featurizing coarse-grained molecular dynamics simulations into residue- and sequence-level graph descriptors, we create a robust and information-rich learning target. Our evaluation demonstrates that this approach yields representations that are more predictive of key biophysical properties than existing methods.

</details>


### [3] [BioBlobs: Differentiable Graph Partitioning for Protein Representation Learning](https://arxiv.org/abs/2510.01632)
*Xin Wang, Carlos Oliver*

Main category: q-bio.BM

TL;DR: BIOBLOBS是一种可微分的图划分模块，用于学习蛋白质表示。它通过动态地将蛋白质结构划分为灵活大小的子结构（blobs）来克服现有蛋白质表示学习模型中依赖于刚性子结构的局限性，从而提高预测性能并提供对蛋白质功能的机制洞察。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质表示学习(PRL)模型通常依赖于固定大小的子结构（如k-hop邻域），这可能导致功能性组装的破碎和表示扭曲，因为蛋白质功能是由不同大小和拓扑的连贯子结构驱动的。

Method: BIOBLOBS将蛋白质原子化问题建模为图划分任务。它使用可微分的图划分算法动态地将蛋白质结构划分为非重叠的子结构（blobs），并将这些blobs量化为共享且可解释的代码本，从而生成用于计算蛋白质嵌入的函数相关蛋白质子结构的离散词汇。

Result: 实验结果表明，BIOBLOBS的表示提高了广泛使用的蛋白质编码器（如GVP-GNN）在各种PRL任务中的性能。

Conclusion: BIOBLOBS证明了直接捕获功能相关蛋白质子结构的重要性，不仅提高了预测性能，还为蛋白质功能提供了机制洞察。该方法为蛋白质表示学习提供了一种新的思路，即通过动态划分蛋白质结构来更好地理解其功能。

Abstract: Protein function is driven by coherent substructures which vary in size and topology, yet current protein representation learning models (PRL) distort these signals by relying on rigid substructures such as k-hop and fixed radius neighbourhoods. We introduce BioBlobs, a plug-and-play, fully differentiable module that represents proteins by dynamically partitioning structures into flexibly-sized, non-overlapping substructures ("blobs"). The resulting blobs are quantized into a shared and interpretable codebook, yielding a discrete vocabulary of function-relevant protein substructures used to compute protein embeddings. We show that BioBlobs representations improve the performance of widely used protein encoders such as GVP-GNN across various PRL tasks. Our approach highlights the value of architectures that directly capture function-relevant protein substructures, enabling both improved predictive performance and mechanistic insight into protein function.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [4] [Neural Network Surrogates for Free Energy Computation of Complex Chemical Systems](https://arxiv.org/abs/2510.01396)
*Wasut Pornpatcharapong*

Main category: cs.LG

TL;DR: 本文提出了一种基于神经网络的代理框架，利用自动微分计算自由能计算所需的雅可比矩阵，从而绕过了对雅可比矩阵进行显式解析计算的瓶颈，并成功应用于复杂化学系统。


<details>
  <summary>Details</summary>
Motivation: 传统的自由能计算方法需要雅可比矩阵，而对于复杂的或机器学习定义的集体变量（CVs），解析计算雅可比矩阵变得不可行，这限制了这些方法的应用范围。

Method: 该研究使用神经网络作为代理，通过自动微分计算CVs的雅可比矩阵，从而避免了显式计算雅可比矩阵。该框架在MgCl2离子配对系统上进行了测试，并与高斯过程回归（GPR）管道结合使用。

Result: 实验结果表明，该方法在简单距离CV和复杂配位数CV的计算中都具有高精度，并且雅可比误差也遵循近高斯分布，使其适用于GPR流程。

Conclusion: 该框架为将复杂和机器学习的CVs纳入基于GPR的自由能计算中开辟了道路，从而扩展了生物化学和材料模拟的应用范围。

Abstract: Free energy reconstruction methods such as Gaussian Process Regression (GPR) require Jacobians of the collective variables (CVs), a bottleneck that restricts the use of complex or machine-learned CVs. We introduce a neural network surrogate framework that learns CVs directly from Cartesian coordinates and uses automatic differentiation to provide Jacobians, bypassing analytical forms. On an MgCl2 ion-pairing system, our method achieved high accuracy for both a simple distance CV and a complex coordination-number CV. Moreover, Jacobian errors also followed a near-Gaussian distribution, making them suitable for GPR pipelines. This framework enables gradient-based free energy methods to incorporate complex and machine-learned CVs, broadening the scope of biochemistry and materials simulations.

</details>


### [5] [Physics-Informed Extreme Learning Machine (PIELM) for Tunnelling-Induced Soil-Pile Interactions](https://arxiv.org/abs/2510.00698)
*Fu-Chen Guo, Pei-Zhi Zhuang, Fei Ren, Hong-Ya Yue, He Yang*

Main category: cs.LG

TL;DR: 本文提出了一种基于物理信息的极限学习机 (PIELM) 框架，用于分析隧道诱发土-桩相互作用，结合物理方程和实验数据，实现快速训练和高精度预测。


<details>
  <summary>Details</summary>
Motivation: 城市地下建设中，隧道施工经常会影响附近的桩基，理解和监测隧道诱发土-桩相互作用对于风险管理至关重要。现有方法如试验、理论分析和数值模拟都存在局限性，本文旨在结合物理信息和数据驱动方法，提高预测精度和效率。

Method: 该研究将桩基建模为Euler-Bernoulli梁，周围土体建模为Pasternak地基，将土-桩相互作用表述为四阶常微分方程（ODE），作为物理信息部分。同时，将测量数据融入PIELM作为数据驱动部分，利用最小二乘法在1秒内训练ELM网络。通过边界元法(BEM)和有限差分法(FDM)验证PIELM方法，并进行参数化研究。

Result: 研究结果表明，监测数据应放置在桩挠度梯度较大的位置，如桩尖/顶和隧道区域。PIELM方法在两个应用案例中表现出良好的性能，能够有效分析隧道诱发土-桩相互作用。

Conclusion: 本文提出的PIELM方法具有实时监测和安全评估桩基的潜力，为岩土工程中的智能预警系统提供支持，结合物理信息和数据驱动方法在隧道诱发土-桩相互作用分析中具有重要意义。

Abstract: Physics-informed machine learning has been a promising data-driven and physics-informed approach in geotechnical engineering. This study proposes a physics-informed extreme learning machine (PIELM) framework for analyzing tunneling-induced soil-pile interactions. The pile foundation is modeled as an Euler-Bernoulli beam, and the surrounding soil is modeled as a Pasternak foundation. The soil-pile interaction is formulated into a fourth-order ordinary differential equation (ODE) that constitutes the physics-informed component, while measured data are incorporated into PIELM as the data-driven component. Combining physics and data yields a loss vector of the extreme learning machine (ELM) network, which is trained within 1 second by the least squares method. After validating the PIELM approach by the boundary element method (BEM) and finite difference method (FDM), parametric studies are carried out to examine the effects of ELM network architecture, data monitoring locations and numbers on the performance of PIELM. The results indicate that monitored data should be placed at positions where the gradients of pile deflections are significant, such as at the pile tip/top and near tunneling zones. Two application examples highlight the critical role of physics-informed and data-driven approach for tunnelling-induced soil-pile interactions. The proposed approach shows great potential for real-time monitoring and safety assessment of pile foundations, and benefits for intelligent early-warning systems in geotechnical engineering.

</details>


### [6] [BigBang-Proton Technical Report: Next-Word-Prediction is Scientific Multitask Learner](https://arxiv.org/abs/2510.00129)
*Hengkui Wu, Liujiang Liu, Jihua He, Qihao Wang, Keke Zhao, Shuyang Hu, Renle Fu, Dahao Liang, Lingyu Zeng, Bruce Liu, Yuan Liu, Jin Zhan, Jiaqiang Niu, Xinglong Jia, Yaqin Hu, Wenjun Ji, Panpan Chi, Ken Chen, Hengyuan Wu, Yingsi Xin, Yongfeng Zhu, Yuexin Wang, Manqi Ruan, Ningtao Bian, Xiaohua Wu, Weipeng Xu*

Main category: cs.LG

TL;DR: 本文提出了BigBang-Proton，一种统一的基于序列的架构，通过在跨尺度、跨结构、跨学科的科学任务上进行预训练，构建了科学多任务学习模型，并在多个科学领域取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）在解决实际科学挑战和产生新的科学见解方面表现有限，缺乏能够超越传统prompt-answer范式的能力。本文旨在探索语言引导的科学计算，并证明其能够与或超过特定任务的科学模型性能。

Method: BigBang-Proton采用了三种创新：理论-实验学习范式（将数值实验数据与理论文本结合）、二元补丁编码（替代BPE）、蒙特卡洛注意力（替代传统Transformer架构）。通过在包含通用文本语料的跨学科科学数据集上进行下一词预测预训练，然后对下游任务进行微调和推理。

Result: BigBang-Proton在50位数字加法运算中达到100%准确率，在粒子物理喷射标记中与领先的专业模型相当，在原子间势能模拟中达到与专业模型相当的MAE，在水质预测中与传统时空模型相当，并在基因建模方面超越了生物基础模型。

Conclusion: 研究结果表明，语言引导的科学计算可以与或超过特定任务的科学模型的性能，同时保持多任务学习能力，并提出了将预训练扩展到宇宙尺度的假设，作为开发材料世界基础模型的重要一步。

Abstract: We introduce BigBang-Proton, a unified sequence-based architecture for auto-regressive language modeling pretrained on cross-scale, cross-structure, cross-discipline real-world scientific tasks to construct a scientific multi-task learner. BigBang-Proton incorporates three fundamental innovations compared to mainstream general-purpose LLMs: Theory-Experiment Learning paradigm aligns large-scale numerical experimental data with theoretical text corpora; Binary Patch Encoding replaces byte pair encoding(BPE) tokenization; Monte Carlo Attention substitutes traditional transformer architectures. Through next-word-prediction pretraining on cross-discipline scientific datasets of real-world problems mixed with general textual corpus, followed by fine-tuning and inference on downstream tasks, BigBang-Proton demonstrates 100\% accuracy in up to 50-digit arithmetic addition operations, performance on par with leading specialized models in particle physics jet tagging, matching MAE of specialized models in inter-atomic potential simulation, performance comparable to traditional spatiotemporal models in water quality prediction, and benchmark-exceeding performance in genome modeling. These results prove that language-guided scientific computing can match or exceed the performance of task-specific scientific models while maintaining multitask learning capabilities. We further hypothesize to scale the pretraining to the universe scale as a fundamental step toward developing material world foundational model.

</details>


### [7] [Transformers Discover Molecular Structure Without Graph Priors](https://arxiv.org/abs/2510.02259)
*Tobias Kreiman, Yutong Bai, Fadi Atieh, Elizabeth Weaver, Eric Qu, Aditi S. Krishnapriyan*

Main category: cs.LG

TL;DR: 本文研究了在没有预定义图结构或物理先验的情况下，纯粹的Transformer模型是否能够近似分子能量和力。实验结果表明，Transformer能够学习到物理上一致的模式，并具有可预测的扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统的分子机器学习方法依赖于图神经网络（GNN），但GNN存在表达能力受限、推理速度慢以及难以扩展等问题。本文旨在探索Transformer模型在分子建模中的潜力，挑战硬编码图结构诱导偏差的必要性。

Method: 研究人员直接在笛卡尔坐标上训练未修改的Transformer模型，无需预定义的图或物理先验。他们使用OMol25数据集作为起点，将Transformer与最先进的等变GNN进行比较，评估其在能量和力预测方面的性能。

Result: Transformer模型能够达到与最先进的GNN相当的能量和力均方误差。它学习到物理上一致的模式，例如注意力权重与原子间距离成反比衰减，并且能够灵活适应不同的分子环境。此外，Transformer的训练表现出与其它领域一致的经验扩展规律。

Conclusion: 研究表明，Transformer模型可以自适应地产生许多GNN的有利特性，挑战了硬编码图结构诱导偏差的必要性，并为分子建模提供了标准化、可扩展的架构。

Abstract: Graph Neural Networks (GNNs) are the dominant architecture for molecular machine learning, particularly for molecular property prediction and machine learning interatomic potentials (MLIPs). GNNs perform message passing on predefined graphs often induced by a fixed radius cutoff or k-nearest neighbor scheme. While this design aligns with the locality present in many molecular tasks, a hard-coded graph can limit expressivity due to the fixed receptive field and slows down inference with sparse graph operations. In this work, we investigate whether pure, unmodified Transformers trained directly on Cartesian coordinates$\unicode{x2013}$without predefined graphs or physical priors$\unicode{x2013}$can approximate molecular energies and forces. As a starting point for our analysis, we demonstrate how to train a Transformer to competitive energy and force mean absolute errors under a matched training compute budget, relative to a state-of-the-art equivariant GNN on the OMol25 dataset. We discover that the Transformer learns physically consistent patterns$\unicode{x2013}$such as attention weights that decay inversely with interatomic distance$\unicode{x2013}$and flexibly adapts them across different molecular environments due to the absence of hard-coded biases. The use of a standard Transformer also unlocks predictable improvements with respect to scaling training resources, consistent with empirical scaling laws observed in other domains. Our results demonstrate that many favorable properties of GNNs can emerge adaptively in Transformers, challenging the necessity of hard-coded graph inductive biases and pointing toward standardized, scalable architectures for molecular modeling.

</details>


### [8] [Inferring Optical Tissue Properties from Photoplethysmography using Hybrid Amortized Inference](https://arxiv.org/abs/2510.02073)
*Jens Behrmann, Maria R. Cervera, Antoine Wehenkel, Andrew C. Miller, Albert Cerussi, Pranay Jain, Vivek Venugopal, Shijie Yan, Guillermo Sapiro, Luca Pegolotti, Jörn-Henrik Jacobsen*

Main category: cs.LG

TL;DR: 本文提出了一种名为PPGen的生物物理模型，结合混合泛化推断（HAI）方法，能够从PPG信号中快速、稳健地估计生理参数，并校正模型偏差，从而在保持预测能力的同时提供临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在PPG信号分析中虽然具有强大的预测能力，但往往依赖于缺乏生理意义的特征，阻碍了临床应用和下一代PPG传感器设计。本文旨在解决这一问题，将基于第一原理的PPG模型与深度生成模型相结合。

Method: 本文提出了PPGen，一个明确将波形特征与生理参数和传感器架构联系起来的PPG脉冲生成器，并基于PPGen提出了混合泛化推断（HAI）算法，用于快速、稳健且可扩展地进行参数推断，同时减轻模型偏差的影响。

Result: 在大量的模拟实验中，HAI能够准确地估计各种噪声和传感器条件下的生理参数，证明了该方法在PPG模型中保持保真度并支持临床解释和硬件设计方面的潜力。

Conclusion: 本文的研究为开发能够兼顾预测能力、临床可解释性和硬件设计的PPG模型开辟了道路，有助于推动可穿戴设备在个人健康监测中的应用。

Abstract: Smart wearables enable continuous tracking of established biomarkers such as heart rate, heart rate variability, and blood oxygen saturation via photoplethysmography (PPG). Beyond these metrics, PPG waveforms contain richer physiological information, as recent deep learning (DL) studies demonstrate. However, DL models often rely on features with unclear physiological meaning, creating a tension between predictive power, clinical interpretability, and sensor design. We address this gap by introducing PPGen, a biophysical model that relates PPG signals to interpretable physiological and optical parameters. Building on PPGen, we propose hybrid amortized inference (HAI), enabling fast, robust, and scalable estimation of relevant physiological parameters from PPG signals while correcting for model misspecification. In extensive in-silico experiments, we show that HAI can accurately infer physiological parameters under diverse noise and sensor conditions. Our results illustrate a path toward PPG models that retain the fidelity needed for DL-based features while supporting clinical interpretation and informed hardware design.

</details>


### [9] [From Supervision to Exploration: What Does Protein Language Model Learn During Reinforcement Learning?](https://arxiv.org/abs/2510.01571)
*Hanqun Cao, Hongrui Zhang, Junde Xu, Zhou Zhang, Lingdong Shen, Minghao Sun, Ge Liu, Jinbo Xu, Wu-Jun Li, Jinren Ni, Cesar de la Fuente-Nunez, Tianfan Fu, Yejin Choi, Pheng-Ann Heng, Fang Wu*

Main category: cs.LG

TL;DR: 本研究探讨了强化学习（RL）如何提升蛋白质语言模型（PLM）的能力，使其超越预训练数据的限制，发现蛋白质序列空间中潜在的功能模式。研究表明，RL的有效性受到任务难度、奖励模型准确性和策略容量的共同影响。


<details>
  <summary>Details</summary>
Motivation: 传统的监督学习方法在优化复杂生物学目标、探索新颖功能区域以及整合多目标标准方面存在局限性。强化学习有望解决这些问题，并推动蛋白质设计的创新。

Method: 研究人员系统性地评估了在四个关键蛋白质设计领域（抗菌肽设计、激酶优化、抗体工程和逆向折叠）中使用多种RL算法和模型架构，以考察RL增强的PLM是否能够超越预训练的限制。

Result: 研究表明，RL能够可靠地提高各个领域的采样效率，并且其有效性受到任务难度、奖励模型准确性和策略容量三者相互作用的影响。当奖励准确且信息丰富，策略容量充足，且任务存在于监督学习之外时，收益会增加。

Conclusion: 该研究为蛋白质设计中的RL提供了实践指导：优先优化奖励模型，匹配RL算法和正则化强度以适应任务难度，并在边际收益最大的地方分配容量。这有助于更好地利用RL来发现蛋白质序列空间中的新颖功能模式。

Abstract: Protein language models (PLMs) have advanced computational protein science through large-scale pretraining and scalable architectures. In parallel, reinforcement learning (RL) has broadened exploration and enabled precise multi-objective optimization in protein design. Yet whether RL can push PLMs beyond their pretraining priors to uncover latent sequence-structure-function rules remains unclear. We address this by pairing RL with PLMs across four domains: antimicrobial peptide design, kinase variant optimization, antibody engineering, and inverse folding. Using diverse RL algorithms and model classes, we ask if RL improves sampling efficiency and, more importantly, if it reveals capabilities not captured by supervised learning. Across benchmarks, RL consistently boosts success rates and sample efficiency. Performance follows a three-factor interaction: task headroom, reward fidelity, and policy capacity jointly determine gains. When rewards are accurate and informative, policies have sufficient capacity, and tasks leave room beyond supervised baselines, improvements scale; when rewards are noisy or capacity is constrained, gains saturate despite exploration. This view yields practical guidance for RL in protein design: prioritize reward modeling and calibration before scaling policy size, match algorithm and regularization strength to task difficulty, and allocate capacity where marginal gains are largest. Implementation is available at https://github.com/chq1155/RL-PLM.

</details>


### [10] [AReUReDi: Annealed Rectified Updates for Refining Discrete Flows with Multi-Objective Guidance](https://arxiv.org/abs/2510.00352)
*Tong Chen, Yinuo Zhang, Pranam Chatterjee*

Main category: cs.LG

TL;DR: AReUReDi是一种新的离散优化算法，通过结合退火、修正和局部平衡采样，实现了对多目标生物分子序列设计的帕累托最优引导，并在肽和SMILES序列设计中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有生成框架主要在连续空间中进行单目标引导，而离散方法缺乏多目标帕累托最优的保证。生物分子工程本质上是多目标优化问题，需要同时优化多个相互冲突的目标，例如亲和力、溶解度和毒性。

Method: AReUReDi (Annealed Rectified Updates for Refining Discrete Flows) 构建在 Rectified Discrete Flows (ReDi) 的基础上，结合了 Tchebycheff 标量化、局部平衡采样和退火 Metropolis-Hastings 更新，以偏置采样到帕累托最优状态，同时保持分布不变性。它通过迭代细化源分布和目标分布之间的耦合来减少因子化误差。

Result: AReUReDi 在肽和 SMILES 序列设计中，同时优化了高达五个治疗特性（包括亲和力、溶解度、溶血性、半衰期和非污染性），并优于进化搜索和扩散方法。

Conclusion: AReUReDi 证明了其作为一种强大的、基于序列的框架，用于多属性生物分子生成，为生物分子工程提供了一种新的方法，能够有效地解决多目标优化问题。

Abstract: Designing sequences that satisfy multiple, often conflicting, objectives is a central challenge in therapeutic and biomolecular engineering. Existing generative frameworks largely operate in continuous spaces with single-objective guidance, while discrete approaches lack guarantees for multi-objective Pareto optimality. We introduce AReUReDi (Annealed Rectified Updates for Refining Discrete Flows), a discrete optimization algorithm with theoretical guarantees of convergence to the Pareto front. Building on Rectified Discrete Flows (ReDi), AReUReDi combines Tchebycheff scalarization, locally balanced proposals, and annealed Metropolis-Hastings updates to bias sampling toward Pareto-optimal states while preserving distributional invariance. Applied to peptide and SMILES sequence design, AReUReDi simultaneously optimizes up to five therapeutic properties (including affinity, solubility, hemolysis, half-life, and non-fouling) and outperforms both evolutionary and diffusion-based baselines. These results establish AReUReDi as a powerful, sequence-based framework for multi-property biomolecule generation.

</details>


### [11] [Flow Autoencoders are Effective Protein Tokenizers](https://arxiv.org/abs/2510.00351)
*Rohit Dilip, Evan Zhang, Ayush Varshney, David Van Valen*

Main category: cs.LG

TL;DR: 本文提出了Kanzi，一种基于流的蛋白质结构Tokenizer，它使用扩散自编码器和流匹配损失进行训练，简化了蛋白质结构Tokenization过程，并实现了在模型尺寸和训练成本更小的情况下优于现有Tokenizers的重建性能。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质结构Tokenization方法依赖于复杂的、难以优化和扩展的定制组件。研究旨在简化Tokenization流程，并提高模型效率。

Method: Kanzi采用扩散自编码器，使用流匹配损失进行训练。该方法将全局坐标代替基于帧的表示，用单一的流匹配损失代替复杂损失，并用标准注意力代替SE(3)-不变注意力。

Result: 实验表明，Kanzi在重建指标上优于现有Tokenizers，同时模型尺寸和训练成本显著降低。基于Kanzi训练的自回归模型在生成任务上优于其他Token操作模型，但尚未达到最先进的连续扩散模型的性能。

Conclusion: Kanzi简化了蛋白质结构Tokenization，提高了模型效率，并为构建多模态生物模型提供了新的可能性。该研究为蛋白质结构建模和生成领域带来了新的思路和方法。

Abstract: Protein structure tokenizers enable the creation of multimodal models of protein structure, sequence, and function. Current approaches to protein structure tokenization rely on bespoke components that are invariant to spatial symmetries, but that are challenging to optimize and scale. We present Kanzi, a flow-based tokenizer for tokenization and generation of protein structures. Kanzi consists of a diffusion autoencoder trained with a flow matching loss. We show that this approach simplifies several aspects of protein structure tokenizers: frame-based representations can be replaced with global coordinates, complex losses are replaced with a single flow matching loss, and SE(3)-invariant attention operations can be replaced with standard attention. We find that these changes stabilize the training of parameter-efficient models that outperform existing tokenizers on reconstruction metrics at a fraction of the model size and training cost. An autoregressive model trained with Kanzi outperforms similar generative models that operate over tokens, although it does not yet match the performance of state-of-the-art continuous diffusion models. Code is available here: https://github.com/rdilip/kanzi/.

</details>


### [12] [Quantum-inspired Benchmark for Estimating Intrinsic Dimension](https://arxiv.org/abs/2510.01335)
*Aritra Das, Joseph T. Iosue, Victor V. Albert*

Main category: cs.LG

TL;DR: 本文提出了QuIIEst基准，这是一组基于量子光学方法的、具有非平凡拓扑结构且维度已知的无限家族的合成数据集，用于评估内在维度估计(IDE)方法的性能。实验结果表明，现有IDE方法在QuIIEst基准上的准确性低于现有基准。


<details>
  <summary>Details</summary>
Motivation: 现有内在维度估计(IDE)方法在实际数据集上的估计结果差异很大，这表明需要更复杂的基准来评估IDE方法的性能，并区分方法本身的偏差与数据集本身的特性。

Method: 研究人员利用量子光学方法构建了QuIIEst基准，该基准包含无限家族的非平凡拓扑结构流形，其内在维度已知。他们使用现有的IDE方法在QuIIEst流形和现有基准上进行测试，并比较了结果。

Result: 在QuIIEst基准上，现有IDE方法的准确性普遍低于在现有基准上的准确性。即使在曲率不均匀的情况下，性能下降也很小，表明QuIIEst基准的挑战性。

Conclusion: QuIIEst基准为评估和改进内在维度估计方法提供了一个新的、更具挑战性的平台。研究还表明，即使对于非流形空间（如Hofstadter蝴蝶），也可以应用IDE方法。

Abstract: Machine learning models can generalize well on real-world datasets. According to the manifold hypothesis, this is possible because datasets lie on a latent manifold with small intrinsic dimension (ID). There exist many methods for ID estimation (IDE), but their estimates vary substantially. This warrants benchmarking IDE methods on manifolds that are more complex than those in existing benchmarks. We propose a Quantum-Inspired Intrinsic-dimension Estimation (QuIIEst) benchmark consisting of infinite families of topologically non-trivial manifolds with known ID. Our benchmark stems from a quantum-optical method of embedding arbitrary homogeneous spaces while allowing for curvature modification and additive noise. The IDE methods tested were generally less accurate on QuIIEst manifolds than on existing benchmarks under identical resource allocation. We also observe minimal performance degradation with increasingly non-uniform curvature, underscoring the benchmark's inherent difficulty. As a result of independent interest, we perform IDE on the fractal Hofstadter's butterfly and identify which methods are capable of extracting the effective dimension of a space that is not a manifold.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [13] [Electron neural closure for turbulent magnetosheath simulations: energy channels](https://arxiv.org/abs/2510.00282)
*George Miloshevich, Luka Vranckx, Felipe Nathan de Oliveira Lopes, Pietro Dazzi, Giuseppe Arrò, Giovanni Lapenta*

Main category: physics.plasm-ph

TL;DR: 本文提出了一种基于全卷积神经网络 (FCNN) 的非局部五阶矩电子压力张量闭合模型，并将其应用于湍流磁鞘体模拟，显著优于传统闭合模型，提高了对能量通道的理解。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解和预测近地空间碰撞非磁等离子体的行为，特别是湍流和磁重联过程中能量交换和耗散机制，以及对空间天气预报和基础设施保护的需求。

Method: 利用 FCNN 训练了一个代理模型，该模型基于能量守恒的半隐式粒子模拟 (ECsim) 代码，并使用较少粒子/胞数量的模拟数据进行训练，验证其结果对高粒子/胞数量模拟的泛化能力。重点评估了学习到的状态方程的统计特性，特别是压力-应变相互作用。

Result: FCNN 学习的状态方程在重建压力-应变分布和条件平均值方面表现优于传统闭合模型（如 MLP 和双绝热表达式）。虽然小尺度特征（特别是压力张量非对角分量）有所缺失，但随着训练数据的增加，结果显著改善，表明模型具有良好的可扩展性。

Conclusion: 该研究表明，基于 FCNN 的电子压力张量闭合模型在湍流磁鞘体模拟中具有潜力，能够更准确地捕捉能量通道，为理解碰撞非等离子体中的能量交换和耗散机制提供新的工具，并为未来空间任务和空间天气预报提供支持。

Abstract: In this work, we introduce a non-local five-moment electron pressure tensor closure parametrized by a Fully Convolutional Neural Network (FCNN). Electron pressure plays an important role in generalized Ohm's law, competing with electron inertia. This model is used in the development of a surrogate model for a fully kinetic energy-conserving semi-implicit Particle-in-Cell simulation of decaying magnetosheath turbulence. We achieve this by training FCNN on a representative set of simulations with a smaller number of particles per cell and showing that our results generalise to a simulation with a large number of particles per cell. We evaluate the statistical properties of the learned equation of state, with a focus on pressure-strain interaction, which is crucial for understanding energy channels in turbulent plasmas. The resulting equation of state learned via FCNN significantly outperforms local closures, such as those learned by Multi-Layer Perceptron (MLP) or double adiabatic expressions. We report that the overall spatial distribution of pressure-strain and its conditional averages are reconstructed well. However, some small-scale features are missed, especially for the off-diagonal components of the pressure tensor. Nevertheless, the results are substantially improved with more training data, indicating favorable scaling and potential for improvement, which will be addressed in future work.

</details>


<div id='hep-lat'></div>

# hep-lat [[Back]](#toc)

### [14] [Combining complex Langevin dynamics with score-based and energy-based diffusion models](https://arxiv.org/abs/2510.01328)
*Gert Aarts, Diaa E. Habibi, Lingxiao Wang, Kai Zhou*

Main category: hep-lat

TL;DR: 本文研究了如何利用扩散模型学习复杂朗之万动力学过程采样的分布，比较了基于得分和基于能量的扩散模型，为解决具有复数玻尔兹曼权重理论中的符号问题提供了新的思路。


<details>
  <summary>Details</summary>
Motivation: 具有复数玻尔兹曼权重的理论在数值模拟中面临符号问题，复杂朗之万动力学是一种潜在的解决方案，但其采样的分布难以理解。扩散模型作为一种学习分布的生成式AI方法，为理解和控制复杂朗之万动力学过程提供了新的可能性。

Method: 研究人员将基于得分和基于能量的扩散模型应用于学习复杂朗之万动力学过程采样的分布。他们使用一个复数四阶模型进行实验，并比较了两种扩散模型的性能。

Result: 研究表明，扩散模型能够学习复杂朗之万动力学过程采样的分布，为理解和控制该过程提供了新的工具。两种扩散模型在学习分布方面表现出不同的特性。

Conclusion: 本文探索了扩散模型与复杂朗之万动力学之间的联系，为解决具有符号问题的物理理论提供了新的方法，并为生成式AI在物理学中的应用开辟了新的方向。

Abstract: Theories with a sign problem due to a complex action or Boltzmann weight can sometimes be numerically solved using a stochastic process in the complexified configuration space. However, the probability distribution effectively sampled by this complex Langevin process is not known a priori and notoriously hard to understand. In generative AI, diffusion models can learn distributions, or their log derivatives, from data. We explore the ability of diffusion models to learn the distributions sampled by a complex Langevin process, comparing score-based and energy-based diffusion models, and speculate about possible applications.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [15] [Learning from the electronic structure of molecules across the periodic table](https://arxiv.org/abs/2510.00224)
*Manasa Kaniselvan, Benjamin Kurt Miller, Meng Gao, Juno Nam, Daniel S. Levine*

Main category: physics.chem-ph

TL;DR: 本文提出了一种利用分子电子结构信息（哈密顿矩阵）训练机器学习原子间势能（MLIP）的新方法，并发布了大规模高质量的哈密顿矩阵数据集，显著提升了低数据场景下的能量预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLIP模型依赖大量原子结构数据，但未充分利用随之而来的哈密顿矩阵数据。为了解决数据限制和泛化能力问题，需要探索新的数据来源，并提升模型在复杂材料和化学环境下的性能。

Method: 提出了HELM（Hamiltonian-trained Electronic-structure Learning for Molecules）模型，该模型能够预测大规模、高元素多样性和大基集的哈密顿矩阵。同时，引入了“哈密顿预训练”方法，从少量原子结构中提取有意义的原子环境描述符，并将其用于改进低数据场景下的能量预测。

Result: 实验结果表明，利用电子相互作用作为数据源可以有效提升MLIP模型的性能，尤其是在数据量有限的情况下。HELM模型能够处理包含100+原子、58种元素和def2-TZVPD基集的大型分子结构。

Conclusion: 本文的研究表明，电子结构信息是化学空间的重要数据来源，可以用于构建更高效、更具泛化能力的MLIP模型，为材料科学和化学领域的计算模拟提供新的思路。

Abstract: Machine-Learned Interatomic Potentials (MLIPs) require vast amounts of atomic structure data to learn forces and energies, and their performance continues to improve with training set size. Meanwhile, the even greater quantities of accompanying data in the Hamiltonian matrix H behind these datasets has so far gone unused for this purpose. Here, we provide a recipe for integrating the orbital interaction data within H towards training pipelines for atomic-level properties. We first introduce HELM ("Hamiltonian-trained Electronic-structure Learning for Molecules"), a state-of-the-art Hamiltonian prediction model which bridges the gap between Hamiltonian prediction and universal MLIPs by scaling to H of structures with 100+ atoms, high elemental diversity, and large basis sets including diffuse functions. To accompany HELM, we release a curated Hamiltonian matrix dataset, 'OMol_CSH_58k', with unprecedented elemental diversity (58 elements), molecular size (up to 150 atoms), and basis set (def2-TZVPD). Finally, we introduce 'Hamiltonian pretraining' as a method to extract meaningful descriptors of atomic environments even from a limited number atomic structures, and repurpose this shared embedding space to improve performance on energy-prediction in low-data regimes. Our results highlight the use of electronic interactions as a rich and transferable data source for representing chemical space.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [16] [Precise Dynamics of Diagonal Linear Networks: A Unifying Analysis by Dynamical Mean-Field Theory](https://arxiv.org/abs/2510.01930)
*Sota Nishiyama, Masaaki Imaizumi*

Main category: stat.ML

TL;DR: 本文利用动力学平均场理论（DMFT）对对角线性网络（DLNs）的梯度流动力学进行统一分析，揭示了损失收敛速度与泛化能力之间的权衡关系，并系统地重现了先前观察到的现象。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常孤立地研究DLNs中出现的特定现象，例如初始化相关的解和增量学习，缺乏对整体动力学的全面理解。

Method: 本文采用动力学平均场理论（DMFT）推导了一个低维的有效过程，该过程捕捉了高维梯度流动力学的渐近行为。通过分析该有效过程，研究人员深入了解了DLNs的动力学。

Result: 研究结果揭示了DLNs的损失收敛速度与泛化能力之间的权衡关系，并系统地重现了先前观察到的现象，例如初始化对解的影响。

Conclusion: 本文加深了对DLNs的理解，并证明了DMFT方法在分析神经网络高维学习动力学中的有效性。

Abstract: Diagonal linear networks (DLNs) are a tractable model that captures several nontrivial behaviors in neural network training, such as initialization-dependent solutions and incremental learning. These phenomena are typically studied in isolation, leaving the overall dynamics insufficiently understood. In this work, we present a unified analysis of various phenomena in the gradient flow dynamics of DLNs. Using Dynamical Mean-Field Theory (DMFT), we derive a low-dimensional effective process that captures the asymptotic gradient flow dynamics in high dimensions. Analyzing this effective process yields new insights into DLN dynamics, including loss convergence rates and their trade-off with generalization, and systematically reproduces many of the previously observed phenomena. These findings deepen our understanding of DLNs and demonstrate the effectiveness of the DMFT approach in analyzing high-dimensional learning dynamics of neural networks.

</details>


### [17] [A universal compression theory: Lottery ticket hypothesis and superpolynomial scaling laws](https://arxiv.org/abs/2510.00504)
*Hong-Yi Wang, Di Luo, Tomaso Poggio, Isaac L. Chuang, Liu Ziyin*

Main category: stat.ML

TL;DR: 本文提出了一种通用的压缩理论，证明了大型神经网络和数据集可以压缩到多项式对数大小，同时保持学习动态和损失景观不变，从而有望显著提升神经网络的效率。


<details>
  <summary>Details</summary>
Motivation: 当前大型AI模型的训练成本高昂，数据效率远低于生物系统（如人脑）。研究旨在探索如何通过压缩模型和数据集来降低训练成本，并提升数据效率，从而更接近人类水平的数据利用。

Method: 研究团队证明了一个通用的压缩定理，表明几乎任何对称函数都可以被压缩到具有O(polylog(d))个元素的函数，且无损。该定理被应用于网络压缩和数据集压缩，并由此推导出了动态彩票假设（LTH）。

Result: 证明了动态彩票假设，即大型网络可以被压缩而不改变其训练动态；证明了可以通过压缩数据集来显著改善神经网络的缩放定律，将L∼d−α的衰减提升到任意快速的幂律衰减，甚至达到exp(−α′m√d)。

Conclusion: 该研究为神经网络的压缩和数据效率提升提供了理论基础，有望显著降低AI模型的训练成本，并推动AI技术向更高效、更接近人类水平的方向发展。

Abstract: When training large-scale models, the performance typically scales with the number of parameters and the dataset size according to a slow power law. A fundamental theoretical and practical question is whether comparable performance can be achieved with significantly smaller models and substantially less data. In this work, we provide a positive and constructive answer. We prove that a generic permutation-invariant function of $d$ objects can be asymptotically compressed into a function of $\operatorname{polylog} d$ objects with vanishing error. This theorem yields two key implications: (Ia) a large neural network can be compressed to polylogarithmic width while preserving its learning dynamics; (Ib) a large dataset can be compressed to polylogarithmic size while leaving the loss landscape of the corresponding model unchanged. (Ia) directly establishes a proof of the \textit{dynamical} lottery ticket hypothesis, which states that any ordinary network can be strongly compressed such that the learning dynamics and result remain unchanged. (Ib) shows that a neural scaling law of the form $L\sim d^{-\alpha}$ can be boosted to an arbitrarily fast power law decay, and ultimately to $\exp(-\alpha' \sqrt[m]{d})$.

</details>


### [18] [Theory of Scaling Laws for In-Context Regression: Depth, Width, Context and Time](https://arxiv.org/abs/2510.01098)
*Blake Bordelon, Mary I. Letey, Cengiz Pehlevan*

Main category: stat.ML

TL;DR: 本文研究了深度线性自注意力模型中线性回归的上下文学习（ICL），并分析了宽度、深度、上下文长度和训练步数等因素对性能的影响。研究发现，深度对ICL性能的影响取决于上下文长度和任务数据的统计结构，并预测了最优的Transformer形状。


<details>
  <summary>Details</summary>
Motivation: 现有理论无法解释为什么在Transformer中宽度和深度应该以特定的比例进行缩放，以及如何根据计算预算优化Transformer的形状。本文旨在探索上下文学习（ICL）问题中宽度和深度对Transformer性能的影响，并回答这些问题。

Method: 本文构建了一个可求解的深度线性注意力模型，分析了三种不同的ICL数据协方差结构：各向同性（ISO）、固定和结构化（FS）以及随机旋转和结构化（RRS）。通过分析这些模型，研究了上下文长度、宽度、深度和训练步数对ICL性能的影响。

Result: 研究发现，在各向同性（ISO）和固定结构化（FS）设置中，只有当上下文长度受限时，深度才能提高ICL性能。而在协方差随机旋转和结构化（RRS）设置中，增加深度可以显著提高ICL性能，即使在无限上下文长度下也是如此。研究还推导出了风险的精确渐近表达式，并根据源/容量条件推导了ICL任务的幂律。

Conclusion: 本文提供了一个新的、可解析的神经网络缩放定律模型，该模型依赖于Transformer的宽度和深度，并预测了Transformer形状与计算之间的函数关系。该模型有助于理解ICL任务的统计结构如何影响最优的宽度/深度比，并为Transformer的架构设计提供了指导。

Abstract: We study in-context learning (ICL) of linear regression in a deep linear self-attention model, characterizing how performance depends on various computational and statistical resources (width, depth, number of training steps, batch size and data per context). In a joint limit where data dimension, context length, and residual stream width scale proportionally, we analyze the limiting asymptotics for three ICL settings: (1) isotropic covariates and tasks (ISO), (2) fixed and structured covariance (FS), and (3) where covariances are randomly rotated and structured (RRS). For ISO and FS settings, we find that depth only aids ICL performance if context length is limited. Alternatively, in the RRS setting where covariances change across contexts, increasing the depth leads to significant improvements in ICL, even at infinite context length. This provides a new solvable toy model of neural scaling laws which depends on both width and depth of a transformer and predicts an optimal transformer shape as a function of compute. This toy model enables computation of exact asymptotics for the risk as well as derivation of powerlaws under source/capacity conditions for the ICL tasks.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [19] [Quantum Fisher information matrices from Rényi relative entropies](https://arxiv.org/abs/2510.02218)
*Mark M. Wilde*

Main category: quant-ph

TL;DR: 本文研究了基于Rényi相对熵的量子Fisher信息矩阵，并推导了多种信息矩阵，揭示了它们与经典Fisher信息矩阵之间的联系，并探讨了在量子机器学习中的应用。


<details>
  <summary>Details</summary>
Motivation: 量子Fisher信息是量子信息科学中的重要概念，在量子估计理论、机器学习和优化等领域有广泛应用。然而，与经典情况不同，量子Fisher信息矩阵没有唯一的推广，因此需要研究不同的Rényi相对熵推广。

Method: 利用分差法计算矩阵导数，推导了基于对数欧几里得、α-z和几何Rényi相对熵的信息矩阵。特别关注了参数化热态，并提出了混合量子-经典算法来估计这些信息矩阵。

Result: 发现对数欧几里得Rényi相对熵对应于Kubo-Mori信息矩阵，几何Rényi相对熵对应于右对数导数Fisher信息矩阵。这些信息矩阵满足数据处理不等式。此外，还推导并研究了α-z信息矩阵的基本性质，并为参数化热态提供了相应的公式。

Conclusion: 本文为量子Fisher信息矩阵的研究提供了新的视角和工具，揭示了不同Rényi相对熵推广之间的关系，并为量子机器学习中的应用，例如量子Boltzmann机器学习，提供了理论基础。

Abstract: Quantum generalizations of the Fisher information are important in quantum information science, with applications in high energy and condensed matter physics and in quantum estimation theory, machine learning, and optimization. One can derive a quantum generalization of the Fisher information matrix in a natural way as the Hessian matrix arising in a Taylor expansion of a smooth divergence. Such an approach is appealing for quantum information theorists, given the ubiquity of divergences in quantum information theory. In contrast to the classical case, there is not a unique quantum generalization of the Fisher information matrix, similar to how there is not a unique quantum generalization of the relative entropy or the R\'enyi relative entropy. In this paper, I derive information matrices arising from the log-Euclidean, $\alpha$-$z$, and geometric R\'enyi relative entropies, with the main technical tool for doing so being the method of divided differences for calculating matrix derivatives. Interestingly, for all non-negative values of the R\'enyi parameter $\alpha$, the log-Euclidean R\'enyi relative entropy leads to the Kubo-Mori information matrix, and the geometric R\'enyi relative entropy leads to the right-logarithmic derivative Fisher information matrix. Thus, the resulting information matrices obey the data-processing inequality for all non-negative values of the R\'enyi parameter $\alpha$ even though the original quantities do not. Additionally, I derive and establish basic properties of $\alpha$-$z$ information matrices resulting from the $\alpha$-$z$ R\'enyi relative entropies. For parameterized thermal states, I establish formulas for their $\alpha$-$z$ information matrices and hybrid quantum-classical algorithms for estimating them, with applications in quantum Boltzmann machine learning.

</details>
