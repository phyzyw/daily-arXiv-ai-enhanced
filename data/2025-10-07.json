{"id": "2510.04285", "title": "Probing Geometry of Next Token Prediction Using Cumulant Expansion of the Softmax Entropy", "authors": ["Karthik Viswanathan", "Sang Eon Park"], "summary": "We introduce a cumulant-expansion framework for quantifying how large language models (LLMs) internalize higher-order statistical structure during next-token prediction. By treating the softmax entropy of each layer's logit distribution as a perturbation around its \"center\" distribution, we derive closed-form cumulant observables that isolate successively higher-order correlations. Empirically, we track these cumulants in GPT-2 and Pythia models on Pile-10K prompts. (i) Structured prompts exhibit a characteristic rise-and-plateau profile across layers, whereas token-shuffled prompts remain flat, revealing the dependence of the cumulant profile on meaningful context. (ii) During training, all cumulants increase monotonically before saturating, directly visualizing the model's progression from capturing variance to learning skew, kurtosis, and higher-order statistical structures. (iii) Mathematical prompts show distinct cumulant signatures compared to general text, quantifying how models employ fundamentally different processing mechanisms for mathematical versus linguistic content. Together, these results establish cumulant analysis as a lightweight, mathematically grounded probe of feature-learning dynamics in high-dimensional neural networks.", "published": "2025-10-05", "categories": ["cs.CL", "cond-mat.stat-mech", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2510.04285v1", "primary_category": "cs.CL"}
{"id": "2510.03900", "title": "Optimal Computation from Fluctuation Responses", "authors": ["Jinghao Lyu", "Kyle J. Ray", "James P. Crutchfield"], "summary": "The energy cost of computation has emerged as a central challenge at the intersection of physics and computer science. Recent advances in statistical physics -- particularly in stochastic thermodynamics -- enable precise characterizations of work, heat, and entropy production in information-processing systems driven far from equilibrium by time-dependent control protocols. A key open question is then how to design protocols that minimize thermodynamic cost while ensur- ing correct outcomes. To this end, we develop a unified framework to identify optimal protocols using fluctuation response relations (FRR) and machine learning. Unlike previous approaches that optimize either distributions or protocols separately, our method unifies both using FRR-derived gradients. Moreover, our method is based primarily on iteratively learning from sampled noisy trajectories, which is generally much easier than solving for the optimal protocol directly from a set of governing equations. We apply the framework to canonical examples -- bit erasure in a double-well potential and translating harmonic traps -- demonstrating how to construct loss functions that trade-off energy cost against task error. The framework extends trivially to underdamped systems, and we show this by optimizing a bit-flip in an underdamped system. In all computations we test, the framework achieves the theoretically optimal protocol or achieves work costs comparable to relevant finite time bounds. In short, the results provide principled strategies for designing thermodynamically efficient protocols in physical information-processing systems. Applications range from quantum gates robust under noise to energy-efficient control of chemical and synthetic biological networks.", "published": "2025-10-04", "categories": ["cond-mat.stat-mech", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/2510.03900v1", "primary_category": "cond-mat.stat-mech"}
{"id": "2510.04227", "title": "A Universal Deep Learning Force Field for Molecular Dynamic Simulation and Vibrational Spectra Prediction", "authors": ["Shengjiao Ji", "Yujin Zhang", "Zihan Zou", "Bin Jiang", "Jun Jiang", "Yi Luo", "Wei Hu"], "summary": "Accurate and efficient simulation of infrared (IR) and Raman spectra is essential for molecular identification and structural analysis. Traditional quantum chemistry methods based on the harmonic approximation neglect anharmonicity and nuclear quantum effects, while ab initio molecular dynamics (AIMD) remains computationally expensive. Here, we integrate our deep equivariant tensor attention network (DetaNet) with a velocity-Verlet integrator to enable fast and accurate machine learning molecular dynamics (MLMD) simulations for spectral prediction. Trained on the QMe14S dataset containing energies, forces, dipole moments, and polarizabilities for 186,102 small organic molecules, DetaNet yields a universal and transferable force field with high-order tensor prediction capability. Using time-correlation functions derived from MLMD and ring-polymer molecular dynamics (RPMD) trajectories, we computed IR and Raman spectra that accurately reproduce anharmonic and nuclear quantum effects. Benchmark tests on isolated molecules, including polycyclic aromatic hydrocarbons, demonstrate that the DetaNet-based MD approach achieves near-experimental spectral accuracy with speedups up to three orders of magnitude over AIMD. Furthermore, the framework extends seamlessly to molecular and inorganic crystals, molecular aggregates, and biological macromolecules such as polypeptides with minimal fine-tuning. In all systems, DetaNet maintains high accuracy while significantly reducing computational cost. Overall, this work establishes a universal machine learning force field and tensor-aware MLMD framework that enable fast, accurate, and broadly applicable dynamic simulations and IR/Raman spectral predictions across diverse molecular and material systems.", "published": "2025-10-05", "categories": ["physics.chem-ph", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/2510.04227v1", "primary_category": "physics.chem-ph"}
{"id": "2510.04548", "title": "Learning Linear Regression with Low-Rank Tasks in-Context", "authors": ["Kaito Takanami", "Takashi Takahashi", "Yoshiyuki Kabashima"], "summary": "In-context learning (ICL) is a key building block of modern large language models, yet its theoretical mechanisms remain poorly understood. It is particularly mysterious how ICL operates in real-world applications where tasks have a common structure. In this work, we address this problem by analyzing a linear attention model trained on low-rank regression tasks. Within this setting, we precisely characterize the distribution of predictions and the generalization error in the high-dimensional limit. Moreover, we find that statistical fluctuations in finite pre-training data induce an implicit regularization. Finally, we identify a sharp phase transition of the generalization error governed by task structure. These results provide a framework for understanding how transformers learn to learn the task structure.", "published": "2025-10-06", "categories": ["cond-mat.dis-nn", "cs.LG", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2510.04548v1", "primary_category": "cond-mat.dis-nn"}
