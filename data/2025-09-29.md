<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [MORPH: Shape-agnostic PDE Foundation Models](https://arxiv.org/abs/2509.21670)
*Mahindra Singh Rautela, Alexander Most, Siddharth Mansingh, Bradley C. Love, Ayan Biswas, Diane Oyen, Earl Lawrence*

Main category: cs.CV

TL;DR: 本文提出了MORPH，一种基于卷积视觉Transformer的、无形状先验的偏微分方程(PDE)基础模型，能够处理异构时空数据集，并在各种下游预测任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有PDE基础模型面临数据稀缺、维度变化、分辨率不同、多物理场混合等挑战，且模拟和测量通常提供部分观测信息。因此，需要一种能够有效学习部分信息并泛化于数据异构性的模型。

Method: MORPH采用卷积视觉Transformer架构，结合逐组件卷积（处理标量和向量通道）、场间交叉注意力（建模和传播不同物理场的信息）以及轴向注意力（降低计算负担）。该模型在多样化的PDE数据集上进行预训练，并使用全模型微调和低秩适配器(LoRA)进行下游任务评估。

Result: MORPH在零样本和全样本泛化中优于从头开始训练的模型，在各种评估中达到或超过了强基线和最新的最先进模型。

Conclusion: MORPH提供了一个灵活且强大的学习异构和多模态科学观测的基础模型，为可扩展和数据高效的科学机器学习铺平了道路。

Abstract: We introduce MORPH, a shape-agnostic, autoregressive foundation model for partial differential equations (PDEs). MORPH is built on a convolutional vision transformer backbone that seamlessly handles heterogeneous spatiotemporal datasets of varying data dimensionality (1D--3D) at different resolutions, multiple fields with mixed scalar and vector components. The architecture combines (i) component-wise convolution, which jointly processes scalar and vector channels to capture local interactions, (ii) inter-field cross-attention, which models and selectively propagates information between different physical fields, (iii) axial attentions, which factorizes full spatiotemporal self-attention along individual spatial and temporal axes to reduce computational burden while retaining expressivity. We pretrain multiple model variants on a diverse collection of heterogeneous PDE datasets and evaluate transfer to a range of downstream prediction tasks. Using both full-model fine-tuning and parameter-efficient low-rank adapters (LoRA), MORPH outperforms models trained from scratch in both zero-shot and full-shot generalization. Across extensive evaluations, MORPH matches or surpasses strong baselines and recent state-of-the-art models. Collectively, these capabilities present a flexible and powerful backbone for learning from heterogeneous and multimodal nature of scientific observations, charting a path toward scalable and data-efficient scientific machine learning.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [2] [PhenoMoler: Phenotype-Guided Molecular Optimization via Chemistry Large Language Model](https://arxiv.org/abs/2509.21424)
*Ran Song, Hui Liu*

Main category: physics.chem-ph

TL;DR: PhenoMoler是一个基于化学大语言模型的框架，通过整合转录组谱信息，实现了基于表型的分子优化，能够对分子结构进行细粒度可控的优化。


<details>
  <summary>Details</summary>
Motivation: 现有分子生成模型主要关注药物与靶标的结合亲和力和特异性，忽略了化合物引起的系统级表型效应。为了解决这一问题，并实现更精准的药物设计，需要将表型信息融入分子生成过程中。

Method: PhenoMoler框架将化学大语言模型与表达谱整合，通过选择性地掩码和重建特定的分子子结构（支架、侧链或连接基），利用交叉注意力机制融合表型信息和分子上下文，从而指导分子重建。

Result: 实验表明，PhenoMoler能够生成化学上有效、新颖且多样化的分子，与FDA批准的药物相比，生成的化合物具有更高的药物相似性（QED）、优化的物理化学性质和更强的靶点结合亲和力。

Conclusion: PhenoMoler为基于表型和结构可控的分子优化提供了新的可能性，有望加速下一代治疗药物的开发。

Abstract: Current molecular generative models primarily focus on improving drug-target binding affinity and specificity, often neglecting the system-level phenotypic effects elicited by compounds. Transcriptional profiles, as molecule-level readouts of drug-induced phenotypic shifts, offer a powerful opportunity to guide molecular design in a phenotype-aware manner. We present PhenoMoler, a phenotype-guided molecular generation framework that integrates a chemistry large language model with expression profiles to enable biologically informed drug design. By conditioning the generation on drug-induced differential expression signatures, PhenoMoler explicitly links transcriptional responses to chemical structure. By selectively masking and reconstructing specific substructures-scaffolds, side chains, or linkers-PhenoMoler supports fine-grained, controllable molecular optimization. Extensive experiments demonstrate that PhenoMoler generates chemically valid, novel, and diverse molecules aligned with desired phenotypic profiles. Compared to FDA-approved drugs, the generated compounds exhibit comparable or enhanced drug-likeness (QED), optimized physicochemical properties, and superior binding affinity to key cancer targets. These findings highlight PhenoMoler's potential for phenotype-guided and structure-controllable molecular optimization.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [Fast-Forward Lattice Boltzmann: Learning Kinetic Behaviour with Physics-Informed Neural Operators](https://arxiv.org/abs/2509.22411)
*Xiao Xue, Marco F. P. ten Eikelder, Mingyang Gao, Xiaoyuan Cheng, Yiming Yang, Yi He, Shuo Wang, Sibo Cheng, Yukun Hu, Peter V. Coveney*

Main category: cs.LG

TL;DR: 本文提出了一种基于物理信息神经网络算子的LBE（晶格Boltzmann方程）框架，该框架能够预测长时间范围内的流体行为，无需显式求解碰撞算子，并实现了动力学超分辨率。


<details>
  <summary>Details</summary>
Motivation: 传统的LBE数值求解计算量大，且受限于严格的时间步长限制。为了克服这些计算挑战，并探索基于机器学习的替代方案，本文旨在开发一种高效且通用的LBE求解方法。

Method: 研究人员构建了一个物理信息神经网络算子框架，该框架融合了LBE的内在动量匹配约束和分布场的全局等变性。该框架具有离散化不变性，能够将粗格上的模型泛化到细格上，并且对底层碰撞模型的具体形式不敏感。

Result: 实验结果表明，该框架在复杂的流体场景中表现出鲁棒性，包括冯卡门涡街、韧性断裂和气泡粘附等，证明了其有效性。

Conclusion: 这项研究开辟了一条新的数据驱动的建模动力学系统路径，为解决LBE的计算瓶颈提供了新的思路，并有望应用于更广泛的科学计算领域。

Abstract: The lattice Boltzmann equation (LBE), rooted in kinetic theory, provides a powerful framework for capturing complex flow behaviour by describing the evolution of single-particle distribution functions (PDFs). Despite its success, solving the LBE numerically remains computationally intensive due to strict time-step restrictions imposed by collision kernels. Here, we introduce a physics-informed neural operator framework for the LBE that enables prediction over large time horizons without step-by-step integration, effectively bypassing the need to explicitly solve the collision kernel. We incorporate intrinsic moment-matching constraints of the LBE, along with global equivariance of the full distribution field, enabling the model to capture the complex dynamics of the underlying kinetic system. Our framework is discretization-invariant, enabling models trained on coarse lattices to generalise to finer ones (kinetic super-resolution). In addition, it is agnostic to the specific form of the underlying collision model, which makes it naturally applicable across different kinetic datasets regardless of the governing dynamics. Our results demonstrate robustness across complex flow scenarios, including von Karman vortex shedding, ligament breakup, and bubble adhesion. This establishes a new data-driven pathway for modelling kinetic systems.

</details>


### [4] [Reparameterizing 4DVAR with neural fields](https://arxiv.org/abs/2509.21751)
*Jaemin Oh*

Main category: cs.LG

TL;DR: 本文提出了一种基于神经网络场的四维变分数据同化（4DVAR）重参数化方法，通过并行时优化和物理信息损失，提高了初始条件估计的稳定性和准确性，且无需地面真实状态或重分析数据。


<details>
  <summary>Details</summary>
Motivation: 传统的4DVAR方法优化成本高昂且难以优化，尤其是在数值天气预报中。现有机器学习方法通常需要地面真实数据或重分析数据，限制了其适用性。

Method: 该方法将完整的时空状态表示为由神经网络参数化的连续函数，实现了对4DVAR成本函数的重参数化。通过神经网络，可以在参数空间中进行并行时优化，并直接通过物理信息损失函数整合物理约束。

Result: 在二维不可压缩纳维-斯托克斯方程的Kolmogorov强迫下，与基线4DVAR实现相比，神经网络重参数化方法产生了更稳定的初始条件估计，避免了伪振荡现象。

Conclusion: 该框架无需地面真实状态或重分析数据，拓宽了其在数据有限环境下的应用范围，为数据同化领域提供了一种新的、更高效且更灵活的解决方案。

Abstract: Four-dimensional variational data assimilation (4DVAR) is a cornerstone of numerical weather prediction, but its cost function is difficult to optimize and computationally intensive. We propose a neural field-based reformulation in which the full spatiotemporal state is represented as a continuous function parameterized by a neural network. This reparameterization removes the time-sequential dependency of classical 4DVAR, enabling parallel-in-time optimization in parameter space. Physical constraints are incorporated directly through a physics-informed loss, simplifying implementation and reducing computational cost. We evaluate the method on the two-dimensional incompressible Navier--Stokes equations with Kolmogorov forcing. Compared to a baseline 4DVAR implementation, the neural reparameterized variants produce more stable initial condition estimates without spurious oscillations. Notably, unlike most machine learning-based approaches, our framework does not require access to ground-truth states or reanalysis data, broadening its applicability to settings with limited reference information.

</details>


### [5] [Shoot from the HIP: Hessian Interatomic Potentials without derivatives](https://arxiv.org/abs/2509.21624)
*Andreas Burger, Luca Thiede, Nikolaj Rønne, Varinia Bernales, Nandita Vijaykumar, Tejs Vegge, Arghya Bhowmik, Alan Aspuru-Guzik*

Main category: cs.LG

TL;DR: 本文提出了一种名为HIP（Hessian Interatomic Potentials）的新方法，该方法利用深度学习模型直接预测分子Hessian，无需自动微分或有限差分。


<details>
  <summary>Details</summary>
Motivation: 传统的Hessian计算在计算化学中是重要的瓶颈，无论是量子力学方法还是神经网络，计算成本都高且难以扩展。现有的机器学习势能模型（MLIPs）虽然能降低计算成本，但通过自动微分计算Hessian仍然存在挑战。

Method: HIP方法基于图神经网络，利用SE(3)-equivariant、对称的Hessian特征（最高阶数为l=2）进行预测。该方法通过消息传递机制提取特征，直接构建Hessian矩阵，避免了传统的微分计算。

Result: 实验结果表明，HIP方法在计算速度上比传统方法快一个到两个数量级，精度更高，内存效率更好，训练更容易，并且对系统规模的扩展性更佳。在过渡态搜索、几何优化、零点能校正和振动分析等下游任务中，HIP方法表现出优越的性能。

Conclusion: HIP方法为直接预测Hessian开辟了新途径，有望加速材料发现和分子设计，并为计算化学领域带来更高效的计算工具。作者开源了HIP代码和模型权重，以促进进一步发展。

Abstract: Fundamental tasks in computational chemistry, from transition state search to vibrational analysis, rely on molecular Hessians, which are the second derivatives of the potential energy. Yet, Hessians are computationally expensive to calculate and scale poorly with system size, with both quantum mechanical methods and neural networks. In this work, we demonstrate that Hessians can be predicted directly from a deep learning model, without relying on automatic differentiation or finite differences. We observe that one can construct SE(3)-equivariant, symmetric Hessians from irreducible representations (irrep) features up to degree $l$=2 computed during message passing in graph neural networks. This makes HIP Hessians one to two orders of magnitude faster, more accurate, more memory efficient, easier to train, and enables more favorable scaling with system size. We validate our predictions across a wide range of downstream tasks, demonstrating consistently superior performance for transition state search, accelerated geometry optimization, zero-point energy corrections, and vibrational analysis benchmarks. We open-source the HIP codebase and model weights to enable further development of the direct prediction of Hessians at https://github.com/BurgerAndreas/hip

</details>


### [6] [Statistical Advantage of Softmax Attention: Insights from Single-Location Regression](https://arxiv.org/abs/2509.21936)
*O. Duranthon, P. Marion, C. Boyer, B. Loureiro, L. Zdeborová*

Main category: cs.LG

TL;DR: 本文通过对单位置回归任务的研究，揭示了softmax注意力机制在统计上的优势，并分析了其他激活函数性能，解释了softmax在大型语言模型中占据主导地位的原因。


<details>
  <summary>Details</summary>
Motivation: 尽管softmax注意力机制在大型语言模型中占据主导地位，但其优势缺乏理论解释，且现有研究多集中于线性化注意力。本文旨在填补这一空白，深入理解softmax的优势。

Method: 本文构建了单位置回归模型，该模型涵盖了信息检索任务，并结合分析和数值结果，在高维极限下利用统计物理思想分析了softmax和线性注意力层的性能，考察了不同激活函数的特性。

Result: 研究表明，在群体层面，softmax注意力机制能够达到贝叶斯风险，而线性注意力机制则无法达到。即使在有限样本的情况下，softmax也始终优于线性注意力。

Conclusion: 本文的分析为理解softmax注意力机制的优势提供了理论依据，并为设计更有效的注意力机制提供了指导，有助于进一步提升大型语言模型的性能。

Abstract: Large language models rely on attention mechanisms with a softmax activation. Yet the dominance of softmax over alternatives (e.g., component-wise or linear) remains poorly understood, and many theoretical works have focused on the easier-to-analyze linearized attention. In this work, we address this gap through a principled study of the single-location regression task, where the output depends on a linear transformation of a single input token at a random location. Building on ideas from statistical physics, we develop an analysis of attention-based predictors in the high-dimensional limit, where generalization performance is captured by a small set of order parameters. At the population level, we show that softmax achieves the Bayes risk, whereas linear attention fundamentally falls short. We then examine other activation functions to identify which properties are necessary for optimal performance. Finally, we analyze the finite-sample regime: we provide an asymptotic characterization of the test error and show that, while softmax is no longer Bayes-optimal, it consistently outperforms linear attention. We discuss the connection with optimization by gradient-based algorithms.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [7] [Toward a Physics of Deep Learning and Brains](https://arxiv.org/abs/2509.22649)
*Arsham Ghavasieh, Meritxell Vila-Minana, Akanksha Khurd, John Beggs, Gerardo Ortiz, Santo Fortunato*

Main category: cond-mat.dis-nn

TL;DR: 该研究表明，深度神经网络和大脑中的神经元雪崩都遵循非平衡统计物理的相同方程，并在准临界状态下表现出最佳学习性能，揭示了生物和人工神经网络之间的通用特征。


<details>
  <summary>Details</summary>
Motivation: 探索深度神经网络和大脑之间是否存在统一的理论框架，并理解它们为何在学习和信息处理方面表现出相似性。

Method: 利用非平衡统计物理的理论，将描述大脑神经元雪崩的方程应用于深度神经网络的活动级联。通过训练具有不同初始化的网络，研究了最大易感性与临界点距离之间的关系，并使用有限尺寸扩展识别了不同的普适性类别。

Result: 研究发现深度神经网络的最佳学习状态位于吸收态和活动态之间，表现出准临界行为，满足近似的裂变噪声缩放关系。最大易感性比临界点距离更能预测学习效果，并识别出包括Barkhausen噪声和定向渗透等不同的普适性类别。

Conclusion: 该研究提供了一个统一的理论框架，表明生物和人工神经网络共享通用特征，为工程化性能更优的网络提供了蓝图，并加深了我们对大脑信息处理机制的理解。

Abstract: Deep neural networks and brains both learn and share superficial similarities: processing nodes are likened to neurons and adjustable weights are likened to modifiable synapses. But can a unified theoretical framework be found to underlie them both? Here we show that the equations used to describe neuronal avalanches in living brains can also be applied to cascades of activity in deep neural networks. These equations are derived from non-equilibrium statistical physics and show that deep neural networks learn best when poised between absorbing and active phases. Because these networks are strongly driven by inputs, however, they do not operate at a true critical point but within a quasi-critical regime -- one that still approximately satisfies crackling noise scaling relations. By training networks with different initializations, we show that maximal susceptibility is a more reliable predictor of learning than proximity to the critical point itself. This provides a blueprint for engineering improved network performance. Finally, using finite-size scaling we identify distinct universality classes, including Barkhausen noise and directed percolation. This theoretical framework demonstrates that universal features are shared by both biological and artificial neural networks.

</details>
