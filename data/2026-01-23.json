{"id": "2601.15771", "title": "Rethinking Drug-Drug Interaction Modeling as Generalizable Relation Learning", "authors": ["Dong Xu", "Jiantao Wu", "Qihua Pan", "Sisi Yuan", "Zexuan Zhu", "Junkai Ji"], "summary": "Drug-drug interaction (DDI) prediction is central to drug discovery and clinical development, particularly in the context of increasingly prevalent polypharmacy. Although existing computational methods achieve strong performance on standard benchmarks, they often fail to generalize to realistic deployment scenarios, where most candidate drug pairs involve previously unseen drugs and validated interactions are scarce. We demonstrate that proximity in the embedding spaces of prevailing molecule-centric DDI models does not reliably correspond to interaction labels, and that simply scaling up model capacity therefore fails to improve generalization. To address these limitations, we propose GenRel-DDI, a generalizable relation learning framework that reformulates DDI prediction as a relation-centric learning problem, in which interaction representations are learned independently of drug identities. This relation-level abstraction enables the capture of transferable interaction patterns that generalize to unseen drugs and novel drug pairs. Extensive experiments across multiple benchmark demonstrate that GenRel-DDI consistently and significantly outperforms state-of-the-art methods, with particularly large gains on strict entity-disjoint evaluations, highlighting the effectiveness and practical utility of relation learning for robust DDI prediction. The code is available at https://github.com/SZU-ADDG/GenRel-DDI.", "published": "2026-01-22", "categories": ["cs.LG", "q-bio.BM"], "pdf_url": "https://arxiv.org/pdf/2601.15771v1", "primary_category": "cs.LG"}
{"id": "2601.15340", "title": "Learning Nonlinear Heterogeneity in Physical Kolmogorov-Arnold Networks", "authors": ["Fabiana Taglietti", "Andrea Pulici", "Maxwell Roxburgh", "Gabriele Seguini", "Ian Vidamour", "Stephan Menzel", "Edoardo Franco", "Michele Laus", "Eleni Vasilaki", "Michele Perego", "Thomas J. Hayward", "Marco Fanciulli", "Jack C. Gartside"], "summary": "Physical neural networks typically train linear synaptic weights while treating device nonlinearities as fixed. We show the opposite - by training the synaptic nonlinearity itself, as in Kolmogorov-Arnold Network (KAN) architectures, we yield markedly higher task performance per physical resource and improved performance-parameter scaling than conventional linear weight-based networks, demonstrating ability of KAN topologies to exploit reconfigurable nonlinear physical dynamics.   We experimentally realise physical KANs in silicon-on-insulator devices we term 'Synaptic Nonlinear Elements' (SYNEs), operating at room temperature, 0.1-1 microampere currents, and 2 MHz speeds with no observed degradation over 10^13 measurements and months-long timescales.   We demonstrate nonlinear function regression, classification, and prediction of Li-Ion battery dynamics from noisy real-world multi-sensor data. Physical KANs outperform equivalently-parameterised software multilayer perceptron networks across all tasks, with up to two orders of magnitude fewer parameters, and two orders of magnitude fewer devices than linear weight based physical networks. These results establish learned physical nonlinearity as a hardware-native computational primitive for compact and efficient learning systems, and SYNE devices as effective substrates for heterogenous nonlinear computing.", "published": "2026-01-20", "categories": ["cond-mat.dis-nn", "cond-mat.mes-hall", "cs.LG", "nlin.AO", "physics.app-ph"], "pdf_url": "https://arxiv.org/pdf/2601.15340v1", "primary_category": "cond-mat.dis-nn"}
