{"id": "2512.09317", "title": "Functional Percolation: A Perspective on Criticality of Form and Function", "authors": ["Galen J. Wilkerson"], "summary": "Understanding the physical constraints and minimal conditions that enable information processing in extended systems remains a central challenge across disciplines, from neuroscience and artificial intelligence to social and physical networks. Here we study how network connectivity both limits and enables information processing by analyzing random networks across the structural percolation transition. Using cascade-mediated dynamics as a minimal and universal mechanism for propagating state-dependent responses, we examine structural, functional, and information-theoretic observables as functions of mean degree in Erdos-Renyi networks. We find that the emergence of a giant connected component coincides with a sharp transition in realizable information processing: complex input-output response functions become accessible, functional diversity increases rapidly, output entropy rises, and directed information flow quantified by transfer entropy extends beyond local neighborhoods. These coincident transitions define a regime of functional percolation, referring to a sharp expansion of the space of realizable input-output functions at the structural percolation transition. Near criticality, networks exhibit a Pareto-optimal tradeoff between functional complexity and diversity, suggesting that percolation criticality provides a universal organizing principle for information processing in systems with local interactions and propagating influences.", "published": "2025-12-10", "categories": ["physics.soc-ph", "cond-mat.stat-mech", "cs.AI", "physics.comp-ph"], "pdf_url": "https://arxiv.org/pdf/2512.09317v1", "primary_category": "physics.soc-ph"}
{"id": "2512.08077", "title": "Unveiling Latent Knowledge in Chemistry Language Models through Sparse Autoencoders", "authors": ["Jaron Cohen", "Alexander G. Hasson", "Sara Tanovic"], "summary": "Since the advent of machine learning, interpretability has remained a persistent challenge, becoming increasingly urgent as generative models support high-stakes applications in drug and material discovery. Recent advances in large language model (LLM) architectures have yielded chemistry language models (CLMs) with impressive capabilities in molecular property prediction and molecular generation. However, how these models internally represent chemical knowledge remains poorly understood. In this work, we extend sparse autoencoder techniques to uncover and examine interpretable features within CLMs. Applying our methodology to the Foundation Models for Materials (FM4M) SMI-TED chemistry foundation model, we extract semantically meaningful latent features and analyse their activation patterns across diverse molecular datasets. Our findings reveal that these models encode a rich landscape of chemical concepts. We identify correlations between specific latent features and distinct domains of chemical knowledge, including structural motifs, physicochemical properties, and pharmacological drug classes. Our approach provides a generalisable framework for uncovering latent knowledge in chemistry-focused AI systems. This work has implications for both foundational understanding and practical deployment; with the potential to accelerate computational chemistry research.", "published": "2025-12-08", "categories": ["cs.LG", "physics.chem-ph"], "pdf_url": "https://arxiv.org/pdf/2512.08077v1", "primary_category": "cs.LG"}
{"id": "2512.09366", "title": "Meta-learning three-factor plasticity rules for structured credit assignment with sparse feedback", "authors": ["Dimitra Maoutsa"], "summary": "Biological neural networks learn complex behaviors from sparse, delayed feedback using local synaptic plasticity, yet the mechanisms enabling structured credit assignment remain elusive. In contrast, artificial recurrent networks solving similar tasks typically rely on biologically implausible global learning rules or hand-crafted local updates. The space of local plasticity rules capable of supporting learning from delayed reinforcement remains largely unexplored. Here, we present a meta-learning framework that discovers local learning rules for structured credit assignment in recurrent networks trained with sparse feedback. Our approach interleaves local neo-Hebbian-like updates during task execution with an outer loop that optimizes plasticity parameters via \\textbf{tangent-propagation through learning}. The resulting three-factor learning rules enable long-timescale credit assignment using only local information and delayed rewards, offering new insights into biologically grounded mechanisms for learning in recurrent circuits.", "published": "2025-12-10", "categories": ["q-bio.NC", "cond-mat.dis-nn", "cs.LG", "physics.bio-ph"], "pdf_url": "https://arxiv.org/pdf/2512.09366v1", "primary_category": "q-bio.NC"}
