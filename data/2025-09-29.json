{"id": "2509.22649", "title": "Toward a Physics of Deep Learning and Brains", "authors": ["Arsham Ghavasieh", "Meritxell Vila-Minana", "Akanksha Khurd", "John Beggs", "Gerardo Ortiz", "Santo Fortunato"], "summary": "Deep neural networks and brains both learn and share superficial similarities: processing nodes are likened to neurons and adjustable weights are likened to modifiable synapses. But can a unified theoretical framework be found to underlie them both? Here we show that the equations used to describe neuronal avalanches in living brains can also be applied to cascades of activity in deep neural networks. These equations are derived from non-equilibrium statistical physics and show that deep neural networks learn best when poised between absorbing and active phases. Because these networks are strongly driven by inputs, however, they do not operate at a true critical point but within a quasi-critical regime -- one that still approximately satisfies crackling noise scaling relations. By training networks with different initializations, we show that maximal susceptibility is a more reliable predictor of learning than proximity to the critical point itself. This provides a blueprint for engineering improved network performance. Finally, using finite-size scaling we identify distinct universality classes, including Barkhausen noise and directed percolation. This theoretical framework demonstrates that universal features are shared by both biological and artificial neural networks.", "published": "2025-09-26", "categories": ["cond-mat.dis-nn", "cond-mat.stat-mech", "cs.AI", "nlin.AO", "physics.bio-ph"], "pdf_url": "http://arxiv.org/pdf/2509.22649v1", "primary_category": "cond-mat.dis-nn"}
{"id": "2509.22411", "title": "Fast-Forward Lattice Boltzmann: Learning Kinetic Behaviour with Physics-Informed Neural Operators", "authors": ["Xiao Xue", "Marco F. P. ten Eikelder", "Mingyang Gao", "Xiaoyuan Cheng", "Yiming Yang", "Yi He", "Shuo Wang", "Sibo Cheng", "Yukun Hu", "Peter V. Coveney"], "summary": "The lattice Boltzmann equation (LBE), rooted in kinetic theory, provides a powerful framework for capturing complex flow behaviour by describing the evolution of single-particle distribution functions (PDFs). Despite its success, solving the LBE numerically remains computationally intensive due to strict time-step restrictions imposed by collision kernels. Here, we introduce a physics-informed neural operator framework for the LBE that enables prediction over large time horizons without step-by-step integration, effectively bypassing the need to explicitly solve the collision kernel. We incorporate intrinsic moment-matching constraints of the LBE, along with global equivariance of the full distribution field, enabling the model to capture the complex dynamics of the underlying kinetic system. Our framework is discretization-invariant, enabling models trained on coarse lattices to generalise to finer ones (kinetic super-resolution). In addition, it is agnostic to the specific form of the underlying collision model, which makes it naturally applicable across different kinetic datasets regardless of the governing dynamics. Our results demonstrate robustness across complex flow scenarios, including von Karman vortex shedding, ligament breakup, and bubble adhesion. This establishes a new data-driven pathway for modelling kinetic systems.", "published": "2025-09-26", "categories": ["cs.LG", "nlin.CG", "physics.comp-ph", "physics.flu-dyn"], "pdf_url": "http://arxiv.org/pdf/2509.22411v1", "primary_category": "cs.LG"}
{"id": "2509.21751", "title": "Reparameterizing 4DVAR with neural fields", "authors": ["Jaemin Oh"], "summary": "Four-dimensional variational data assimilation (4DVAR) is a cornerstone of numerical weather prediction, but its cost function is difficult to optimize and computationally intensive. We propose a neural field-based reformulation in which the full spatiotemporal state is represented as a continuous function parameterized by a neural network. This reparameterization removes the time-sequential dependency of classical 4DVAR, enabling parallel-in-time optimization in parameter space. Physical constraints are incorporated directly through a physics-informed loss, simplifying implementation and reducing computational cost. We evaluate the method on the two-dimensional incompressible Navier--Stokes equations with Kolmogorov forcing. Compared to a baseline 4DVAR implementation, the neural reparameterized variants produce more stable initial condition estimates without spurious oscillations. Notably, unlike most machine learning-based approaches, our framework does not require access to ground-truth states or reanalysis data, broadening its applicability to settings with limited reference information.", "published": "2025-09-26", "categories": ["cs.LG", "physics.comp-ph", "physics.flu-dyn"], "pdf_url": "http://arxiv.org/pdf/2509.21751v1", "primary_category": "cs.LG"}
{"id": "2509.21670", "title": "MORPH: Shape-agnostic PDE Foundation Models", "authors": ["Mahindra Singh Rautela", "Alexander Most", "Siddharth Mansingh", "Bradley C. Love", "Ayan Biswas", "Diane Oyen", "Earl Lawrence"], "summary": "We introduce MORPH, a shape-agnostic, autoregressive foundation model for partial differential equations (PDEs). MORPH is built on a convolutional vision transformer backbone that seamlessly handles heterogeneous spatiotemporal datasets of varying data dimensionality (1D--3D) at different resolutions, multiple fields with mixed scalar and vector components. The architecture combines (i) component-wise convolution, which jointly processes scalar and vector channels to capture local interactions, (ii) inter-field cross-attention, which models and selectively propagates information between different physical fields, (iii) axial attentions, which factorizes full spatiotemporal self-attention along individual spatial and temporal axes to reduce computational burden while retaining expressivity. We pretrain multiple model variants on a diverse collection of heterogeneous PDE datasets and evaluate transfer to a range of downstream prediction tasks. Using both full-model fine-tuning and parameter-efficient low-rank adapters (LoRA), MORPH outperforms models trained from scratch in both zero-shot and full-shot generalization. Across extensive evaluations, MORPH matches or surpasses strong baselines and recent state-of-the-art models. Collectively, these capabilities present a flexible and powerful backbone for learning from heterogeneous and multimodal nature of scientific observations, charting a path toward scalable and data-efficient scientific machine learning.", "published": "2025-09-25", "categories": ["cs.CV", "cs.AI", "cs.LG", "physics.comp-ph"], "pdf_url": "http://arxiv.org/pdf/2509.21670v1", "primary_category": "cs.CV"}
{"id": "2509.21624", "title": "Shoot from the HIP: Hessian Interatomic Potentials without derivatives", "authors": ["Andreas Burger", "Luca Thiede", "Nikolaj RÃ¸nne", "Varinia Bernales", "Nandita Vijaykumar", "Tejs Vegge", "Arghya Bhowmik", "Alan Aspuru-Guzik"], "summary": "Fundamental tasks in computational chemistry, from transition state search to vibrational analysis, rely on molecular Hessians, which are the second derivatives of the potential energy. Yet, Hessians are computationally expensive to calculate and scale poorly with system size, with both quantum mechanical methods and neural networks. In this work, we demonstrate that Hessians can be predicted directly from a deep learning model, without relying on automatic differentiation or finite differences. We observe that one can construct SE(3)-equivariant, symmetric Hessians from irreducible representations (irrep) features up to degree $l$=2 computed during message passing in graph neural networks. This makes HIP Hessians one to two orders of magnitude faster, more accurate, more memory efficient, easier to train, and enables more favorable scaling with system size. We validate our predictions across a wide range of downstream tasks, demonstrating consistently superior performance for transition state search, accelerated geometry optimization, zero-point energy corrections, and vibrational analysis benchmarks. We open-source the HIP codebase and model weights to enable further development of the direct prediction of Hessians at https://github.com/BurgerAndreas/hip", "published": "2025-09-25", "categories": ["cs.LG", "physics.chem-ph", "physics.comp-ph"], "pdf_url": "http://arxiv.org/pdf/2509.21624v1", "primary_category": "cs.LG"}
{"id": "2509.21424", "title": "PhenoMoler: Phenotype-Guided Molecular Optimization via Chemistry Large Language Model", "authors": ["Ran Song", "Hui Liu"], "summary": "Current molecular generative models primarily focus on improving drug-target binding affinity and specificity, often neglecting the system-level phenotypic effects elicited by compounds. Transcriptional profiles, as molecule-level readouts of drug-induced phenotypic shifts, offer a powerful opportunity to guide molecular design in a phenotype-aware manner. We present PhenoMoler, a phenotype-guided molecular generation framework that integrates a chemistry large language model with expression profiles to enable biologically informed drug design. By conditioning the generation on drug-induced differential expression signatures, PhenoMoler explicitly links transcriptional responses to chemical structure. By selectively masking and reconstructing specific substructures-scaffolds, side chains, or linkers-PhenoMoler supports fine-grained, controllable molecular optimization. Extensive experiments demonstrate that PhenoMoler generates chemically valid, novel, and diverse molecules aligned with desired phenotypic profiles. Compared to FDA-approved drugs, the generated compounds exhibit comparable or enhanced drug-likeness (QED), optimized physicochemical properties, and superior binding affinity to key cancer targets. These findings highlight PhenoMoler's potential for phenotype-guided and structure-controllable molecular optimization.", "published": "2025-09-25", "categories": ["physics.chem-ph", "cs.AI"], "pdf_url": "http://arxiv.org/pdf/2509.21424v1", "primary_category": "physics.chem-ph"}
{"id": "2509.21936", "title": "Statistical Advantage of Softmax Attention: Insights from Single-Location Regression", "authors": ["O. Duranthon", "P. Marion", "C. Boyer", "B. Loureiro", "L. ZdeborovÃ¡"], "summary": "Large language models rely on attention mechanisms with a softmax activation. Yet the dominance of softmax over alternatives (e.g., component-wise or linear) remains poorly understood, and many theoretical works have focused on the easier-to-analyze linearized attention. In this work, we address this gap through a principled study of the single-location regression task, where the output depends on a linear transformation of a single input token at a random location. Building on ideas from statistical physics, we develop an analysis of attention-based predictors in the high-dimensional limit, where generalization performance is captured by a small set of order parameters. At the population level, we show that softmax achieves the Bayes risk, whereas linear attention fundamentally falls short. We then examine other activation functions to identify which properties are necessary for optimal performance. Finally, we analyze the finite-sample regime: we provide an asymptotic characterization of the test error and show that, while softmax is no longer Bayes-optimal, it consistently outperforms linear attention. We discuss the connection with optimization by gradient-based algorithms.", "published": "2025-09-26", "categories": ["cs.LG", "cond-mat.dis-nn"], "pdf_url": "http://arxiv.org/pdf/2509.21936v1", "primary_category": "cs.LG"}
