<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Unveiling Latent Knowledge in Chemistry Language Models through Sparse Autoencoders](https://arxiv.org/abs/2512.08077)
*Jaron Cohen, Alexander G. Hasson, Sara Tanovic*

Main category: cs.LG

TL;DR: 本文将稀疏自编码器技术应用于化学语言模型（CLMs），揭示了模型内部编码的化学知识，并展示了这些知识与结构、物理性质和药物类别之间的关联。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在化学领域的应用日益广泛，但其内部决策过程不透明，难以判断模型是否真正理解化学原理，而非仅仅是统计模式匹配。因此，需要一种方法来解释这些模型的内部表示，以促进科学发现和模型泛化。

Method: 研究人员将稀疏自编码器（SAEs）应用于SMI-TED化学基础模型，训练SAEs以提取模型内部表示的稀疏特征，并分析这些特征在不同分子数据集上的激活模式。

Result: 研究发现，CLMs 编码了丰富的化学概念，识别出特定稀疏特征与结构基序、物理化学性质和药物类别之间的相关性，表明模型学习了化学知识。

Conclusion: 本文提供了一个通用的框架，用于揭示化学相关人工智能系统中的潜在知识，有望加速计算化学研究，并为AI辅助药物开发提供更可靠的基础。

Abstract: Since the advent of machine learning, interpretability has remained a persistent challenge, becoming increasingly urgent as generative models support high-stakes applications in drug and material discovery. Recent advances in large language model (LLM) architectures have yielded chemistry language models (CLMs) with impressive capabilities in molecular property prediction and molecular generation. However, how these models internally represent chemical knowledge remains poorly understood. In this work, we extend sparse autoencoder techniques to uncover and examine interpretable features within CLMs. Applying our methodology to the Foundation Models for Materials (FM4M) SMI-TED chemistry foundation model, we extract semantically meaningful latent features and analyse their activation patterns across diverse molecular datasets. Our findings reveal that these models encode a rich landscape of chemical concepts. We identify correlations between specific latent features and distinct domains of chemical knowledge, including structural motifs, physicochemical properties, and pharmacological drug classes. Our approach provides a generalisable framework for uncovering latent knowledge in chemistry-focused AI systems. This work has implications for both foundational understanding and practical deployment; with the potential to accelerate computational chemistry research.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [2] [Functional Percolation: A Perspective on Criticality of Form and Function](https://arxiv.org/abs/2512.09317)
*Galen J. Wilkerson*

Main category: physics.soc-ph

TL;DR: 本文研究了随机网络在结构渗透相变时信息处理的特性，发现结构渗透相变与信息处理能力、功能多样性、输出熵和信息流的急剧转变同时发生，定义了“功能渗透”现象。


<details>
  <summary>Details</summary>
Motivation: 理解物理约束和最小条件如何使扩展系统进行信息处理是跨学科的挑战。本文旨在探索网络连接如何限制和促进信息处理，并寻找信息处理的普适性组织原则。

Method: 通过分析Erd˝ os–R´ enyi随机网络在结构渗透相变时的级联介导动力学，研究了结构、功能和信息论的观测指标与平均度之间的关系。

Result: 研究发现，巨连接分量出现与可实现的信息处理能力急剧转变同时发生，功能多样性迅速增加，输出熵升高，有向信息流超出局部邻域。在临界状态附近，网络表现出功能复杂性和多样性之间的帕累托最优权衡。

Conclusion: 功能渗透现象表明，渗透临界性可能为具有局部交互和传播影响的系统中的信息处理提供了一种普适的组织原则，并为理解各种神经、社会、生物和物理系统中信息处理的底层机制提供了新的视角。

Abstract: Understanding the physical constraints and minimal conditions that enable information processing in extended systems remains a central challenge across disciplines, from neuroscience and artificial intelligence to social and physical networks. Here we study how network connectivity both limits and enables information processing by analyzing random networks across the structural percolation transition. Using cascade-mediated dynamics as a minimal and universal mechanism for propagating state-dependent responses, we examine structural, functional, and information-theoretic observables as functions of mean degree in Erdos-Renyi networks. We find that the emergence of a giant connected component coincides with a sharp transition in realizable information processing: complex input-output response functions become accessible, functional diversity increases rapidly, output entropy rises, and directed information flow quantified by transfer entropy extends beyond local neighborhoods. These coincident transitions define a regime of functional percolation, referring to a sharp expansion of the space of realizable input-output functions at the structural percolation transition. Near criticality, networks exhibit a Pareto-optimal tradeoff between functional complexity and diversity, suggesting that percolation criticality provides a universal organizing principle for information processing in systems with local interactions and propagating influences.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [3] [Meta-learning three-factor plasticity rules for structured credit assignment with sparse feedback](https://arxiv.org/abs/2512.09366)
*Dimitra Maoutsa*

Main category: q-bio.NC

TL;DR: 本文提出了一种元学习框架，用于发现支持循环神经网络中稀疏反馈学习的局部学习规则，这些规则仅依赖于局部信息和延迟奖励，并提供了对生物学上合理的学习机制的新见解。


<details>
  <summary>Details</summary>
Motivation: 生物神经元网络能够利用局部突触可塑性从稀疏、延迟的反馈中学习复杂的行为，但其机制尚不明确。现有的人工循环网络通常依赖于生物学上不可行的全局学习规则或手工设计的局部更新，而支持延迟强化学习的局部可塑性规则空间尚未得到充分探索。

Method: 该研究采用元学习框架，在循环网络中交替进行局部新-赫布式更新（task execution）和外循环优化可塑性参数（通过学习的切线传播）。具体来说，参数化可塑性规则为局部信号（突触前活动、突触后活动和突触大小）的函数，并通过第二个强化学习循环进行元学习。

Result: 研究结果表明，所提出的三因素学习规则能够仅使用局部信息和延迟奖励实现长时尺度信用分配。

Conclusion: 该研究为理解生物学上合理的循环电路学习机制提供了新的见解，并探索了在生物约束下实现结构化信用分配的局部学习规则。

Abstract: Biological neural networks learn complex behaviors from sparse, delayed feedback using local synaptic plasticity, yet the mechanisms enabling structured credit assignment remain elusive. In contrast, artificial recurrent networks solving similar tasks typically rely on biologically implausible global learning rules or hand-crafted local updates. The space of local plasticity rules capable of supporting learning from delayed reinforcement remains largely unexplored. Here, we present a meta-learning framework that discovers local learning rules for structured credit assignment in recurrent networks trained with sparse feedback. Our approach interleaves local neo-Hebbian-like updates during task execution with an outer loop that optimizes plasticity parameters via \textbf{tangent-propagation through learning}. The resulting three-factor learning rules enable long-timescale credit assignment using only local information and delayed rewards, offering new insights into biologically grounded mechanisms for learning in recurrent circuits.

</details>
