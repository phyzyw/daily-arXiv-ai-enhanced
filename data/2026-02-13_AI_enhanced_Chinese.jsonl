{"id": "2602.11626", "title": "ArGEnT: Arbitrary Geometry-encoded Transformer for Operator Learning", "authors": ["Wenqian Chen", "Yucheng Fu", "Michael Penwarden", "Pratanu Roy", "Panos Stinis"], "summary": "Learning solution operators for systems with complex, varying geometries and parametric physical settings is a central challenge in scientific machine learning. In many-query regimes such as design optimization, control and inverse problems, surrogate modeling must generalize across geometries while allowing flexible evaluation at arbitrary spatial locations. In this work, we propose Arbitrary Geometry-encoded Transformer (ArGEnT), a geometry-aware attention-based architecture for operator learning on arbitrary domains. ArGEnT employs Transformer attention mechanisms to encode geometric information directly from point-cloud representations with three variants-self-attention, cross-attention, and hybrid-attention-that incorporates different strategies for incorporating geometric features. By integrating ArGEnT into DeepONet as the trunk network, we develop a surrogate modeling framework capable of learning operator mappings that depend on both geometric and non-geometric inputs without the need to explicitly parametrize geometry as a branch network input. Evaluation on benchmark problems spanning fluid dynamics, solid mechanics and electrochemical systems, we demonstrate significantly improved prediction accuracy and generalization performance compared with the standard DeepONet and other existing geometry-aware saurrogates. In particular, the cross-attention transformer variant enables accurate geometry-conditioned predictions with reduced reliance on signed distance functions. By combining flexible geometry encoding with operator-learning capabilities, ArGEnT provides a scalable surrogate modeling framework for optimization, uncertainty quantification, and data-driven modeling of complex physical systems.", "abs": "", "categories": ["cs.LG", "cs.AI", "physics.chem-ph", "physics.comp-ph", "physics.flu-dyn"], "AI": {"tldr": "ArGEnT是一种基于Transformer的架构，用于在具有任意几何形状和物理设置的系统中学习算子，显著提高了预测精度和泛化性能。", "motivation": "在许多科学机器学习应用中，需要学习能够泛化到不同几何形状和物理参数的算子，尤其是在设计优化、控制和逆问题等需要大量查询的场景下，传统的代理模型难以满足需求。", "method": "ArGEnT将Transformer注意力机制应用于算子学习，直接从点云表示中编码几何信息，包含自注意力、交叉注意力、混合注意力三种变体，并将其集成到DeepONet中，无需显式地将几何形状作为分支网络输入。", "result": "在流体动力学、固体力学和电化学系统等基准问题上的评估表明，ArGEnT相比于标准DeepONet和其他几何感知代理模型，具有显著提高的预测精度和泛化性能，交叉注意力变体在减少对符号距离函数依赖的同时，实现了准确的几何条件预测。", "conclusion": "ArGEnT提供了一个可扩展的代理建模框架，结合了灵活的几何编码和算子学习能力，适用于优化、不确定性量化和复杂物理系统的驱动数据建模。"}}
{"id": "2602.11378", "title": "Toward Adaptive Non-Intrusive Reduced-Order Models: Design and Challenges", "authors": ["Amirpasha Hedayat", "Alberto Padovan", "Karthik Duraisamy"], "summary": "Projection-based Reduced Order Models (ROMs) are often deployed as static surrogates, which limits their practical utility once a system leaves the training manifold. We formalize and study adaptive non-intrusive ROMs that update both the latent subspace and the reduced dynamics online. Building on ideas from static non-intrusive ROMs, specifically, Operator Inference (OpInf) and the recently-introduced Non-intrusive Trajectory-based optimization of Reduced-Order Models (NiTROM), we propose three formulations: Adaptive OpInf (sequential basis/operator refits), Adaptive NiTROM (joint Riemannian optimization of encoder/decoder and polynomial dynamics), and a hybrid that initializes NiTROM with an OpInf update. We describe the online data window, adaptation window, and computational budget, and analyze cost scaling. On a transiently perturbed lid-driven cavity flow, static Galerkin/OpInf/NiTROM drift or destabilize when forecasting beyond training. In contrast, Adaptive OpInf robustly suppresses amplitude drift with modest cost; Adaptive NiTROM is shown to attain near-exact energy tracking under frequent updates but is sensitive to its initialization and optimization depth; the hybrid is most reliable under regime changes and minimal offline data, yielding physically coherent fields and bounded energy. We argue that predictive claims for ROMs must be cost-aware and transparent, with clear separation of training/adaptation/deployment regimes and explicit reporting of online budgets and full-order model queries. This work provides a practical template for building self-correcting, non-intrusive ROMs that remain effective as the dynamics evolve well beyond the initial manifold.", "abs": "", "categories": ["cs.LG", "cs.CE", "math.NA", "physics.comp-ph"], "AI": {"tldr": "本文提出并研究了自适应非侵入式降阶模型（ROMs），旨在解决传统ROM在超出训练范围时失效的问题。通过在线更新潜空间和降阶动力学，实现了对系统动态变化的适应。", "motivation": "传统降阶模型在超出训练数据范围时，预测能力会下降，限制了其在实际工程中的应用。因此，需要开发能够适应系统动态变化的自适应降阶模型。", "method": "基于Operator Inference (OpInf)和Non-intrusive Trajectory-based optimization of Reduced-Order Models (NiTROM)，提出了三种自适应ROMs：自适应OpInf、自适应NiTROM以及混合方法。这些方法通过在线数据窗口和适应窗口，对潜空间和降阶动力学进行更新。", "result": "在瞬态扰动下的驱动腔流实验中，静态Galerkin/OpInf/NiTROM模型在预测超出训练范围时会漂移或失稳。自适应OpInf能够有效抑制振幅漂移，自适应NiTROM在频繁更新下能实现近乎精确的能量跟踪，但对初始化敏感；混合方法在规制变化和少量离线数据下表现最可靠。", "conclusion": "本文提供了一个构建自校正、非侵入式ROMs的实用模板，使其能够在系统动态发生显著变化时保持有效。强调了预测ROMs时需要考虑成本因素，并明确区分训练、适应和部署阶段。"}}
{"id": "2602.11216", "title": "Protein Language Model Embeddings Improve Generalization of Implicit Transfer Operators", "authors": ["Panagiotis Antoniadis", "Beatrice Pavesi", "Simon Olsson", "Ole Winther"], "summary": "Molecular dynamics (MD) is a central computational tool in physics, chemistry, and biology, enabling quantitative prediction of experimental observables as expectations over high-dimensional molecular distributions such as Boltzmann distributions and transition densities. However, conventional MD is fundamentally limited by the high computational cost required to generate independent samples. Generative molecular dynamics (GenMD) has recently emerged as an alternative, learning surrogates of molecular distributions either from data or through interaction with energy models. While these methods enable efficient sampling, their transferability across molecular systems is often limited. In this work, we show that incorporating auxiliary sources of information can improve the data efficiency and generalization of transferable implicit transfer operators (TITO) for molecular dynamics. We find that coarse-grained TITO models are substantially more data-efficient than Boltzmann Emulators, and that incorporating protein language model (pLM) embeddings further improves out-of-distribution generalization. Our approach, PLaTITO, achieves state-of-the-art performance on equilibrium sampling benchmarks for out-of-distribution protein systems, including fast-folding proteins. We further study the impact of additional conditioning signals -- such as structural embeddings, temperature, and large-language-model-derived embeddings -- on model performance.", "abs": "", "categories": ["cs.LG", "physics.bio-ph"], "AI": {"tldr": "本文提出了一种名为PLaTITO的新方法，它将蛋白质语言模型嵌入到隐式转移算子(TITO)中，显著提高了分子动力学模拟的泛化能力和数据效率，尤其是在蛋白质系统外分布的采样任务中。", "motivation": "传统的分子动力学模拟计算成本高昂，而生成式分子动力学(GenMD)方法虽然提高了采样效率，但其泛化能力有限。为了解决这个问题，研究人员旨在开发更具数据效率和泛化能力的GenMD方法。", "method": "研究人员将蛋白质语言模型(pLM)嵌入到粗粒化的TITO模型中，构建了PLaTITO。该模型利用蛋白质序列和结构模型的信息，并结合了大型语言模型派生的注释，以提高模型性能。此外，还研究了结构嵌入、温度和大型语言模型派生的嵌入等附加条件信号对模型性能的影响。", "result": "PLaTITO在蛋白质系统外分布的平衡采样基准测试中取得了最先进的性能，包括快速折叠蛋白质。实验表明，PLaTITO比Boltzmann Emulators更有效率，并且能够恢复非Arrhenius蛋白质折叠和展开速率。", "conclusion": "PLaTITO的引入为分子动力学模拟提供了一种更有效、更具泛化能力的解决方案，有望加速蛋白质和其他生物分子的研究，并为新药开发等领域带来突破。"}}
{"id": "2602.11262", "title": "Unlearnable phases of matter", "authors": ["Tarun Advaith Kumar", "Yijian Zou", "Amir-Reza Negari", "Roger G. Melko", "Timothy H. Hsieh"], "summary": "We identify fundamental limitations in machine learning by demonstrating that non-trivial mixed-state phases of matter are computationally hard to learn. Focusing on unsupervised learning of distributions, we show that autoregressive neural networks fail to learn global properties of distributions characterized by locally indistinguishable (LI) states. We demonstrate that conditional mutual information (CMI) is a useful diagnostic for LI: we show that for classical distributions, long-range CMI of a state implies a spatially LI partner. By introducing a restricted statistical query model, we prove that nontrivial phases with long-range CMI, such as strong-to-weak spontaneous symmetry breaking phases, are hard to learn. We validate our claims by using recurrent, convolutional, and Transformer neural networks to learn the syndrome and physical distributions of toric/surface code under bit flip noise. Our findings suggest hardness of learning as a diagnostic tool for detecting mixed-state phases and transitions and error-correction thresholds, and they suggest CMI and more generally ``non-local Gibbsness'' as metrics for how hard a distribution is to learn.", "abs": "", "categories": ["cond-mat.dis-nn", "cs.LG", "quant-ph"], "AI": {"tldr": "本文证明了某些非平凡混合态相（即特定的概率分布）对于神经网络学习来说是困难的，这些相具有局部不可区分的状态，并且提出了条件互信息（CMI）作为衡量分布学习难度的指标。", "motivation": "机器学习在各个领域扮演着越来越重要的角色，因此理解其局限性至关重要。目前缺乏对哪些数据分布难以学习以及其难以学习的普遍原因的系统性理解。", "method": "研究人员引入了一种受限的统计查询学习模型，该模型捕捉了基于梯度的数据驱动训练中自回归神经网络的关键特征。他们通过证明局部不可区分的状态无法在此模型中高效学习，并建立了多项式时间下界。此外，他们还使用循环、卷积和Transformer神经网络来学习 toric/surface code 在位翻转噪声下的状态和物理分布。", "result": "研究发现，具有长程CMI的非平凡混合态相（例如强到弱自发对称性破缺相）难以学习。他们还发现，对于经典分布，长程CMI意味着存在局部不可区分的伙伴，反之亦然（在一维几何中）。", "conclusion": "本文的发现表明，学习难度可以作为检测混合态相和相变以及纠错阈值的诊断工具。CMI以及更广泛的“非局部吉布斯性”可以作为衡量分布学习难度的指标，为理解机器学习的局限性提供了新的视角。"}}
