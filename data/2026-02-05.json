{"id": "2602.04139", "title": "Generative Neural Operators through Diffusion Last Layer", "authors": ["Sungwon Park", "Anthony Zhou", "Hongjoong Kim", "Amir Barati Farimani"], "summary": "Neural operators have emerged as a powerful paradigm for learning discretization-invariant function-to-function mappings in scientific computing. However, many practical systems are inherently stochastic, making principled uncertainty quantification essential for reliable deployment. To address this, we introduce a simple add-on, the diffusion last layer (DLL), a lightweight probabilistic head that can be attached to arbitrary neural operator backbones to model predictive uncertainty. Motivated by the relative smoothness and low-dimensional structure often exhibited by PDE solution distributions, DLL parameterizes the conditional output distribution directly in function space through a low-rank Karhunen-Loève expansion, enabling efficient and expressive uncertainty modeling. Across stochastic PDE operator learning benchmarks, DLL improves generalization and uncertainty-aware prediction. Moreover, even in deterministic long-horizon rollout settings, DLL enhances rollout stability and provides meaningful estimates of epistemic uncertainty for backbone neural operators.", "published": "2026-02-04", "categories": ["cs.LG", "physics.comp-ph"], "pdf_url": "https://arxiv.org/pdf/2602.04139v1", "primary_category": "cs.LG"}
{"id": "2602.04861", "title": "From Evaluation to Design: Using Potential Energy Surface Smoothness Metrics to Guide Machine Learning Interatomic Potential Architectures", "authors": ["Ryan Liu", "Eric Qu", "Tobias Kreiman", "Samuel M. Blau", "Aditi S. Krishnapriyan"], "summary": "Machine Learning Interatomic Potentials (MLIPs) sometimes fail to reproduce the physical smoothness of the quantum potential energy surface (PES), leading to erroneous behavior in downstream simulations that standard energy and force regression evaluations can miss. Existing evaluations, such as microcanonical molecular dynamics (MD), are computationally expensive and primarily probe near-equilibrium states. To improve evaluation metrics for MLIPs, we introduce the Bond Smoothness Characterization Test (BSCT). This efficient benchmark probes the PES via controlled bond deformations and detects non-smoothness, including discontinuities, artificial minima, and spurious forces, both near and far from equilibrium. We show that BSCT correlates strongly with MD stability while requiring a fraction of the cost of MD. To demonstrate how BSCT can guide iterative model design, we utilize an unconstrained Transformer backbone as a testbed, illustrating how refinements such as a new differentiable $k$-nearest neighbors algorithm and temperature-controlled attention reduce artifacts identified by our metric. By optimizing model design systematically based on BSCT, the resulting MLIP simultaneously achieves a low conventional E/F regression error, stable MD simulations, and robust atomistic property predictions. Our results establish BSCT as both a validation metric and as an \"in-the-loop\" model design proxy that alerts MLIP developers to physical challenges that cannot be efficiently evaluated by current MLIP benchmarks.", "published": "2026-02-04", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI", "physics.chem-ph"], "pdf_url": "https://arxiv.org/pdf/2602.04861v1", "primary_category": "cs.LG"}
{"id": "2602.04696", "title": "Beyond Learning on Molecules by Weakly Supervising on Molecules", "authors": ["Gordan Prastalo", "Kevin Maik Jablonka"], "summary": "Molecular representations are inherently task-dependent, yet most pre-trained molecular encoders are not. Task conditioning promises representations that reorganize based on task descriptions, but existing approaches rely on expensive labeled data. We show that weak supervision on programmatically derived molecular motifs is sufficient. Our Adaptive Chemical Embedding Model (ACE-Mol) learns from hundreds of motifs paired with natural language descriptors that are cheap to compute, trivial to scale. Conventional encoders slowly search the embedding space for task-relevant structure, whereas ACE-Mol immediately aligns its representations with the task. ACE-Mol achieves state-of-the-art performance across molecular property prediction benchmarks with interpretable, chemically meaningful representations.", "published": "2026-02-04", "categories": ["physics.chem-ph", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2602.04696v1", "primary_category": "physics.chem-ph"}
{"id": "2602.04850", "title": "El Agente Quntur: A research collaborator agent for quantum chemistry", "authors": ["Juan B. Pérez-Sánchez", "Yunheng Zou", "Jorge A. Campos-Gonzalez-Angulo", "Marcel Müller", "Ignacio Gustin", "Andrew Wang", "Han Hao", "Tsz Wai Ko", "Changhyeok Choi", "Eric S. Isbrandt", "Mohammad Ghazi Vakili", "Hanyong Xu", "Chris Crebolder", "Varinia Bernales", "Alán Aspuru-Guzik"], "summary": "Quantum chemistry is a foundational enabling tool for the fields of chemistry, materials science, computational biology and others. Despite of its power, the practical application of quantum chemistry simulations remains in the hands of qualified experts due to methodological complexity, software heterogeneity, and the need for informed interpretation of results. To bridge the accessibility gap for these tools and expand their reach to chemists with broader backgrounds, we introduce El Agente Quntur, a hierarchical, multi-agent AI system designed to operate not merely as an automation tool but as a research collaborator for computational quantum chemistry. Quntur was designed following three main strategies: i) elimination of hard-coded procedural policies in favour of reasoning-driven decisions, ii) construction of general and composable actions that facilitate generalization and efficiency, and iii) implementation of guided deep research to integrate abstract quantum-chemical reasoning across subdisciplines and a detailed understanding of the software's internal logic and syntax. Although instantiated in ORCA, these design principles are applicable to research agents more generally and easily expandable to additional quantum chemistry packages and beyond. Quntur supports the full range of calculations available in ORCA 6.0 and reasons over software documentation and scientific literature to plan, execute, adapt, and analyze in silico chemistry experiments following best practices. We discuss the advances and current bottlenecks in agentic systems operating at the research level in computational chemistry, and outline a roadmap toward a fully autonomous end-to-end computational chemistry research agent.", "published": "2026-02-04", "categories": ["physics.chem-ph", "cs.AI", "cs.MA"], "pdf_url": "https://arxiv.org/pdf/2602.04850v1", "primary_category": "physics.chem-ph"}
{"id": "2602.04849", "title": "El Agente Estructural: An Artificially Intelligent Molecular Editor", "authors": ["Changhyeok Choi", "Yunheng Zou", "Marcel Müller", "Han Hao", "Yeonghun Kang", "Juan B. Pérez-Sánchez", "Ignacio Gustin", "Hanyong Xu", "Mohammad Ghazi Vakili", "Chris Crebolder", "Alán Aspuru-Guzik", "Varinia Bernales"], "summary": "We present El Agente Estructural, a multimodal, natural-language-driven geometry-generation and manipulation agent for autonomous chemistry and molecular modelling. Unlike molecular generation or editing via generative models, Estructural mimics how human experts directly manipulate molecular systems in three dimensions by integrating a comprehensive set of domain-informed tools and vision-language models. This design enables precise control over atomic or functional group replacements, atomic connectivity, and stereochemistry without the need to rebuild extensive core molecular frameworks. Through a series of representative case studies, we demonstrate that Estructural enables chemically meaningful geometry manipulation across a wide range of real-world scenarios. These include site-selective functionalization, ligand binding, ligand exchange, stereochemically controlled structure construction, isomer interconversion, fragment-level structural analysis, image-guided generation of structures from schematic reaction mechanisms, and mechanism-driven geometry generation and modification. These examples illustrate how multimodal reasoning, when combined with specialized geometry-aware tools, supports interactive and context-aware molecular modelling beyond structure generation. Looking forward, the integration of Estructural into El Agente Quntur, an autonomous multi-agent quantum chemistry platform, enhances its capabilities by adding sophisticated tools for the generation and editing of three-dimensional structures.", "published": "2026-02-04", "categories": ["physics.chem-ph", "cs.AI", "cs.MA"], "pdf_url": "https://arxiv.org/pdf/2602.04849v1", "primary_category": "physics.chem-ph"}
{"id": "2602.04883", "title": "Protein Autoregressive Modeling via Multiscale Structure Generation", "authors": ["Yanru Qu", "Cheng-Yen Hsieh", "Zaixiang Zheng", "Ge Liu", "Quanquan Gu"], "summary": "We present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for protein backbone generation via coarse-to-fine next-scale prediction. Using the hierarchical nature of proteins, PAR generates structures that mimic sculpting a statue, forming a coarse topology and refining structural details over scales. To achieve this, PAR consists of three key components: (i) multi-scale downsampling operations that represent protein structures across multiple scales during training; (ii) an autoregressive transformer that encodes multi-scale information and produces conditional embeddings to guide structure generation; (iii) a flow-based backbone decoder that generates backbone atoms conditioned on these embeddings. Moreover, autoregressive models suffer from exposure bias, caused by the training and the generation procedure mismatch, and substantially degrades structure generation quality. We effectively alleviate this issue by adopting noisy context learning and scheduled sampling, enabling robust backbone generation. Notably, PAR exhibits strong zero-shot generalization, supporting flexible human-prompted conditional generation and motif scaffolding without requiring fine-tuning. On the unconditional generation benchmark, PAR effectively learns protein distributions and produces backbones of high design quality, and exhibits favorable scaling behavior. Together, these properties establish PAR as a promising framework for protein structure generation.", "published": "2026-02-04", "categories": ["cs.LG", "cs.AI", "q-bio.BM", "q-bio.QM"], "pdf_url": "https://arxiv.org/pdf/2602.04883v1", "primary_category": "cs.LG"}
{"id": "2602.04774", "title": "Theory of Optimal Learning Rate Schedules and Scaling Laws for a Random Feature Model", "authors": ["Blake Bordelon", "Francesco Mori"], "summary": "Setting the learning rate for a deep learning model is a critical part of successful training, yet choosing this hyperparameter is often done empirically with trial and error. In this work, we explore a solvable model of optimal learning rate schedules for a powerlaw random feature model trained with stochastic gradient descent (SGD). We consider the optimal schedule $η_T^\\star(t)$ where $t$ is the current iterate and $T$ is the total training horizon. This schedule is computed both numerically and analytically (when possible) using optimal control methods. Our analysis reveals two regimes which we term the easy phase and hard phase. In the easy phase the optimal schedule is a polynomial decay $η_T^\\star(t) \\simeq T^{-ξ} (1-t/T)^δ$ where $ξ$ and $δ$ depend on the properties of the features and task. In the hard phase, the optimal schedule resembles warmup-stable-decay with constant (in $T$) initial learning rate and annealing performed over a vanishing (in $T$) fraction of training steps. We investigate joint optimization of learning rate and batch size, identifying a degenerate optimality condition. Our model also predicts the compute-optimal scaling laws (where model size and training steps are chosen optimally) in both easy and hard regimes. Going beyond SGD, we consider optimal schedules for the momentum $β(t)$, where speedups in the hard phase are possible. We compare our optimal schedule to various benchmarks in our task including (1) optimal constant learning rates $η_T(t) \\sim T^{-ξ}$ (2) optimal power laws $η_T(t) \\sim T^{-ξ} t^{-χ}$, finding that our schedule achieves better rates than either of these. Our theory suggests that learning rate transfer across training horizon depends on the structure of the model and task. We explore these ideas in simple experimental pretraining setups.", "published": "2026-02-04", "categories": ["cond-mat.dis-nn", "cs.LG", "stat.ML"], "pdf_url": "https://arxiv.org/pdf/2602.04774v1", "primary_category": "cond-mat.dis-nn"}
{"id": "2602.04404", "title": "Theory of Speciation Transitions in Diffusion Models with General Class Structure", "authors": ["Beatrice Achilli", "Marco Benedetti", "Giulio Biroli", "Marc Mézard"], "summary": "Diffusion Models generate data by reversing a stochastic diffusion process, progressively transforming noise into structured samples drawn from a target distribution. Recent theoretical work has shown that this backward dynamics can undergo sharp qualitative transitions, known as speciation transitions, during which trajectories become dynamically committed to data classes. Existing theoretical analyses, however, are limited to settings where classes are identifiable through first moments, such as mixtures of Gaussians with well-separated means. In this work, we develop a general theory of speciation in diffusion models that applies to arbitrary target distributions admitting well-defined classes. We formalize the notion of class structure through Bayes classification and characterize speciation times in terms of free-entropy difference between classes. This criterion recovers known results in previously studied Gaussian-mixture models, while extending to situations in which classes are not distinguishable by first moments and may instead differ through higher-order or collective features. Our framework also accommodates multiple classes and predicts the existence of successive speciation times associated with increasingly fine-grained class commitment. We illustrate the theory on two analytically tractable examples: mixtures of one-dimensional Ising models at different temperatures and mixtures of zero-mean Gaussians with distinct covariance structures. In the Ising case, we obtain explicit expressions for speciation times by mapping the problem onto a random-field Ising model and solving it via the replica method. Our results provide a unified and broadly applicable description of speciation transitions in diffusion-based generative models.", "published": "2026-02-04", "categories": ["cs.LG", "cond-mat.dis-nn"], "pdf_url": "https://arxiv.org/pdf/2602.04404v1", "primary_category": "cs.LG"}
