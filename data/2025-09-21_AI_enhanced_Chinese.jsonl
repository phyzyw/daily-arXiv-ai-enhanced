{"id": "2509.15219", "title": "Out-of-Sight Trajectories: Tracking, Fusion, and Prediction", "authors": ["Haichao Zhang", "Yi Xu", "Yun Fu"], "summary": "Trajectory prediction is a critical task in computer vision and autonomous systems, playing a key role in autonomous driving, robotics, surveillance, and virtual reality. Existing methods often rely on complete and noise-free observational data, overlooking the challenges associated with out-of-sight objects and the inherent noise in sensor data caused by limited camera coverage, obstructions, and the absence of ground truth for denoised trajectories. These limitations pose safety risks and hinder reliable prediction in real-world scenarios. In this extended work, we present advancements in Out-of-Sight Trajectory (OST), a novel task that predicts the noise-free visual trajectories of out-of-sight objects using noisy sensor data. Building on our previous research, we broaden the scope of Out-of-Sight Trajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending its applicability to autonomous driving, robotics, surveillance, and virtual reality. Our enhanced Vision-Positioning Denoising Module leverages camera calibration to establish a vision-positioning mapping, addressing the lack of visual references, while effectively denoising noisy sensor data in an unsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB datasets, our approach achieves state-of-the-art performance in both trajectory denoising and prediction, significantly surpassing previous baselines. Additionally, we introduce comparisons with traditional denoising methods, such as Kalman filtering, and adapt recent trajectory prediction models to our task, providing a comprehensive benchmark. This work represents the first initiative to integrate vision-positioning projection for denoising noisy sensor trajectories of out-of-sight agents, paving the way for future advances. The code and preprocessed datasets are available at github.com/Hai-chao-Zhang/OST", "abs": "", "categories": ["cs.CV", "cs.LG", "cs.MA", "cs.MM", "cs.RO", "68T45, 68U10, 68T07, 68T40, 93C85, 93E11, 62M20, 62M10, 68U05, 94A12", "F.2.2; I.2.9; I.2.10; I.4.1; I.4.8; I.4.9; I.5.4; I.3.7"], "AI": {"tldr": "本文提出了一种名为Out-of-Sight Trajectory (OST) 的新任务，旨在利用噪声传感器数据预测视野之外物体的无噪声视觉轨迹，并取得了显著的轨迹去噪和预测效果。", "motivation": "现有轨迹预测方法通常依赖于完整且无噪声的观测数据，而忽略了视野之外物体和传感器数据噪声带来的挑战，这在现实场景中会带来安全风险。", "method": "本文增强了Vision-Positioning Denoising Module，利用相机标定建立视觉定位映射，从而在无监督的情况下有效地对噪声传感器数据进行去噪。该方法同时扩展了OOSTraj任务，使其适用于行人与车辆。", "result": "在Vi-Fi和JRDB数据集上的广泛评估中，该方法在轨迹去噪和预测方面均实现了最先进的性能，显著优于先前基线，并与传统去噪方法（如卡尔曼滤波）和现有轨迹预测模型进行了比较。", "conclusion": "本文首次将视觉定位投影技术应用于去噪视野之外的代理轨迹，为该领域未来的发展开辟了道路，并为自动驾驶、机器人、虚拟现实和监控等应用提供了新的解决方案。"}}
{"id": "2509.15217", "title": "Generalizable Geometric Image Caption Synthesis", "authors": ["Yue Xin", "Wenyuan Wang", "Rui Pan", "Ruida Wang", "Howard Meng", "Renjie Pi", "Shizhe Diao", "Tong Zhang"], "summary": "Multimodal large language models have various practical applications that demand strong reasoning abilities. Despite recent advancements, these models still struggle to solve complex geometric problems. A key challenge stems from the lack of high-quality image-text pair datasets for understanding geometric images. Furthermore, most template-based data synthesis pipelines typically fail to generalize to questions beyond their predefined templates. In this paper, we bridge this gap by introducing a complementary process of Reinforcement Learning with Verifiable Rewards (RLVR) into the data generation pipeline. By adopting RLVR to refine captions for geometric images synthesized from 50 basic geometric relations and using reward signals derived from mathematical problem-solving tasks, our pipeline successfully captures the key features of geometry problem-solving. This enables better task generalization and yields non-trivial improvements. Furthermore, even in out-of-distribution scenarios, the generated dataset enhances the general reasoning capabilities of multimodal large language models, yielding accuracy improvements of $2.8\\%\\text{-}4.8\\%$ in statistics, arithmetic, algebraic, and numerical tasks with non-geometric input images of MathVista and MathVerse, along with $2.4\\%\\text{-}3.9\\%$ improvements in Art, Design, Tech, and Engineering tasks in MMMU.", "abs": "", "categories": ["cs.AI", "cs.CV", "cs.LG"], "AI": {"tldr": "本文提出了一种利用强化学习和可验证奖励（RLVR）生成高质量几何图像-文本对的数据合成方法，以提升多模态大语言模型（MLLMs）的几何推理能力，并实现了泛化能力。", "motivation": "现有MLLMs在解决复杂几何问题时仍存在困难，高质量的几何图像-文本数据集匮乏，且传统模板数据合成方法难以泛化。为了弥补这些不足，提升MLLMs的跨模态推理能力，需要高质量的几何问题数据集。", "method": "本文采用RLVR方法，基于50个基本几何关系合成图像，并利用数学问题解决任务中的奖励信号来优化图像的文本描述。使用RAFT方法设计了包含推理和描述奖励的奖励函数。", "result": "实验表明，生成的合成数据集能够提升MLLMs在统计、算术、代数和数值任务中的准确率，在非几何图像的MathVista和MathVerse数据集上分别提升了2.8%-4.8%，在Art & Design和Tech & Engineering任务中在MMMU数据集上提升了2.4%-3.9%。", "conclusion": "本文提出的RLVR数据合成方法有效提升了MLLMs的几何推理和泛化能力，为构建更强大的多模态大语言模型提供了新的途径。"}}
{"id": "2509.15124", "title": "Learning Mechanistic Subtypes of Neurodegeneration with a Physics-Informed Variational Autoencoder Mixture Model", "authors": ["Sanduni Pinnawala", "Annabelle Hartanto", "Ivor J. A. Simpson", "Peter A. Wijeratne"], "summary": "Modelling the underlying mechanisms of neurodegenerative diseases demands methods that capture heterogeneous and spatially varying dynamics from sparse, high-dimensional neuroimaging data. Integrating partial differential equation (PDE) based physics knowledge with machine learning provides enhanced interpretability and utility over classic numerical methods. However, current physics-integrated machine learning methods are limited to considering a single PDE, severely limiting their application to diseases where multiple mechanisms are responsible for different groups (i.e., subtypes) and aggravating problems with model misspecification and degeneracy. Here, we present a deep generative model for learning mixtures of latent dynamic models governed by physics-based PDEs, going beyond traditional approaches that assume a single PDE structure. Our method integrates reaction-diffusion PDEs within a variational autoencoder (VAE) mixture model framework, supporting inference of subtypes of interpretable latent variables (e.g. diffusivity and reaction rates) from neuroimaging data. We evaluate our method on synthetic benchmarks and demonstrate its potential for uncovering mechanistic subtypes of Alzheimer's disease progression from positron emission tomography (PET) data.", "abs": "", "categories": ["eess.IV", "cs.CV", "cs.LG"], "AI": {"tldr": "本文提出了一种基于物理信息的变分自编码器混合模型（BrainPhys），用于学习神经退行性疾病的潜在动态模型，并能够识别疾病亚型。", "motivation": "传统的物理信息机器学习方法通常假设单一的偏微分方程（PDE）结构，难以处理神经退行性疾病中多种机制和亚型，且容易出现模型错位和简并问题。", "method": "BrainPhys将反应-扩散PDE整合到变分自编码器（VAE）混合模型框架中，允许同时学习多个可能的PDE，从而支持从神经影像数据中推断可解释的潜在变量（如扩散率和反应速率）的亚型。", "result": "在合成基准测试中，该模型能够恢复机制模型的簇及其参数。应用于ADNI的tau和amyloid PET数据集，发现支持2-成分混合模型。", "conclusion": "BrainPhys为理解神经退行性疾病的异质性提供了新的方法，能够揭示疾病亚型的机制，并有望提高疾病建模和预测的准确性。"}}
{"id": "2509.15167", "title": "Semi-Supervised 3D Medical Segmentation from 2D Natural Images Pretrained Model", "authors": ["Pak-Hei Yeung", "Jayroop Ramesh", "Pengfei Lyu", "Ana Namburete", "Jagath Rajapakse"], "summary": "This paper explores the transfer of knowledge from general vision models pretrained on 2D natural images to improve 3D medical image segmentation. We focus on the semi-supervised setting, where only a few labeled 3D medical images are available, along with a large set of unlabeled images. To tackle this, we propose a model-agnostic framework that progressively distills knowledge from a 2D pretrained model to a 3D segmentation model trained from scratch. Our approach, M&N, involves iterative co-training of the two models using pseudo-masks generated by each other, along with our proposed learning rate guided sampling that adaptively adjusts the proportion of labeled and unlabeled data in each training batch to align with the models' prediction accuracy and stability, minimizing the adverse effect caused by inaccurate pseudo-masks. Extensive experiments on multiple publicly available datasets demonstrate that M&N achieves state-of-the-art performance, outperforming thirteen existing semi-supervised segmentation approaches under all different settings. Importantly, ablation studies show that M&N remains model-agnostic, allowing seamless integration with different architectures. This ensures its adaptability as more advanced models emerge. The code is available at https://github.com/pakheiyeung/M-N.", "abs": "", "categories": ["cs.CV", "cs.AI", "cs.LG"], "AI": {"tldr": "本文提出了一种模型无关的框架M&N，通过知识蒸馏将2D自然图像预训练模型中的知识迁移到3D医学图像分割模型，尤其适用于标注数据有限的半监督学习场景。", "motivation": "3D医学图像分割需要大量标注数据，而2D自然图像领域拥有丰富的预训练模型。本文旨在探索如何利用这些预训练模型的知识来提升3D医学图像分割的性能，特别是在标注数据不足的情况下。", "method": "M&N框架采用迭代协同训练策略，2D和3D模型相互学习，利用彼此生成的伪掩码进行训练。同时，引入了基于学习率引导的采样方法，自适应调整训练批次中标记数据和未标记数据的比例，以优化模型预测准确性和稳定性。", "result": "在多个公开数据集上的实验表明，M&N框架实现了最先进的性能，优于13种现有的半监督分割方法。消融实验验证了M&N框架的模型无关性，使其能够与不同的网络架构无缝集成。", "conclusion": "M&N框架提供了一种灵活且通用的知识迁移方法，能够有效利用2D预训练模型的知识来提升3D医学图像分割的性能，尤其适用于标注数据有限的场景，为未来的医学图像分析研究提供了新的思路。"}}
{"id": "2509.15076", "title": "Forecasting and Visualizing Air Quality from Sky Images with Vision-Language Models", "authors": ["Mohammad Saleh Vahdatpour", "Maryam Eyvazi", "Yanqing Zhang"], "summary": "Air pollution remains a critical threat to public health and environmental sustainability, yet conventional monitoring systems are often constrained by limited spatial coverage and accessibility. This paper proposes an AI-driven agent that predicts ambient air pollution levels from sky images and synthesizes realistic visualizations of pollution scenarios using generative modeling. Our approach combines statistical texture analysis with supervised learning for pollution classification, and leverages vision-language model (VLM)-guided image generation to produce interpretable representations of air quality conditions. The generated visuals simulate varying degrees of pollution, offering a foundation for user-facing interfaces that improve transparency and support informed environmental decision-making. These outputs can be seamlessly integrated into intelligent applications aimed at enhancing situational awareness and encouraging behavioral responses based on real-time forecasts. We validate our method using a dataset of urban sky images and demonstrate its effectiveness in both pollution level estimation and semantically consistent visual synthesis. The system design further incorporates human-centered user experience principles to ensure accessibility, clarity, and public engagement in air quality forecasting. To support scalable and energy-efficient deployment, future iterations will incorporate a green CNN architecture enhanced with FPGA-based incremental learning, enabling real-time inference on edge platforms.", "abs": "", "categories": ["cs.LG", "cs.CV"], "AI": {"tldr": "本文提出了一种基于视觉-语言模型(VLM)的AI系统，该系统能够从天空图像预测空气污染水平并生成逼真的污染场景可视化，旨在弥补传统监测系统的不足。", "motivation": "传统空气质量监测系统存在空间覆盖范围有限、成本高昂等问题，迫切需要更高效、更具可解释性的预测方法，以支持环境决策和公众健康。", "method": "该方法结合统计纹理分析和监督学习进行污染分类，并利用VLM引导图像生成，创建可解释的空气质量可视化。系统设计考虑了以人为本的用户体验，并计划未来采用绿色CNN架构和FPGA加速，实现边缘平台的实时推理。", "result": "实验结果表明，该系统在城市天空图像数据集上能够有效地估计污染水平并生成语义一致的视觉效果。", "conclusion": "该系统为空气质量预测提供了新的视角，通过可视化呈现污染状况，可以提高公众意识，促进环境决策，并为未来在边缘设备上部署实时空气质量预测系统奠定基础。"}}
{"id": "2509.15058", "title": "Communication Efficient Split Learning of ViTs with Attention-based Double Compression", "authors": ["Federico Alvetreti", "Jary Pomponi", "Paolo Di Lorenzo", "Simone Scardapane"], "summary": "This paper proposes a novel communication-efficient Split Learning (SL) framework, named Attention-based Double Compression (ADC), which reduces the communication overhead required for transmitting intermediate Vision Transformers activations during the SL training process. ADC incorporates two parallel compression strategies. The first one merges samples' activations that are similar, based on the average attention score calculated in the last client layer; this strategy is class-agnostic, meaning that it can also merge samples having different classes, without losing generalization ability nor decreasing final results. The second strategy follows the first and discards the least meaningful tokens, further reducing the communication cost. Combining these strategies not only allows for sending less during the forward pass, but also the gradients are naturally compressed, allowing the whole model to be trained without additional tuning or approximations of the gradients. Simulation results demonstrate that Attention-based Double Compression outperforms state-of-the-art SL frameworks by significantly reducing communication overheads while maintaining high accuracy.", "abs": "", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "AI": {"tldr": "本文提出了一种名为Attention-based Double Compression (ADC) 的新型通信高效的Split Learning (SL) 框架，通过并行压缩策略显著降低了Vision Transformers (ViTs) 在训练过程中的通信开销，同时保持了高精度。", "motivation": "深度神经网络的训练需要大量的计算资源和内存，传统的云端学习模式存在通信开销大和隐私泄露的风险。Split Learning (SL) 是一种解决这些问题的有前景的方案，但通信瓶颈仍然是实际应用中的一个重要限制。", "method": "ADC框架采用两种并行压缩策略：首先，基于最后一层客户端的平均注意力分数，合并相似的样本激活值（不考虑类别）；其次，丢弃最不重要的token，进一步降低通信成本。这两种策略共同作用，压缩了前向传播的数据，并自然地压缩了梯度。", "result": "实验结果表明，ADC在各种压缩比下都优于现有的SL框架，显著降低了通信开销，同时保持了高精度。在CIFAR100数据集上训练DeiT-S模型时，ADC表现出优越的性能。", "conclusion": "ADC为在通信受限的环境下实现高效的Split Learning提供了有效的解决方案，使得在边缘设备上进行深度学习训练更加可行，并有助于保护用户数据隐私。"}}
{"id": "2509.15045", "title": "Synthetic-to-Real Object Detection using YOLOv11 and Domain Randomization Strategies", "authors": ["Luisa Torquato Niño", "Hamza A. A. Gardi"], "summary": "This paper addresses the synthetic-to-real domain gap in object detection, focusing on training a YOLOv11 model to detect a specific object (a soup can) using only synthetic data and domain randomization strategies. The methodology involves extensive experimentation with data augmentation, dataset composition, and model scaling. While synthetic validation metrics were consistently high, they proved to be poor predictors of real-world performance. Consequently, models were also evaluated qualitatively, through visual inspection of predictions, and quantitatively, on a manually labeled real-world test set, to guide development. Final mAP@50 scores were provided by the official Kaggle competition. Key findings indicate that increasing synthetic dataset diversity, specifically by including varied perspectives and complex backgrounds, combined with carefully tuned data augmentation, were crucial in bridging the domain gap. The best performing configuration, a YOLOv11l model trained on an expanded and diverse dataset, achieved a final mAP@50 of 0.910 on the competition's hidden test set. This result demonstrates the potential of a synthetic-only training approach while also highlighting the remaining challenges in fully capturing real-world variability.", "abs": "", "categories": ["cs.CV", "cs.LG"], "AI": {"tldr": "本文研究了仅使用合成数据和领域随机化策略，利用YOLOv11模型进行汤罐目标检测，并成功在Kaggle竞赛中取得了优异成绩，证明了完全合成训练的可行性。", "motivation": "现实世界中收集和标注大量数据集成本高昂，本文旨在探索使用合成数据和领域随机化策略来解决目标检测中的领域差距问题，降低数据获取成本。", "method": "研究团队使用Falcon的Duality AI模拟器生成合成数据，并对数据增强、数据集组成和模型缩放进行了广泛实验。他们选择了YOLOv11模型，并结合精心调整的数据增强策略，通过视觉检查和真实世界测试集评估模型性能。", "result": "最佳配置（YOLOv11l模型，扩展且多样化的数据集）在竞赛的隐藏测试集中达到了0.910的mAP@50分数。合成验证指标与真实世界性能之间存在差异，需要通过视觉检查和真实数据集评估来指导模型开发。", "conclusion": "研究结果表明，通过增加合成数据集的多样性（包括不同的视角和复杂的背景）以及精心调整数据增强，可以有效弥合领域差距，证明了仅使用合成数据进行训练的可行性，同时也指出了在完全捕捉真实世界变化方面仍然存在的挑战。"}}
{"id": "2509.15156", "title": "Leveraging Geometric Visual Illusions as Perceptual Inductive Biases for Vision Models", "authors": ["Haobo Yang", "Minghao Guo", "Dequan Yang", "Wenyu Wang"], "summary": "Contemporary deep learning models have achieved impressive performance in image classification by primarily leveraging statistical regularities within large datasets, but they rarely incorporate structured insights drawn directly from perceptual psychology. To explore the potential of perceptually motivated inductive biases, we propose integrating classic geometric visual illusions well-studied phenomena from human perception into standard image-classification training pipelines. Specifically, we introduce a synthetic, parametric geometric-illusion dataset and evaluate three multi-source learning strategies that combine illusion recognition tasks with ImageNet classification objectives. Our experiments reveal two key conceptual insights: (i) incorporating geometric illusions as auxiliary supervision systematically improves generalization, especially in visually challenging cases involving intricate contours and fine textures; and (ii) perceptually driven inductive biases, even when derived from synthetic stimuli traditionally considered unrelated to natural image recognition, can enhance the structural sensitivity of both CNN and transformer-based architectures. These results demonstrate a novel integration of perceptual science and machine learning and suggest new directions for embedding perceptual priors into vision model design.", "abs": "", "categories": ["cs.CV", "cs.AI"], "AI": {"tldr": "本文研究了如何将经典的几何视觉错觉融入到图像分类训练中，以提升模型的泛化能力和结构敏感性，尤其是在处理复杂轮廓和纹理时。", "motivation": "现代深度学习模型虽然在图像分类上表现出色，但往往忽略了人类视觉理解中的结构信息和上下文信号。本文旨在探索将基于感知心理学的结构化先验知识融入到视觉模型设计中。", "method": "研究人员构建了一个包含五个经典几何视觉错觉的合成数据集，并设计了三种多源学习策略（单头联合、多头并行和混合策略），将错觉识别任务与ImageNet分类目标相结合进行训练。", "result": "实验结果表明，将几何错觉作为辅助监督信号可以系统地提高模型的泛化能力，尤其是在涉及复杂轮廓和精细纹理的视觉挑战性案例中。即使是源自合成刺激的感知驱动的先验知识，也能增强CNN和Transformer架构对结构的敏感性。", "conclusion": "该研究表明，将感知科学与机器学习相结合，可以设计出更具鲁棒性的视觉模型，并为将感知先验知识嵌入到视觉模型设计中开辟了新的方向。 几何错觉可以帮助视觉模型更稳健地识别结构。"}}
{"id": "2509.15130", "title": "WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance", "authors": ["Chenxi Song", "Yanming Yang", "Tong Zhao", "Ruibo Li", "Chi Zhang"], "summary": "Recent video diffusion models demonstrate strong potential in spatial intelligence tasks due to their rich latent world priors. However, this potential is hindered by their limited controllability and geometric inconsistency, creating a gap between their strong priors and their practical use in 3D/4D tasks. As a result, current approaches often rely on retraining or fine-tuning, which risks degrading pretrained knowledge and incurs high computational costs. To address this, we propose WorldForge, a training-free, inference-time framework composed of three tightly coupled modules. Intra-Step Recursive Refinement introduces a recursive refinement mechanism during inference, which repeatedly optimizes network predictions within each denoising step to enable precise trajectory injection. Flow-Gated Latent Fusion leverages optical flow similarity to decouple motion from appearance in the latent space and selectively inject trajectory guidance into motion-related channels. Dual-Path Self-Corrective Guidance compares guided and unguided denoising paths to adaptively correct trajectory drift caused by noisy or misaligned structural signals. Together, these components inject fine-grained, trajectory-aligned guidance without training, achieving both accurate motion control and photorealistic content generation. Extensive experiments across diverse benchmarks validate our method's superiority in realism, trajectory consistency, and visual fidelity. This work introduces a novel plug-and-play paradigm for controllable video synthesis, offering a new perspective on leveraging generative priors for spatial intelligence.", "abs": "", "categories": ["cs.GR", "cs.AI", "cs.CV"], "AI": {"tldr": "本文提出了WorldForge，一个无需训练的框架，利用预训练的视频扩散模型进行3D/4D任务，如单视角3D场景生成和动态4D场景重渲染，实现精确的相机轨迹控制。", "motivation": "现有视频扩散模型在3D/4D任务中存在可控性差和几何不一致的问题，通常需要重新训练或微调，这会降低预训练知识并增加计算成本。", "method": "WorldForge框架由三个模块组成：步内递归细化（Intra-Step Recursive Refinement）用于精确轨迹注入；流门控潜空间融合（Flow-Gated Latent Fusion）用于解耦运动和外观；双路径自校正引导（Dual-Path Self-Corrective Guidance）用于自适应校正轨迹漂移。这些模块在推理时注入轨迹对齐的引导，无需训练。", "result": "WorldForge在多个基准测试中表现优异，在真实感、轨迹一致性和视觉保真度方面优于现有方法，实现了准确的运动控制和逼真的内容生成。", "conclusion": "WorldForge引入了一种可插拔的、可控的视频合成范式，为利用生成先验进行空间智能提供了一种新视角，为可控视频合成开辟了新的可能性。"}}
{"id": "2509.15011", "title": "Sea-ing Through Scattered Rays: Revisiting the Image Formation Model for Realistic Underwater Image Generation", "authors": ["Vasiliki Ismiroglou", "Malte Pedersen", "Stefan H. Bengtson", "Andreas Aakerberg", "Thomas B. Moeslund"], "summary": "In recent years, the underwater image formation model has found extensive use in the generation of synthetic underwater data. Although many approaches focus on scenes primarily affected by discoloration, they often overlook the model's ability to capture the complex, distance-dependent visibility loss present in highly turbid environments. In this work, we propose an improved synthetic data generation pipeline that includes the commonly omitted forward scattering term, while also considering a nonuniform medium. Additionally, we collected the BUCKET dataset under controlled turbidity conditions to acquire real turbid footage with the corresponding reference images. Our results demonstrate qualitative improvements over the reference model, particularly under increasing turbidity, with a selection rate of 82. 5\\% by survey participants. Data and code can be accessed on the project page: vap.aau.dk/sea-ing-through-scattered-rays.", "abs": "", "categories": ["cs.CV", "cs.AI"], "AI": {"tldr": "本文提出了改进的合成水下图像生成流程，考虑了前向散射和非均匀介质的影响，并收集了具有控制水浑浊度的 BUCKET 数据集，在水下图像生成中取得了显著的改进。", "motivation": "现有水下图像生成模型通常忽略了水下复杂的水浑浊环境，尤其是在高浑浊度环境下，前向散射的影响不可忽略。数据收集困难导致水下数据集稀缺，合成数据生成被认为是解决数据稀缺性的有效方法。", "method": "本文改进了水下图像生成模型（IFM），加入了常用的前向散射项，并考虑了非均匀介质。同时，收集了 BUCKET 数据集，该数据集在控制的水浑浊度条件下获取了真实的水下视频和对应的参考图像。", "result": "实验结果表明，改进后的模型在水下图像生成中，尤其是在水浑浊度增加的情况下，优于现有模型，调查参与者选择率达到 82.5%。", "conclusion": "本文的改进模型能够更真实地模拟高浑浊度水下环境，为水下计算机视觉领域的数据集扩充和算法研究提供了有价值的资源，有助于推动水下生态评估和保护工作。"}}
{"id": "2509.14998", "title": "A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical Decision-making", "authors": ["Xiao Wu", "Ting-Zhu Huang", "Liang-Jian Deng", "Yanyuan Qiao", "Imran Razzak", "Yutong Xie"], "summary": "Medical decision-making often involves integrating knowledge from multiple clinical specialties, typically achieved through multidisciplinary teams. Inspired by this collaborative process, recent work has leveraged large language models (LLMs) in multi-agent collaboration frameworks to emulate expert teamwork. While these approaches improve reasoning through agent interaction, they are limited by static, pre-assigned roles, which hinder adaptability and dynamic knowledge integration. To address these limitations, we propose KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration framework that enables LLM agents to dynamically form and expand expert teams based on the evolving diagnostic context. KAMAC begins with one or more expert agents and then conducts a knowledge-driven discussion to identify and fill knowledge gaps by recruiting additional specialists as needed. This supports flexible, scalable collaboration in complex clinical scenarios, with decisions finalized through reviewing updated agent comments. Experiments on two real-world medical benchmarks demonstrate that KAMAC significantly outperforms both single-agent and advanced multi-agent methods, particularly in complex clinical scenarios (i.e., cancer prognosis) requiring dynamic, cross-specialty expertise. Our code is publicly available at: https://github.com/XiaoXiao-Woo/KAMAC.", "abs": "", "categories": ["cs.AI", "cs.CV"], "AI": {"tldr": "本文提出了一种名为KAMAC的知识驱动自适应多智能体协作框架，旨在通过动态组建和扩展专家团队来提升医学决策能力，克服了传统多智能体协作中角色静态分配的局限性。", "motivation": "传统的医学决策通常需要多学科团队协作，而现有的大语言模型多智能体协作框架存在角色静态分配，难以适应动态的知识整合需求，因此需要一种更具适应性的协作方法。", "method": "KAMAC框架通过知识驱动的讨论，动态地识别知识差距并招募合适的专家，从而灵活地构建专家团队。该框架从一个或多个专家智能体开始，根据诊断上下文的变化进行专家招募和团队扩展，最终通过审查智能体评论来做出决策。", "result": "在两个真实的医学benchmark上的实验表明，KAMAC显著优于单智能体和先进的多智能体方法，尤其是在需要动态跨学科专业知识的复杂临床场景（如癌症预后）中表现更佳。", "conclusion": "KAMAC框架为医学决策支持提供了一种新的思路，通过知识驱动的自适应协作，能够更有效地应对复杂临床问题，有望提升医疗决策的质量和效率。"}}
{"id": "2509.14980", "title": "M4Diffuser: Multi-View Diffusion Policy with Manipulability-Aware Control for Robust Mobile Manipulation", "authors": ["Ju Dong", "Lei Zhang", "Liding Zhang", "Yao Ling", "Yu Fu", "Kaixin Bai", "Zoltán-Csaba Márton", "Zhenshan Bing", "Zhaopeng Chen", "Alois Christian Knoll", "Jianwei Zhang"], "summary": "Mobile manipulation requires the coordinated control of a mobile base and a robotic arm while simultaneously perceiving both global scene context and fine-grained object details. Existing single-view approaches often fail in unstructured environments due to limited fields of view, exploration, and generalization abilities. Moreover, classical controllers, although stable, struggle with efficiency and manipulability near singularities. To address these challenges, we propose M4Diffuser, a hybrid framework that integrates a Multi-View Diffusion Policy with a novel Reduced and Manipulability-aware QP (ReM-QP) controller for mobile manipulation. The diffusion policy leverages proprioceptive states and complementary camera perspectives with both close-range object details and global scene context to generate task-relevant end-effector goals in the world frame. These high-level goals are then executed by the ReM-QP controller, which eliminates slack variables for computational efficiency and incorporates manipulability-aware preferences for robustness near singularities. Comprehensive experiments in simulation and real-world environments show that M4Diffuser achieves 7 to 56 percent higher success rates and reduces collisions by 3 to 31 percent over baselines. Our approach demonstrates robust performance for smooth whole-body coordination, and strong generalization to unseen tasks, paving the way for reliable mobile manipulation in unstructured environments. Details of the demo and supplemental material are available on our project website https://sites.google.com/view/m4diffuser.", "abs": "", "categories": ["cs.RO", "cs.AI", "cs.CV"], "AI": {"tldr": "M4Diffuser是一个混合框架，结合了多视角扩散策略和改进的QP控制器，旨在提高移动机器人的鲁棒性和效率，尤其是在复杂环境中。", "motivation": "现有移动操作方法存在局限性：传统优化控制器计算效率低，操纵性差；学习型方法缺乏稳定性，且单视角感知能力有限，难以捕捉全局场景和精细物体细节。", "method": "M4Diffuser采用了一种混合方法：首先，多视角扩散策略利用本体状态和互补的摄像头视角生成端执行器目标；然后，改进的QP控制器（ReM-QP）通过移除松弛变量和考虑操纵性来高效地执行这些目标。", "result": "实验表明，M4Diffuser在模拟和真实环境中，成功率提高了7%-56%，碰撞减少了3%-31%，展现了流畅的整体协调性和对未知任务的泛化能力。", "conclusion": "M4Diffuser为在非结构化环境中实现可靠的移动操作铺平了道路，通过结合扩散策略和优化控制，提升了鲁棒性、效率和泛化能力。"}}
{"id": "2509.14966", "title": "RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching", "authors": ["Xingwu Zhang", "Guanxuan Li", "Zhuocheng Zhang", "Zijun Long"], "summary": "The rapidly growing number of product categories in large-scale e-commerce makes accurate object identification for automated packing in warehouses substantially more difficult. As the catalog grows, intra-class variability and a long tail of rare or visually similar items increase, and when combined with diverse packaging, cluttered containers, frequent occlusion, and large viewpoint changes-these factors amplify discrepancies between query and reference images, causing sharp performance drops for methods that rely solely on 2D appearance features. Thus, we propose RoboEye, a two-stage identification framework that dynamically augments 2D semantic features with domain-adapted 3D reasoning and lightweight adapters to bridge training deployment gaps. In the first stage, we train a large vision model to extract 2D features for generating candidate rankings. A lightweight 3D-feature-awareness module then estimates 3D feature quality and predicts whether 3D re-ranking is necessary, preventing performance degradation and avoiding unnecessary computation. When invoked, the second stage uses our robot 3D retrieval transformer, comprising a 3D feature extractor that produces geometry-aware dense features and a keypoint-based matcher that computes keypoint-correspondence confidences between query and reference images instead of conventional cosine-similarity scoring. Experiments show that RoboEye improves Recall@1 by 7.1% over the prior state of the art (RoboLLM). Moreover, RoboEye operates using only RGB images, avoiding reliance on explicit 3D inputs and reducing deployment costs. The code used in this paper is publicly available at: https://github.com/longkukuhi/RoboEye.", "abs": "", "categories": ["cs.CV", "cs.AI", "cs.RO"], "AI": {"tldr": "RoboEye是一种两阶段的物体识别框架，通过动态地将2D语义特征与领域自适应的3D推理相结合，显著提升了在大型仓库环境中物体识别的准确率，尤其是在存在视角变化、遮挡和包装差异的情况下。", "motivation": "随着电商产品目录的快速增长，仓库自动化中的物体识别面临着更大的挑战，传统的仅依赖2D外观特征的方法在复杂仓库环境中性能下降，因为它们容易受到视角变化、遮挡、包装差异和长尾分布的影响。", "method": "RoboEye框架包含两个阶段：首先，使用大型视觉模型提取2D特征进行候选排序；然后，一个轻量级的3D特征感知模块评估3D特征质量并决定是否需要3D重新排序。如果需要，则使用机器人3D检索Transformer进行3D特征提取和基于关键点的匹配，计算查询和参考图像之间的关键点对应关系。", "result": "实验结果表明，RoboEye在Recall@1指标上比之前的最先进方法（RoboLLM）提高了7.1%，并且仅使用RGB图像，无需显式3D输入，降低了部署成本。", "conclusion": "RoboEye通过结合2D语义特征和3D几何特征，有效解决了传统2D特征方法在复杂仓库环境中的泛化能力问题，为自动化仓库中的物体识别提供了更可靠和经济的解决方案。"}}
{"id": "2509.14860", "title": "MARIC: Multi-Agent Reasoning for Image Classification", "authors": ["Wonduk Seo", "Minhyeong Yu", "Hyunjin An", "Seunghyun Lee"], "summary": "Image classification has traditionally relied on parameter-intensive model training, requiring large-scale annotated datasets and extensive fine tuning to achieve competitive performance. While recent vision language models (VLMs) alleviate some of these constraints, they remain limited by their reliance on single pass representations, often failing to capture complementary aspects of visual content. In this paper, we introduce Multi Agent based Reasoning for Image Classification (MARIC), a multi agent framework that reformulates image classification as a collaborative reasoning process. MARIC first utilizes an Outliner Agent to analyze the global theme of the image and generate targeted prompts. Based on these prompts, three Aspect Agents extract fine grained descriptions along distinct visual dimensions. Finally, a Reasoning Agent synthesizes these complementary outputs through integrated reflection step, producing a unified representation for classification. By explicitly decomposing the task into multiple perspectives and encouraging reflective synthesis, MARIC mitigates the shortcomings of both parameter-heavy training and monolithic VLM reasoning. Experiments on 4 diverse image classification benchmark datasets demonstrate that MARIC significantly outperforms baselines, highlighting the effectiveness of multi-agent visual reasoning for robust and interpretable image classification.", "abs": "", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MA"], "AI": {"tldr": "MARIC是一种基于多智能体框架的图像分类方法，它将图像分类任务分解为协作推理过程，显著提升了图像分类的准确性和可解释性。", "motivation": "传统图像分类方法依赖于参数密集的模型训练和大规模标注数据集，而现有的视觉-语言模型（VLMs）在捕捉图像内容的互补方面存在局限性，MARIC旨在解决这些问题。", "method": "MARIC框架包含三个主要智能体：Outliner Agent（分析全局主题并生成提示）、Aspect Agents（提取细粒度的视觉描述）和Reasoning Agent（综合各智能体的输出并进行反思）。通过分解任务并鼓励反思性综合，MARIC实现了更强大的图像分类。", "result": "在四个不同的图像分类基准数据集上的实验表明，MARIC显著优于基线模型，证明了多智能体视觉推理的有效性。", "conclusion": "MARIC为图像分类提供了一种可扩展且可解释的范式，超越了传统的训练密集型或单次推理方法，为未来的图像分类研究提供了新的思路。"}}
{"id": "2509.15207", "title": "FlowRL: Matching Reward Distributions for LLM Reasoning", "authors": ["Xuekai Zhu", "Daixuan Cheng", "Dinghuai Zhang", "Hengli Li", "Kaiyan Zhang", "Che Jiang", "Youbang Sun", "Ermo Hua", "Yuxin Zuo", "Xingtai Lv", "Qizheng Zhang", "Lin Chen", "Fanghao Shao", "Bo Xue", "Yunchong Song", "Zhenjie Yang", "Ganqu Cui", "Ning Ding", "Jianfeng Gao", "Xiaodong Liu", "Bowen Zhou", "Hongyuan Mei", "Zhouhan Lin"], "summary": "We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning (RL). Recent advanced reasoning models adopt reward-maximizing methods (\\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, thus reducing diversity. In contrast, we transform scalar rewards into a normalized target distribution using a learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution. We implement this idea as a flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories. We conduct experiments on math and code reasoning tasks: FlowRL achieves a significant average improvement of $10.0\\%$ over GRPO and $5.1\\%$ over PPO on math benchmarks, and performs consistently better on code reasoning tasks. These results highlight reward distribution-matching as a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning.", "abs": "", "categories": ["cs.LG", "cs.AI", "cs.CL"], "AI": {"tldr": "FlowRL是一种新的LLM强化学习算法，它通过匹配奖励分布而不是最大化奖励来促进多样化的推理，从而避免了传统方法中的模式坍塌。", "motivation": "现有的基于奖励最大化的LLM强化学习方法（如PPO和GRPO）容易过度优化主导奖励信号，忽略其他有效的推理路径，导致推理路径多样性降低，泛化能力不足。", "method": "FlowRL将标量奖励转换为可学习的划分函数生成的归一化目标分布，并通过最小化策略与目标分布之间的逆KL散度来实现奖励分布匹配，从而实现流平衡优化。", "result": "在数学和代码推理任务上，FlowRL相比于GRPO和PPO分别平均提升了10.0%和5.1%，并在代码推理任务中表现更佳。", "conclusion": "FlowRL的奖励分布匹配方法是实现LLM强化学习中高效探索和多样化推理的关键一步，有效解决了传统奖励最大化方法中的模式坍塌问题。"}}
{"id": "2509.15194", "title": "Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation", "authors": ["Yujun Zhou", "Zhenwen Liang", "Haolin Liu", "Wenhao Yu", "Kishan Panaganti", "Linfeng Song", "Dian Yu", "Xiangliang Zhang", "Haitao Mi", "Dong Yu"], "summary": "Large language models (LLMs) are increasingly trained with reinforcement learning from verifiable rewards (RLVR), yet real-world deployment demands models that can self-improve without labels or external judges. Existing label-free methods, confidence minimization, self-consistency, or majority-vote objectives, stabilize learning but steadily shrink exploration, causing an entropy collapse: generations become shorter, less diverse, and brittle. Unlike prior approaches such as Test-Time Reinforcement Learning (TTRL), which primarily adapt models to the immediate unlabeled dataset at hand, our goal is broader: to enable general improvements without sacrificing the model's inherent exploration capacity and generalization ability, i.e., evolving. We formalize this issue and propose EVolution-Oriented and Label-free Reinforcement Learning (EVOL-RL), a simple rule that couples stability with variation under a label-free setting. EVOL-RL keeps the majority-voted answer as a stable anchor (selection) while adding a novelty-aware reward that favors responses whose reasoning differs from what has already been produced (variation), measured in semantic space. Implemented with GRPO, EVOL-RL also uses asymmetric clipping to preserve strong signals and an entropy regularizer to sustain search. This majority-for-selection + novelty-for-variation design prevents collapse, maintains longer and more informative chains of thought, and improves both pass@1 and pass@n. EVOL-RL consistently outperforms the majority-only TTRL baseline; e.g., training on label-free AIME24 lifts Qwen3-4B-Base AIME25 pass@1 from TTRL's 4.6% to 16.4%, and pass@16 from 18.5% to 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks stronger generalization across domains (e.g., GPQA). Furthermore, we demonstrate that EVOL-RL also boosts performance in the RLVR setting, highlighting its broad applicability.", "abs": "", "categories": ["cs.LG", "cs.CL"], "AI": {"tldr": "本文提出了一种名为EVOL-RL的无标签强化学习方法，通过平衡选择（基于多数票）和变化（基于新颖性）来避免语言模型在自我进化过程中出现的熵塌缩问题，从而提升模型的泛化能力和性能。", "motivation": "现有无标签强化学习方法虽然能稳定学习，但往往导致模型探索能力下降，产生熵塌缩，使得生成结果更短、多样性降低、鲁棒性差。研究旨在开发能够自主进化、持续从海量无标签数据中学习的大型语言模型，解决探索与利用的平衡问题。", "method": "EVOL-RL将选择（selection）与变化（variation）相结合，选择多数票的答案作为稳定锚点，并引入新颖性奖励，鼓励生成与已有答案不同的推理过程。该方法结合了GRPO算法、非对称裁剪和熵正则化，以防止熵塌缩，维持搜索。", "result": "实验表明，EVOL-RL在AIME24数据集上显著优于仅使用多数票的TTRL基线，将Qwen3-4B-Base在AIME25上的pass@1从4.6%提升至16.4%，pass@16从18.5%提升至37.9%。EVOL-RL不仅避免了多样性塌缩，还提升了跨领域（如GPQA）的泛化能力，并在RLVR设置下也能提升性能。", "conclusion": "EVOL-RL提供了一种有效的方法，能够在无标签设置下实现语言模型的自主进化，提升模型的性能和泛化能力，为构建更智能的语言模型奠定基础。"}}
{"id": "2509.15188", "title": "Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning", "authors": ["Yeongbin Seo", "Dongha Lee", "Jaehyung Kim", "Jinyoung Yeo"], "summary": "Autoregressive (AR) language models generate text one token at a time, which limits their inference speed. Diffusion-based language models offer a promising alternative, as they can decode multiple tokens in parallel. However, we identify a key bottleneck in current diffusion LMs: the long decoding-window problem, where tokens generated far from the input context often become irrelevant or repetitive. Previous solutions like semi-autoregressive address this issue by splitting windows into blocks, but this sacrifices speed and bidirectionality, eliminating the main advantage of diffusion models. To overcome this, we propose Convolutional decoding (Conv), a normalization-based method that narrows the decoding window without hard segmentation, leading to better fluency and flexibility. Additionally, we introduce Rejecting Rule-based Fine-Tuning (R2FT), a post-hoc training scheme that better aligns tokens at positions far from context. Our methods achieve state-of-the-art results on open-ended generation benchmarks (e.g., AlpacaEval) among diffusion LM baselines, with significantly lower step size than previous works, demonstrating both speed and quality improvements.", "abs": "", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7"], "AI": {"tldr": "本文提出了一种新的扩散语言模型解码方法，通过卷积解码 (Conv) 缩小解码窗口并结合拒绝规则的微调 (R2FT)，解决了扩散语言模型中长解码窗口问题，在保证速度和灵活性的同时提升了文本生成质量。", "motivation": "现有扩散语言模型存在长解码窗口问题，导致生成文本与输入上下文关联性差，且半自回归方法虽然能缓解此问题，但牺牲了速度和双向性优势。因此，需要一种既能解决长解码窗口问题，又能保持扩散模型速度和双向性的方法。", "method": "本文提出了两种方法：卷积解码 (Conv) 使用归一化缩小解码窗口，避免了硬分割带来的质量下降；拒绝规则的微调 (R2FT) 是一种后期训练方案，用于更好地对齐远离上下文的token。", "result": "实验结果表明，结合卷积解码和拒绝规则微调的方法在开放式文本生成基准测试中取得了最先进的结果，并且在更少的步数下实现了更高的质量，证明了速度和质量的双重提升。", "conclusion": "本文提出的卷积解码和拒绝规则微调方法有效解决了扩散语言模型中的长解码窗口问题，提升了文本生成质量，为扩散语言模型在开放式文本生成任务中的应用提供了新的思路。"}}
{"id": "2509.15157", "title": "Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning", "authors": ["Shiwan Zhao", "Xuyang Zhao", "Jiaming Zhou", "Aobo Kong", "Qicheng Li", "Yong Qin"], "summary": "Supervised fine-tuning (SFT) of large language models can be viewed as an off-policy learning problem, where expert demonstrations come from a fixed behavior policy while training aims to optimize a target policy. Importance sampling is the standard tool for correcting this distribution mismatch, but large policy gaps lead to high variance and training instability. Existing approaches mitigate this issue using KL penalties or clipping, which passively constrain updates rather than actively reducing the gap. We propose a simple yet effective data rewriting framework that proactively shrinks the policy gap by keeping correct solutions as on-policy data and rewriting incorrect ones with guided re-solving, falling back to expert demonstrations only when needed. This aligns the training distribution with the target policy before optimization, reducing importance sampling variance and stabilizing off-policy fine-tuning. Experiments on five mathematical reasoning benchmarks demonstrate consistent and significant gains over both vanilla SFT and the state-of-the-art Dynamic Fine-Tuning (DFT) approach. The data and code will be released at https://github.com/NKU-HLT/Off-Policy-SFT.", "abs": "", "categories": ["cs.LG", "cs.CL"], "AI": {"tldr": "本文提出了一种数据重写框架，通过主动缩小策略差距来稳定离策略监督微调，显著降低了训练变异性，并在数学推理任务上取得了优异效果。", "motivation": "传统的监督微调（SFT）存在策略差距问题，导致训练不稳定和过拟合。现有方法主要通过KL惩罚或裁剪来被动约束更新，未能有效解决根本的策略差距。", "method": "该框架通过数据重写来主动缩小策略差距：如果模型生成正确的答案，则保留；否则，利用正确答案引导模型重新解决问题（digest-and-retell），如果仍然失败，则回退到专家演示。", "result": "在五个数学推理基准测试中，该方法在原始SFT和最先进的动态微调（DFT）方法上都取得了显著且一致的性能提升。", "conclusion": "数据重写框架通过对训练数据进行预处理，更紧密地对齐了训练分布和目标策略，从而降低了重要性抽样方差，稳定了离策略微调，为后续的RL或混合方法奠定了更稳定的基础。"}}
{"id": "2509.15110", "title": "TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference", "authors": ["Dan Zhang", "Min Cai", "Jonathan Li", "Ziniu Hu", "Yisong Yue", "Yuxiao Dong", "Jie Tang"], "summary": "Reward models are central to both reinforcement learning (RL) with language models and inference-time verification. However, existing reward models often lack temporal consistency, leading to ineffective policy updates and unstable RL training. We introduce TDRM, a method for learning smoother and more reliable reward models by minimizing temporal differences during training. This temporal-difference (TD) regularization produces smooth rewards and improves alignment with long-term objectives. Incorporating TDRM into the actor-critic style online RL loop yields consistent empirical gains. It is worth noting that TDRM is a supplement to verifiable reward methods, and both can be used in series. Experiments show that TD-trained process reward models (PRMs) improve performance across Best-of-N (up to 6.6%) and tree-search (up to 23.7%) settings. When combined with Reinforcement Learning with Verifiable Rewards (RLVR), TD-trained PRMs lead to more data-efficient RL -- achieving comparable performance with just 2.5k data to what baseline methods require 50.1k data to attain -- and yield higher-quality language model policies on 8 model variants (5 series), e.g., Qwen2.5-(0.5B, 1,5B), GLM4-9B-0414, GLM-Z1-9B-0414, Qwen2.5-Math-(1.5B, 7B), and DeepSeek-R1-Distill-Qwen-(1.5B, 7B). We release all code at https://github.com/THUDM/TDRM.", "abs": "", "categories": ["cs.LG", "cs.CL"], "AI": {"tldr": "本文提出了TDRM方法，通过最小化时间差正则化来学习更平滑、更可靠的奖励模型，从而提升LLM强化学习和推理的性能。", "motivation": "现有奖励模型缺乏时间一致性，导致策略更新无效和强化学习训练不稳定，难以区分推理过程中的有益和非最优步骤。", "method": "TDRM采用n步时间差学习（TD learning）对过程奖励模型（PRM）进行训练，最小化奖励模型在相邻时间步之间的差异，并可与可验证奖励方法结合使用。", "result": "实验表明，TDRM在Best-of-N和树搜索设置中提升了性能（分别提升6.6%和23.7%），与RLVR结合使用时，数据效率显著提高（仅需2.5k数据达到baseline 50.1k数据所达到的效果），并在8种模型变体上实现了更高质量的语言模型策略。", "conclusion": "TDRM提供了一种有效的方法来提升奖励模型的平滑性和可靠性，从而改善LLM的强化学习训练和推理能力，为LLM的数学推理、代码合成和指令遵循等任务提供了新的思路。"}}
{"id": "2509.14926", "title": "Patent Language Model Pretraining with ModernBERT", "authors": ["Amirhossein Yousefiramandi", "Ciaran Cooney"], "summary": "Transformer-based language models such as BERT have become foundational in NLP, yet their performance degrades in specialized domains like patents, which contain long, technical, and legally structured text. Prior approaches to patent NLP have primarily relied on fine-tuning general-purpose models or domain-adapted variants pretrained with limited data. In this work, we pretrain 3 domain-specific masked language models for patents, using the ModernBERT architecture and a curated corpus of over 60 million patent records. Our approach incorporates architectural optimizations, including FlashAttention, rotary embeddings, and GLU feed-forward layers. We evaluate our models on four downstream patent classification tasks. Our model, ModernBERT-base-PT, consistently outperforms the general-purpose ModernBERT baseline on three out of four datasets and achieves competitive performance with a baseline PatentBERT. Additional experiments with ModernBERT-base-VX and Mosaic-BERT-large demonstrate that scaling the model size and customizing the tokenizer further enhance performance on selected tasks. Notably, all ModernBERT variants retain substantially faster inference over - 3x that of PatentBERT - underscoring their suitability for time-sensitive applications. These results underscore the benefits of domain-specific pretraining and architectural improvements for patent-focused NLP tasks.", "abs": "", "categories": ["cs.CL", "cs.AI", "cs.LG"], "AI": {"tldr": "本文提出了针对专利领域的ModernBERT预训练模型，通过架构优化和大规模专利数据训练，在专利分类任务上取得了优于通用BERT和PatentBERT的性能，同时保持了更快的推理速度。", "motivation": "通用语言模型在专利等专业领域表现不佳，现有专利NLP方法主要依赖于通用模型的微调或有限数据预训练。因此，需要针对专利领域进行专业化的预训练。", "method": "使用ModernBERT架构，结合FlashAttention、旋转嵌入和GLU前馈层等架构优化，在一个包含超过6000万专利记录的大规模语料库上进行预训练，并应用于四个专利分类任务。", "result": "ModernBERT-base-PT模型在三个数据集上优于通用ModernBERT基线，并在一个数据集上与PatentBERT表现相当。更大模型和自定义分词器进一步提升了性能，所有ModernBERT变体都具有超过3倍的推理速度。", "conclusion": "本文证明了领域特定预训练和架构改进对于专利相关NLP任务具有显著优势，并为构建高效且高性能的专利NLP模型提供了新的思路。"}}
{"id": "2509.15174", "title": "SMARTER: A Data-efficient Framework to Improve Toxicity Detection with Explanation via Self-augmenting Large Language Models", "authors": ["Huy Nghiem", "Advik Sachdeva", "Hal Daumé III"], "summary": "WARNING: This paper contains examples of offensive materials. Toxic content has become pervasive on social media platforms. We introduce SMARTER, a data-efficient two-stage framework for explainable content moderation using Large Language Models (LLMs). In Stage 1, we leverage LLMs' own outputs to generate synthetic explanations for both correct and incorrect labels, enabling alignment via preference optimization with minimal human supervision. In Stage 2, we refine explanation quality through cross-model training, allowing weaker models to align stylistically and semantically with stronger ones. Experiments on three benchmark tasks -- HateXplain, Latent Hate, and Implicit Hate -- demonstrate that SMARTER enables LLMs to achieve up to a 13.5% macro-F1 improvement over standard few-shot baselines while using only a fraction of the full training data. Our framework offers a scalable strategy for low-resource settings by harnessing LLMs' self-improving capabilities for both classification and explanation.", "abs": "", "categories": ["cs.CL", "cs.AI"], "AI": {"tldr": "本文提出了SMARTER框架，利用大型语言模型（LLMs）进行数据高效且可解释的内容审核。该框架通过自我增强和跨模型训练，在低资源环境下显著提升了毒性检测的性能和可解释性。", "motivation": "社交媒体上毒性内容泛滥，传统的内容审核依赖人工，效率低下且成本高昂。现有机器学习模型需要大量数据进行训练，且缺乏可解释性，难以获得用户的信任。", "method": "SMARTER是一个两阶段框架：第一阶段，LLMs利用自身输出生成合成解释，通过偏好优化与少量人工监督进行对齐；第二阶段，通过跨模型训练，使较弱的模型在风格和语义上与较强的模型对齐。", "result": "在HateXplain、Latent Hate和Implicit Hate三个基准数据集上，SMARTER在少量样本设置下，将LLMs的macro-F1得分提高了高达13.5%，且仅使用了少量训练数据。", "conclusion": "SMARTER提供了一种可扩展的策略，可以在低资源设置下利用LLMs的自我改进能力，实现高性能和可解释的内容审核，为NLP实践者提供了一种实用方法。"}}
{"id": "2509.15098", "title": "TextMine: LLM-Powered Knowledge Extraction for Humanitarian Mine Action", "authors": ["Chenyue Zhou", "Gürkan Solmaz", "Flavio Cirillo", "Kiril Gashteovski", "Jonathan Fürst"], "summary": "Humanitarian Mine Action has generated extensive best-practice knowledge, but much remains locked in unstructured reports. We introduce TextMine, an ontology-guided pipeline that uses Large Language Models to extract knowledge triples from HMA texts. TextMine integrates document chunking, domain-aware prompting, triple extraction, and both reference-based and LLM-as-a-Judge evaluation. We also create the first HMA ontology and a curated dataset of real-world demining reports. Experiments show ontology-aligned prompts boost extraction accuracy by 44.2%, cut hallucinations by 22.5%, and improve format conformance by 20.9% over baselines. While validated on Cambodian reports, TextMine can adapt to global demining efforts or other domains, transforming unstructured data into structured knowledge.", "abs": "", "categories": ["cs.CL", "cs.AI"], "AI": {"tldr": "本文提出了TextMine，一个基于大型语言模型(LLM)的知识抽取流程，用于从人道主义扫雷行动(HMA)报告中提取结构化知识，并构建了第一个HMA本体和数据集。", "motivation": "人道主义扫雷行动积累了大量最佳实践知识，但这些知识大多存在于非结构化报告中，难以利用。本文旨在将这些非结构化数据转化为结构化知识，以改进决策和提高运营效率。", "method": "TextMine采用了一种本体引导的流程，结合了文档分块、领域知识提示、三元组抽取以及基于参考和LLM-as-a-Judge的评估方法。该流程能够处理段落级别的文本，支持指代消解和多步推理。", "result": "实验结果表明，本体引导的提示可以提高抽取准确率44.2%，减少幻觉22.5%，并提高格式一致性20.9%。TextMine在柬埔寨报告上的验证结果表明其有效性。", "conclusion": "TextMine为将非结构化数据转化为结构化知识提供了一种有效的方法，可以应用于全球扫雷行动或其他领域，并有望促进人道主义扫雷行动的知识共享和学习。"}}
{"id": "2509.15027", "title": "CLEAR: A Comprehensive Linguistic Evaluation of Argument Rewriting by Large Language Models", "authors": ["Thomas Huber", "Christina Niklaus"], "summary": "While LLMs have been extensively studied on general text generation tasks, there is less research on text rewriting, a task related to general text generation, and particularly on the behavior of models on this task. In this paper we analyze what changes LLMs make in a text rewriting setting. We focus specifically on argumentative texts and their improvement, a task named Argument Improvement (ArgImp). We present CLEAR: an evaluation pipeline consisting of 57 metrics mapped to four linguistic levels: lexical, syntactic, semantic and pragmatic. This pipeline is used to examine the qualities of LLM-rewritten arguments on a broad set of argumentation corpora and compare the behavior of different LLMs on this task and analyze the behavior of different LLMs on this task in terms of linguistic levels. By taking all four linguistic levels into consideration, we find that the models perform ArgImp by shortening the texts while simultaneously increasing average word length and merging sentences. Overall we note an increase in the persuasion and coherence dimensions.", "abs": "", "categories": ["cs.CL", "cs.AI"], "AI": {"tldr": "本文提出了CLEAR评估框架，用于全面评估大型语言模型（LLMs）在论证改进（ArgImp）任务中的行为，并发现模型倾向于缩短文本、增加平均单词长度和合并句子，从而提高论证的说服力和连贯性。", "motivation": "虽然LLMs在通用文本生成任务中表现出色，但在文本改写，尤其是论证改进方面的研究相对较少。本文旨在填补这一空白，分析LLMs在ArgImp任务中的具体改写行为。", "method": "研究人员构建了CLEAR评估框架，包含57个指标，涵盖词汇、句法、语义和语用四个语言层次。该框架应用于多个论证语料库，比较不同LLMs的行为，并分析其在不同语言层次上的表现。", "result": "研究发现，LLMs在进行ArgImp时，主要通过缩短文本、增加平均单词长度和合并句子来改进论证。同时，论证的说服力和连贯性也得到了提高。", "conclusion": "CLEAR框架为评估LLMs在论证改进任务中的表现提供了一个全面的工具，有助于更好地理解LLMs在论证写作中的作用，并为提升论证质量提供指导。"}}
{"id": "2509.14930", "title": "Cross-Modal Knowledge Distillation for Speech Large Language Models", "authors": ["Enzhi Wang", "Qicheng Li", "Zhiyuan Tang", "Yuhang Jia"], "summary": "In this work, we present the first systematic evaluation of catastrophic forgetting and modality inequivalence in speech large language models, showing that introducing speech capabilities can degrade knowledge and reasoning even when inputs remain textual, and performance further decreases with spoken queries. To address these challenges, we propose a cross-modal knowledge distillation framework that leverages both text-to-text and speech-to-text channels to transfer knowledge from a text-based teacher model to a speech LLM. Extensive experiments on dialogue and audio understanding tasks validate the effectiveness of our approach in preserving textual knowledge, improving cross-modal alignment, and enhancing reasoning in speech-based interactions.", "abs": "", "categories": ["cs.CL", "cs.AI"], "AI": {"tldr": "本文研究了语音大型语言模型（Speech LLMs）在引入语音能力后出现的灾难性遗忘和跨模态不一致问题，并提出了一种跨模态知识蒸馏框架来解决这些问题，从而保留文本知识并提升语音交互中的推理能力。", "motivation": "语音LLMs在引入语音能力后，文本知识和推理能力会下降，尤其是在语音查询时。这源于灾难性遗忘和模态不一致，即语音和文本模态之间的对齐不足。", "method": "提出了一种跨模态知识蒸馏框架，利用文本到文本和语音到文本两种通道，将文本教师模型的知识迁移到语音LLM。该框架在对话和音频理解任务中进行训练。", "result": "实验结果表明，该方法有效保留了文本知识，改善了跨模态对齐，并提升了语音交互中的推理能力。在VoiceBench数据集上，语音LLM的性能得到了显著提升。", "conclusion": "该研究首次系统地评估了语音LLMs中的灾难性遗忘和模态不一致问题，并提供了一种有效的解决方案，为构建更强大的语音交互系统奠定了基础。"}}
{"id": "2509.14886", "title": "A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation", "authors": ["Ye Shen", "Junying Wang", "Farong Wen", "Yijin Guo", "Qi Jia", "Zicheng Zhang", "Guangtao Zhai"], "summary": "The rapid progress of Multi-Modal Large Language Models (MLLMs) has spurred the creation of numerous benchmarks. However, conventional full-coverage Question-Answering evaluations suffer from high redundancy and low efficiency. Inspired by human interview processes, we propose a multi-to-one interview paradigm for efficient MLLM evaluation. Our framework consists of (i) a two-stage interview strategy with pre-interview and formal interview phases, (ii) dynamic adjustment of interviewer weights to ensure fairness, and (iii) an adaptive mechanism for question difficulty-level chosen. Experiments on different benchmarks show that the proposed paradigm achieves significantly higher correlation with full-coverage results than random sampling, with improvements of up to 17.6% in PLCC and 16.7% in SRCC, while reducing the number of required questions. These findings demonstrate that the proposed paradigm provides a reliable and efficient alternative for large-scale MLLM benchmarking.", "abs": "", "categories": ["cs.CL", "cs.AI"], "AI": {"tldr": "本文提出了一种“多对一”面试范式，用于更高效地评估多模态大型语言模型（MLLM），通过两阶段面试、动态调整评估者权重和自适应难度调整，显著提高了评估效率和准确性。", "motivation": "现有MLLM评估方法（全覆盖问答）存在冗余问题，效率低下。受现实世界面试启发，本文旨在开发一种更高效、可靠的MLLM评估范式。", "method": "本文提出的“多对一”面试范式包含三个关键组件：(1) 两阶段面试策略（预面试和正式面试）；(2) 动态调整评估者权重以确保公平性；(3) 自适应难度机制，根据面试表现调整问题难度。", "result": "实验结果表明，该范式在MMT-Bench、ScienceQA和SEED-Bench等基准测试中，与随机抽样相比，PLCC和SRCC分别提高了17.6%和16.7%，同时减少了所需问题数量，与全覆盖问答测试相比也取得了显著提升。", "conclusion": "本文提出的“多对一”面试范式为大规模MLLM基准测试提供了一种可靠、高效的替代方案，能够更全面、准确、公平地评估MLLM的能力。"}}
{"id": "2509.14851", "title": "Empathy-R1: A Chain-of-Empathy and Reinforcement Learning Framework for Long-Form Mental Health Support", "authors": ["Xianrong Yao", "Dong She", "Chenxu Zhang", "Yimeng Zhang", "Yueru Sun", "Noman Ahmed", "Yang Gao", "Zhanpeng Jin"], "summary": "Empathy is critical for effective mental health support, especially when addressing Long Counseling Texts (LCTs). However, existing Large Language Models (LLMs) often generate replies that are semantically fluent but lack the structured reasoning necessary for genuine psychological support, particularly in a Chinese context. To bridge this gap, we introduce Empathy-R1, a novel framework that integrates a Chain-of-Empathy (CoE) reasoning process with Reinforcement Learning (RL) to enhance response quality for LCTs. Inspired by cognitive-behavioral therapy, our CoE paradigm guides the model to sequentially reason about a help-seeker's emotions, causes, and intentions, making its thinking process both transparent and interpretable. Our framework is empowered by a new large-scale Chinese dataset, Empathy-QA, and a two-stage training process. First, Supervised Fine-Tuning instills the CoE's reasoning structure. Subsequently, RL, guided by a dedicated reward model, refines the therapeutic relevance and contextual appropriateness of the final responses. Experiments show that Empathy-R1 achieves strong performance on key automatic metrics. More importantly, human evaluations confirm its superiority, showing a clear preference over strong baselines and achieving a Win@1 rate of 44.30% on our new benchmark. By enabling interpretable and contextually nuanced responses, Empathy-R1 represents a significant advancement in developing responsible and genuinely beneficial AI for mental health support.", "abs": "", "categories": ["cs.CL", "cs.AI"], "AI": {"tldr": "Empathy-R1是一个结合“同理心链”（Chain-of-Empathy）推理和强化学习的框架，旨在提升大型语言模型在处理长篇心理健康支持文本（LCTs）时的回复质量，尤其是在中文语境下。", "motivation": "现有大型语言模型在生成心理健康支持回复时，虽然语义流畅，但缺乏结构化的心理支持推理，尤其是在处理复杂的长篇文本时。中文语境下缺乏高质量的LCT数据集进一步加剧了这一问题。", "method": "Empathy-R1框架通过“同理心链”（CoE）推理过程引导模型依次分析求助者的情绪、原因和意图，并结合一个新构建的大规模中文数据集Empathy-QA和两阶段训练流程（监督微调 + 强化学习）来提升回复的治疗相关性和上下文适应性。", "result": "实验结果表明，Empathy-R1在自动评估指标上表现出色，更重要的是，人类评估确认了其优越性，在新的基准测试中达到了44.30%的Win@1率。", "conclusion": "Empathy-R1通过生成可解释且具有上下文细微差别的回复，代表了在开发负责任且真正有益的心理健康支持AI方面的重要进步。"}}
