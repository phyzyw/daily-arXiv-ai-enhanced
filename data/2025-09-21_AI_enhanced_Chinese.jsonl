{"id": "2509.15219", "title": "Out-of-Sight Trajectories: Tracking, Fusion, and Prediction", "authors": ["Haichao Zhang", "Yi Xu", "Yun Fu"], "summary": "Trajectory prediction is a critical task in computer vision and autonomous systems, playing a key role in autonomous driving, robotics, surveillance, and virtual reality. Existing methods often rely on complete and noise-free observational data, overlooking the challenges associated with out-of-sight objects and the inherent noise in sensor data caused by limited camera coverage, obstructions, and the absence of ground truth for denoised trajectories. These limitations pose safety risks and hinder reliable prediction in real-world scenarios. In this extended work, we present advancements in Out-of-Sight Trajectory (OST), a novel task that predicts the noise-free visual trajectories of out-of-sight objects using noisy sensor data. Building on our previous research, we broaden the scope of Out-of-Sight Trajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending its applicability to autonomous driving, robotics, surveillance, and virtual reality. Our enhanced Vision-Positioning Denoising Module leverages camera calibration to establish a vision-positioning mapping, addressing the lack of visual references, while effectively denoising noisy sensor data in an unsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB datasets, our approach achieves state-of-the-art performance in both trajectory denoising and prediction, significantly surpassing previous baselines. Additionally, we introduce comparisons with traditional denoising methods, such as Kalman filtering, and adapt recent trajectory prediction models to our task, providing a comprehensive benchmark. This work represents the first initiative to integrate vision-positioning projection for denoising noisy sensor trajectories of out-of-sight agents, paving the way for future advances. The code and preprocessed datasets are available at github.com/Hai-chao-Zhang/OST", "abs": "", "categories": ["cs.CV", "cs.LG", "cs.MA", "cs.MM", "cs.RO", "68T45, 68U10, 68T07, 68T40, 93C85, 93E11, 62M20, 62M10, 68U05, 94A12", "F.2.2; I.2.9; I.2.10; I.4.1; I.4.8; I.4.9; I.5.4; I.3.7"], "AI": {"tldr": "本文提出了一种新的任务“视野外轨迹预测”(Out-of-Sight Trajectory, OST)，旨在利用噪声传感器数据预测视野外物体的无噪声视觉轨迹，并取得了显著效果。该方法扩展了之前的研究，适用于行人、车辆等多种对象。", "motivation": "现有轨迹预测方法通常依赖于完整且无噪声的观测数据，而现实场景中存在视野遮挡、传感器噪声等问题，导致预测可靠性降低，存在安全隐患。", "method": "本文提出了一种Vision-Positioning Denoising Module，利用相机标定建立视觉定位映射，在无监督学习的条件下有效去除噪声传感器数据。该模块结合Vi-Fi和JRDB数据集进行评估，并与其他传统降噪方法和轨迹预测模型进行对比。", "result": "实验结果表明，本文提出的方法在轨迹降噪和预测方面均达到最先进的性能，显著优于之前的基线模型。", "conclusion": "本文首次将视觉定位投影技术应用于降噪视野外代理人的传感器轨迹，为该领域未来的发展奠定了基础，并提供了代码和预处理数据集。"}}
{"id": "2509.15217", "title": "Generalizable Geometric Image Caption Synthesis", "authors": ["Yue Xin", "Wenyuan Wang", "Rui Pan", "Ruida Wang", "Howard Meng", "Renjie Pi", "Shizhe Diao", "Tong Zhang"], "summary": "Multimodal large language models have various practical applications that demand strong reasoning abilities. Despite recent advancements, these models still struggle to solve complex geometric problems. A key challenge stems from the lack of high-quality image-text pair datasets for understanding geometric images. Furthermore, most template-based data synthesis pipelines typically fail to generalize to questions beyond their predefined templates. In this paper, we bridge this gap by introducing a complementary process of Reinforcement Learning with Verifiable Rewards (RLVR) into the data generation pipeline. By adopting RLVR to refine captions for geometric images synthesized from 50 basic geometric relations and using reward signals derived from mathematical problem-solving tasks, our pipeline successfully captures the key features of geometry problem-solving. This enables better task generalization and yields non-trivial improvements. Furthermore, even in out-of-distribution scenarios, the generated dataset enhances the general reasoning capabilities of multimodal large language models, yielding accuracy improvements of $2.8\\%\\text{-}4.8\\%$ in statistics, arithmetic, algebraic, and numerical tasks with non-geometric input images of MathVista and MathVerse, along with $2.4\\%\\text{-}3.9\\%$ improvements in Art, Design, Tech, and Engineering tasks in MMMU.", "abs": "", "categories": ["cs.AI", "cs.CV", "cs.LG"], "AI": {"tldr": "本文提出了一种基于强化学习可验证奖励 (RLVR) 的数据生成流程，用于合成高质量的几何图像描述，从而提升多模态大语言模型 (MLLM) 的几何推理能力和泛化性能。", "motivation": "现有 MLLM 在处理复杂几何问题时仍存在挑战，主要原因是缺乏高质量的几何图像-文本对数据集，且现有数据合成方法难以泛化。同时，MLLM 在数学推理任务中表现出文本输入优于视觉输入的现象，需要提升其跨模态推理能力。", "method": "该方法利用 RLVR 流程，基于 50 种基本几何关系合成图像，并通过数学问题求解任务中的奖励信号来优化图像描述。采用 RAFT 方法设计奖励函数，兼顾推理和描述质量。", "result": "生成的合成数据集显著提升了 MLLM 在统计、算术、代数和数值任务中的准确率（2.8%-4.8%），并在 Art & Design 和 Tech & Engineering 任务中也取得了改进（2.4%-3.9%）。即使在分布外场景下，也能增强 MLLM 的通用推理能力。", "conclusion": "该研究表明，通过 RLVR 生成高质量的几何图像描述数据集，可以有效提升 MLLM 的几何推理能力和泛化性能，为构建更强大的多模态人工智能系统奠定基础。"}}
{"id": "2509.15167", "title": "Semi-Supervised 3D Medical Segmentation from 2D Natural Images Pretrained Model", "authors": ["Pak-Hei Yeung", "Jayroop Ramesh", "Pengfei Lyu", "Ana Namburete", "Jagath Rajapakse"], "summary": "This paper explores the transfer of knowledge from general vision models pretrained on 2D natural images to improve 3D medical image segmentation. We focus on the semi-supervised setting, where only a few labeled 3D medical images are available, along with a large set of unlabeled images. To tackle this, we propose a model-agnostic framework that progressively distills knowledge from a 2D pretrained model to a 3D segmentation model trained from scratch. Our approach, M&N, involves iterative co-training of the two models using pseudo-masks generated by each other, along with our proposed learning rate guided sampling that adaptively adjusts the proportion of labeled and unlabeled data in each training batch to align with the models' prediction accuracy and stability, minimizing the adverse effect caused by inaccurate pseudo-masks. Extensive experiments on multiple publicly available datasets demonstrate that M&N achieves state-of-the-art performance, outperforming thirteen existing semi-supervised segmentation approaches under all different settings. Importantly, ablation studies show that M&N remains model-agnostic, allowing seamless integration with different architectures. This ensures its adaptability as more advanced models emerge. The code is available at https://github.com/pakheiyeung/M-N.", "abs": "", "categories": ["cs.CV", "cs.AI", "cs.LG"], "AI": {"tldr": "本文提出了一种模型无关的框架M&N，通过迭代共训练和自适应采样，将2D自然图像预训练模型的知识迁移到3D医学图像分割模型，从而在半监督学习场景下提升分割性能。", "motivation": "3D医学图像分割需要大量标注数据，而2D自然图像领域拥有丰富的预训练模型。本文旨在探索如何利用这些预训练模型的知识，特别是在标注数据有限的情况下，提升3D医学图像分割的性能。", "method": "M&N框架通过迭代共训练2D预训练模型和3D分割模型，利用彼此生成的伪掩码进行训练。同时，引入了基于学习率引导的采样策略，自适应调整训练批次中标记数据和未标记数据的比例，以优化模型预测准确性和稳定性。", "result": "在多个公开数据集上的实验表明，M&N框架在各种半监督分割方法中实现了最先进的性能，并且在不同模型架构上都表现出良好的适应性。", "conclusion": "M&N框架提供了一种通用的、模型无关的知识蒸馏方法，可以有效地将2D自然图像预训练模型的知识迁移到3D医学图像分割任务中，尤其适用于标注数据稀缺的场景，为医学图像分析提供了一种新的解决方案。"}}
{"id": "2509.15124", "title": "Learning Mechanistic Subtypes of Neurodegeneration with a Physics-Informed Variational Autoencoder Mixture Model", "authors": ["Sanduni Pinnawala", "Annabelle Hartanto", "Ivor J. A. Simpson", "Peter A. Wijeratne"], "summary": "Modelling the underlying mechanisms of neurodegenerative diseases demands methods that capture heterogeneous and spatially varying dynamics from sparse, high-dimensional neuroimaging data. Integrating partial differential equation (PDE) based physics knowledge with machine learning provides enhanced interpretability and utility over classic numerical methods. However, current physics-integrated machine learning methods are limited to considering a single PDE, severely limiting their application to diseases where multiple mechanisms are responsible for different groups (i.e., subtypes) and aggravating problems with model misspecification and degeneracy. Here, we present a deep generative model for learning mixtures of latent dynamic models governed by physics-based PDEs, going beyond traditional approaches that assume a single PDE structure. Our method integrates reaction-diffusion PDEs within a variational autoencoder (VAE) mixture model framework, supporting inference of subtypes of interpretable latent variables (e.g. diffusivity and reaction rates) from neuroimaging data. We evaluate our method on synthetic benchmarks and demonstrate its potential for uncovering mechanistic subtypes of Alzheimer's disease progression from positron emission tomography (PET) data.", "abs": "", "categories": ["eess.IV", "cs.CV", "cs.LG"], "AI": {"tldr": "本文提出了一种基于物理信息的变分自编码器混合模型（BrainPhys），用于学习神经退行性疾病的潜在动态模型，并能够识别疾病亚型。", "motivation": "传统的物理信息机器学习方法通常假设单一的偏微分方程（PDE）结构，难以处理神经退行性疾病的多机制和亚型异质性，容易出现模型错位和简并问题。", "method": "BrainPhys将反应-扩散PDE整合到变分自编码器（VAE）混合模型框架中，允许同时学习多个可能的PDE，从而支持从神经影像数据中推断可解释的潜在变量（如扩散率和反应速率）的亚型。", "result": "在合成基准测试中，该模型能够恢复机制模型的簇及其参数。在ADNI的tau和amyloid PET数据集上应用该模型，发现了支持2-成分混合模型的证据。", "conclusion": "BrainPhys为理解神经退行性疾病的机制亚型提供了新的方法，有助于克服传统方法中的模型错位和简并问题，并有望应用于阿尔茨海默病等疾病的亚型识别和机制研究。"}}
{"id": "2509.15058", "title": "Communication Efficient Split Learning of ViTs with Attention-based Double Compression", "authors": ["Federico Alvetreti", "Jary Pomponi", "Paolo Di Lorenzo", "Simone Scardapane"], "summary": "This paper proposes a novel communication-efficient Split Learning (SL) framework, named Attention-based Double Compression (ADC), which reduces the communication overhead required for transmitting intermediate Vision Transformers activations during the SL training process. ADC incorporates two parallel compression strategies. The first one merges samples' activations that are similar, based on the average attention score calculated in the last client layer; this strategy is class-agnostic, meaning that it can also merge samples having different classes, without losing generalization ability nor decreasing final results. The second strategy follows the first and discards the least meaningful tokens, further reducing the communication cost. Combining these strategies not only allows for sending less during the forward pass, but also the gradients are naturally compressed, allowing the whole model to be trained without additional tuning or approximations of the gradients. Simulation results demonstrate that Attention-based Double Compression outperforms state-of-the-art SL frameworks by significantly reducing communication overheads while maintaining high accuracy.", "abs": "", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "AI": {"tldr": "本文提出了一种名为Attention-based Double Compression (ADC) 的新型通信高效的Split Learning (SL)框架，通过双重压缩策略显著降低了Vision Transformer (ViT)激活值的通信开销，同时保持了高精度。", "motivation": "深度神经网络的训练需要大量的计算资源和内存，传统的云端学习模式存在通信开销大和隐私泄露的风险。Split Learning (SL) 是一种解决这些问题的有前景的方案，但通信瓶颈仍然是实际应用中的一个重要限制。", "method": "ADC框架采用两种并行压缩策略：首先，基于最后一层客户端的平均注意力分数，合并相似的样本激活值（不区分类别）；其次，丢弃最不重要的token，进一步降低通信成本。这两种策略共同作用，压缩了前向传播中的激活值，并自然地压缩了梯度。", "result": "实验结果表明，ADC在各种压缩比下都优于现有的SL框架，显著降低了通信开销，同时保持了高精度。在CIFAR100数据集上，使用DeiT-S模型进行训练，ADC在极端通信约束下也能可靠运行，且仅有微小的精度损失。", "conclusion": "ADC提供了一种有效的通信高效SL解决方案，特别适用于Vision Transformer，能够在保证模型精度的前提下，显著降低通信成本，为在资源受限的边缘设备上部署深度学习模型提供了新的可能性。"}}
{"id": "2509.15076", "title": "Forecasting and Visualizing Air Quality from Sky Images with Vision-Language Models", "authors": ["Mohammad Saleh Vahdatpour", "Maryam Eyvazi", "Yanqing Zhang"], "summary": "Air pollution remains a critical threat to public health and environmental sustainability, yet conventional monitoring systems are often constrained by limited spatial coverage and accessibility. This paper proposes an AI-driven agent that predicts ambient air pollution levels from sky images and synthesizes realistic visualizations of pollution scenarios using generative modeling. Our approach combines statistical texture analysis with supervised learning for pollution classification, and leverages vision-language model (VLM)-guided image generation to produce interpretable representations of air quality conditions. The generated visuals simulate varying degrees of pollution, offering a foundation for user-facing interfaces that improve transparency and support informed environmental decision-making. These outputs can be seamlessly integrated into intelligent applications aimed at enhancing situational awareness and encouraging behavioral responses based on real-time forecasts. We validate our method using a dataset of urban sky images and demonstrate its effectiveness in both pollution level estimation and semantically consistent visual synthesis. The system design further incorporates human-centered user experience principles to ensure accessibility, clarity, and public engagement in air quality forecasting. To support scalable and energy-efficient deployment, future iterations will incorporate a green CNN architecture enhanced with FPGA-based incremental learning, enabling real-time inference on edge platforms.", "abs": "", "categories": ["cs.LG", "cs.CV"], "AI": {"tldr": "本文提出了一种基于视觉-语言模型(VLM)的AI系统，该系统能够从天空图像预测空气污染水平并生成逼真的污染场景可视化，旨在弥补传统监测系统的不足。", "motivation": "传统空气质量监测系统存在空间覆盖范围有限、成本高昂等问题，迫切需要更高效、更易于获取的替代方案。利用天空图像作为空气污染的视觉指标，结合人工智能技术，可以实现更广泛的空气质量预测。", "method": "该方法结合统计纹理分析与监督学习进行污染分类，并利用VLM引导的图像生成技术生成可解释的空气质量条件表示。系统设计遵循以人为本的用户体验原则，并计划未来采用绿色CNN架构和FPGA加速，以实现边缘平台的实时推理。", "result": "实验结果表明，该系统在城市天空图像数据集上有效预测空气污染水平，并能生成语义一致的视觉合成图像，证明了其在污染水平估计和视觉合成方面的有效性。", "conclusion": "该研究为空气质量预测提供了新的思路，通过视觉信息和生成模型，可以为公众提供更透明、更易理解的空气质量信息，支持环境决策，并有望在低资源地区推广应用。"}}
{"id": "2509.15045", "title": "Synthetic-to-Real Object Detection using YOLOv11 and Domain Randomization Strategies", "authors": ["Luisa Torquato Niño", "Hamza A. A. Gardi"], "summary": "This paper addresses the synthetic-to-real domain gap in object detection, focusing on training a YOLOv11 model to detect a specific object (a soup can) using only synthetic data and domain randomization strategies. The methodology involves extensive experimentation with data augmentation, dataset composition, and model scaling. While synthetic validation metrics were consistently high, they proved to be poor predictors of real-world performance. Consequently, models were also evaluated qualitatively, through visual inspection of predictions, and quantitatively, on a manually labeled real-world test set, to guide development. Final mAP@50 scores were provided by the official Kaggle competition. Key findings indicate that increasing synthetic dataset diversity, specifically by including varied perspectives and complex backgrounds, combined with carefully tuned data augmentation, were crucial in bridging the domain gap. The best performing configuration, a YOLOv11l model trained on an expanded and diverse dataset, achieved a final mAP@50 of 0.910 on the competition's hidden test set. This result demonstrates the potential of a synthetic-only training approach while also highlighting the remaining challenges in fully capturing real-world variability.", "abs": "", "categories": ["cs.CV", "cs.LG"], "AI": {"tldr": "本文研究了仅使用合成数据和领域随机化策略，利用YOLOv11模型进行汤罐目标检测，并成功在Kaggle竞赛中取得了优异成绩，证明了完全合成训练的可行性。", "motivation": "现实世界中收集和标注大量数据集成本高昂，本文旨在探索使用合成数据和领域随机化策略来解决目标检测中的领域差距问题，降低数据获取成本。", "method": "研究团队使用Falcon的Duality AI模拟器生成合成数据，并对数据增强、数据集组成和模型缩放进行了广泛实验。他们选择了YOLOv11模型，并结合精心调整的数据增强策略，通过视觉检查和真实世界测试集评估模型性能。", "result": "最佳配置（YOLOv11l模型，扩展且多样化的数据集）在竞赛的隐藏测试集中达到了0.910的mAP@50分数。合成验证指标与真实世界性能之间存在差异，需要结合视觉检查和真实数据集评估。", "conclusion": "研究结果表明，通过增加合成数据集的多样性（包括不同的视角和复杂的背景）以及精心调整数据增强，可以有效地弥合领域差距。该研究证明了完全合成训练在特定场景下的可行性，并指出了未来在完全捕捉真实世界变化方面的挑战。"}}
{"id": "2509.15156", "title": "Leveraging Geometric Visual Illusions as Perceptual Inductive Biases for Vision Models", "authors": ["Haobo Yang", "Minghao Guo", "Dequan Yang", "Wenyu Wang"], "summary": "Contemporary deep learning models have achieved impressive performance in image classification by primarily leveraging statistical regularities within large datasets, but they rarely incorporate structured insights drawn directly from perceptual psychology. To explore the potential of perceptually motivated inductive biases, we propose integrating classic geometric visual illusions well-studied phenomena from human perception into standard image-classification training pipelines. Specifically, we introduce a synthetic, parametric geometric-illusion dataset and evaluate three multi-source learning strategies that combine illusion recognition tasks with ImageNet classification objectives. Our experiments reveal two key conceptual insights: (i) incorporating geometric illusions as auxiliary supervision systematically improves generalization, especially in visually challenging cases involving intricate contours and fine textures; and (ii) perceptually driven inductive biases, even when derived from synthetic stimuli traditionally considered unrelated to natural image recognition, can enhance the structural sensitivity of both CNN and transformer-based architectures. These results demonstrate a novel integration of perceptual science and machine learning and suggest new directions for embedding perceptual priors into vision model design.", "abs": "", "categories": ["cs.CV", "cs.AI"], "AI": {"tldr": "本文研究了如何将经典的几何视觉错觉融入到图像分类训练中，以提升模型的泛化能力，尤其是在处理复杂轮廓和纹理时。", "motivation": "现代深度学习模型虽然在图像分类上表现出色，但往往忽略了人类视觉理解中的结构化信息。本文旨在探索将基于感知心理学的结构化先验知识融入到视觉模型设计中，弥补这一差距。", "method": "研究人员构建了一个包含五个经典几何视觉错觉的合成数据集，并设计了三种多源学习策略（单头联合、多头并行和混合策略），将错觉识别任务与ImageNet分类目标相结合，进行训练。", "result": "实验结果表明，将几何错觉作为辅助监督信号可以系统地提高模型的泛化能力，尤其是在涉及复杂轮廓和精细纹理的视觉挑战性案例中。即使是源自合成刺激的感知驱动的先验知识，也能增强CNN和Transformer架构对结构的敏感性。", "conclusion": "本文展示了感知科学与机器学习的创新融合，表明几何错觉可以帮助视觉模型更稳健地识别结构，并为将感知先验知识嵌入到视觉模型设计中开辟了新的方向。"}}
{"id": "2509.15130", "title": "WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance", "authors": ["Chenxi Song", "Yanming Yang", "Tong Zhao", "Ruibo Li", "Chi Zhang"], "summary": "Recent video diffusion models demonstrate strong potential in spatial intelligence tasks due to their rich latent world priors. However, this potential is hindered by their limited controllability and geometric inconsistency, creating a gap between their strong priors and their practical use in 3D/4D tasks. As a result, current approaches often rely on retraining or fine-tuning, which risks degrading pretrained knowledge and incurs high computational costs. To address this, we propose WorldForge, a training-free, inference-time framework composed of three tightly coupled modules. Intra-Step Recursive Refinement introduces a recursive refinement mechanism during inference, which repeatedly optimizes network predictions within each denoising step to enable precise trajectory injection. Flow-Gated Latent Fusion leverages optical flow similarity to decouple motion from appearance in the latent space and selectively inject trajectory guidance into motion-related channels. Dual-Path Self-Corrective Guidance compares guided and unguided denoising paths to adaptively correct trajectory drift caused by noisy or misaligned structural signals. Together, these components inject fine-grained, trajectory-aligned guidance without training, achieving both accurate motion control and photorealistic content generation. Extensive experiments across diverse benchmarks validate our method's superiority in realism, trajectory consistency, and visual fidelity. This work introduces a novel plug-and-play paradigm for controllable video synthesis, offering a new perspective on leveraging generative priors for spatial intelligence.", "abs": "", "categories": ["cs.GR", "cs.AI", "cs.CV"], "AI": {"tldr": "本文提出了WorldForge，一个无需训练的框架，利用预训练的视频扩散模型进行3D/4D任务，如单视角3D场景生成和动态4D场景重渲染，实现精确的相机轨迹控制。", "motivation": "现有视频扩散模型在3D/4D任务中存在可控性差和几何不一致的问题，通常需要重新训练或微调，这会降低预训练知识并带来高计算成本。", "method": "WorldForge框架由三个模块组成：步内递归细化（Intra-Step Recursive Refinement）用于精确轨迹注入；流门控潜空间融合（Flow-Gated Latent Fusion）用于解耦运动和外观；双路径自校正引导（Dual-Path Self-Corrective Guidance）用于自适应校正轨迹漂移。这些模块在推理时注入轨迹对齐的引导，无需训练。", "result": "实验结果表明，WorldForge在真实感、轨迹一致性和视觉保真度方面优于现有方法，实现了准确的运动控制和逼真的内容生成。", "conclusion": "WorldForge引入了一种可插拔的、可控视频合成范式，为利用生成先验进行空间智能提供了一种新视角。"}}
{"id": "2509.15011", "title": "Sea-ing Through Scattered Rays: Revisiting the Image Formation Model for Realistic Underwater Image Generation", "authors": ["Vasiliki Ismiroglou", "Malte Pedersen", "Stefan H. Bengtson", "Andreas Aakerberg", "Thomas B. Moeslund"], "summary": "In recent years, the underwater image formation model has found extensive use in the generation of synthetic underwater data. Although many approaches focus on scenes primarily affected by discoloration, they often overlook the model's ability to capture the complex, distance-dependent visibility loss present in highly turbid environments. In this work, we propose an improved synthetic data generation pipeline that includes the commonly omitted forward scattering term, while also considering a nonuniform medium. Additionally, we collected the BUCKET dataset under controlled turbidity conditions to acquire real turbid footage with the corresponding reference images. Our results demonstrate qualitative improvements over the reference model, particularly under increasing turbidity, with a selection rate of 82. 5\\% by survey participants. Data and code can be accessed on the project page: vap.aau.dk/sea-ing-through-scattered-rays.", "abs": "", "categories": ["cs.CV", "cs.AI"], "AI": {"tldr": "本文提出了改进的合成水下图像生成流程，考虑了前向散射和非均匀介质的影响，并收集了具有控制的浑浊度条件的 BUCKET 数据集，在浑浊环境下显著提升了图像质量。", "motivation": "现有水下图像生成模型通常忽略了复杂的水下环境，尤其是在高浑浊度环境下，未能充分捕捉距离相关的能见度损失，导致合成图像不够真实。此外，现有数据集缺乏对浑浊环境的覆盖。", "method": "本文改进了水下图像生成模型（IFM），加入了常用的前向散射项，并考虑了非均匀介质。同时，收集了 BUCKET 数据集，该数据集在控制的浑浊度条件下采集了真实水下视频和对应的参考图像。", "result": "实验结果表明，改进后的模型在增加浑浊度的情况下，相比于参考模型有显著的定性提升，调查参与者选择率达到 82.5%。", "conclusion": "本文的研究改进了水下图像生成模型，为水下计算机视觉领域的数据匮乏问题提供了一种有效的解决方案，尤其是在高浑浊度环境下，有助于推动环境监测和生态评估等应用。"}}
{"id": "2509.14998", "title": "A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical Decision-making", "authors": ["Xiao Wu", "Ting-Zhu Huang", "Liang-Jian Deng", "Yanyuan Qiao", "Imran Razzak", "Yutong Xie"], "summary": "Medical decision-making often involves integrating knowledge from multiple clinical specialties, typically achieved through multidisciplinary teams. Inspired by this collaborative process, recent work has leveraged large language models (LLMs) in multi-agent collaboration frameworks to emulate expert teamwork. While these approaches improve reasoning through agent interaction, they are limited by static, pre-assigned roles, which hinder adaptability and dynamic knowledge integration. To address these limitations, we propose KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration framework that enables LLM agents to dynamically form and expand expert teams based on the evolving diagnostic context. KAMAC begins with one or more expert agents and then conducts a knowledge-driven discussion to identify and fill knowledge gaps by recruiting additional specialists as needed. This supports flexible, scalable collaboration in complex clinical scenarios, with decisions finalized through reviewing updated agent comments. Experiments on two real-world medical benchmarks demonstrate that KAMAC significantly outperforms both single-agent and advanced multi-agent methods, particularly in complex clinical scenarios (i.e., cancer prognosis) requiring dynamic, cross-specialty expertise. Our code is publicly available at: https://github.com/XiaoXiao-Woo/KAMAC.", "abs": "", "categories": ["cs.AI", "cs.CV"], "AI": {"tldr": "本文提出了一种名为KAMAC的知识驱动自适应多智能体协作框架，旨在通过动态组建和扩展专家团队来提升LLM在复杂医疗决策中的表现。", "motivation": "现有LLM多智能体协作框架通常采用静态预设的角色，难以适应动态变化的诊断情境和知识需求，因此需要一种更具适应性的协作方式。", "method": "KAMAC框架从一个或多个专家智能体开始，通过知识驱动的讨论识别知识差距，并根据需要动态招募新的专家智能体，形成灵活可扩展的团队，最终通过审查智能体评论来做出决策。", "result": "在两个真实世界的医疗基准测试中，KAMAC显著优于单智能体和先进的多智能体方法，尤其是在需要动态跨专业知识的复杂临床场景（如癌症预后）中表现更佳。", "conclusion": "KAMAC框架为LLM在医疗决策中的应用提供了一种新的思路，通过知识驱动的自适应协作，能够更好地应对复杂医疗问题，并有望提升医疗决策的准确性和效率。"}}
{"id": "2509.14980", "title": "M4Diffuser: Multi-View Diffusion Policy with Manipulability-Aware Control for Robust Mobile Manipulation", "authors": ["Ju Dong", "Lei Zhang", "Liding Zhang", "Yao Ling", "Yu Fu", "Kaixin Bai", "Zoltán-Csaba Márton", "Zhenshan Bing", "Zhaopeng Chen", "Alois Christian Knoll", "Jianwei Zhang"], "summary": "Mobile manipulation requires the coordinated control of a mobile base and a robotic arm while simultaneously perceiving both global scene context and fine-grained object details. Existing single-view approaches often fail in unstructured environments due to limited fields of view, exploration, and generalization abilities. Moreover, classical controllers, although stable, struggle with efficiency and manipulability near singularities. To address these challenges, we propose M4Diffuser, a hybrid framework that integrates a Multi-View Diffusion Policy with a novel Reduced and Manipulability-aware QP (ReM-QP) controller for mobile manipulation. The diffusion policy leverages proprioceptive states and complementary camera perspectives with both close-range object details and global scene context to generate task-relevant end-effector goals in the world frame. These high-level goals are then executed by the ReM-QP controller, which eliminates slack variables for computational efficiency and incorporates manipulability-aware preferences for robustness near singularities. Comprehensive experiments in simulation and real-world environments show that M4Diffuser achieves 7 to 56 percent higher success rates and reduces collisions by 3 to 31 percent over baselines. Our approach demonstrates robust performance for smooth whole-body coordination, and strong generalization to unseen tasks, paving the way for reliable mobile manipulation in unstructured environments. Details of the demo and supplemental material are available on our project website https://sites.google.com/view/m4diffuser.", "abs": "", "categories": ["cs.RO", "cs.AI", "cs.CV"], "AI": {"tldr": "M4Diffuser是一个混合框架，结合了多视角扩散策略和改进的QP控制器，旨在提高移动机器人的鲁棒性和效率，尤其是在复杂环境中。", "motivation": "现有移动操作方法存在局限性：传统优化控制器计算效率低，操纵性差；学习型方法缺乏稳定性，且单视角感知能力有限，难以捕捉全局场景和精细物体细节。", "method": "M4Diffuser采用了一种混合方法：(1) 多视角扩散Transformer策略，利用多视角相机信息生成任务相关的末端执行器目标；(2) 改进的QP控制器(ReM-QP)，去除了松弛变量，并引入了操纵性感知偏好，以提高计算效率和鲁棒性。", "result": "在仿真和真实环境中，M4Diffuser的成功率提高了7%-56%，碰撞次数减少了3%-31%，展现了流畅的全身协调性和对未知任务的强大泛化能力。", "conclusion": "M4Diffuser为在非结构化环境中实现可靠的移动操作铺平了道路，通过结合扩散策略和改进的QP控制器，有效解决了现有方法的局限性。"}}
{"id": "2509.14966", "title": "RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching", "authors": ["Xingwu Zhang", "Guanxuan Li", "Zhuocheng Zhang", "Zijun Long"], "summary": "The rapidly growing number of product categories in large-scale e-commerce makes accurate object identification for automated packing in warehouses substantially more difficult. As the catalog grows, intra-class variability and a long tail of rare or visually similar items increase, and when combined with diverse packaging, cluttered containers, frequent occlusion, and large viewpoint changes-these factors amplify discrepancies between query and reference images, causing sharp performance drops for methods that rely solely on 2D appearance features. Thus, we propose RoboEye, a two-stage identification framework that dynamically augments 2D semantic features with domain-adapted 3D reasoning and lightweight adapters to bridge training deployment gaps. In the first stage, we train a large vision model to extract 2D features for generating candidate rankings. A lightweight 3D-feature-awareness module then estimates 3D feature quality and predicts whether 3D re-ranking is necessary, preventing performance degradation and avoiding unnecessary computation. When invoked, the second stage uses our robot 3D retrieval transformer, comprising a 3D feature extractor that produces geometry-aware dense features and a keypoint-based matcher that computes keypoint-correspondence confidences between query and reference images instead of conventional cosine-similarity scoring. Experiments show that RoboEye improves Recall@1 by 7.1% over the prior state of the art (RoboLLM). Moreover, RoboEye operates using only RGB images, avoiding reliance on explicit 3D inputs and reducing deployment costs. The code used in this paper is publicly available at: https://github.com/longkukuhi/RoboEye.", "abs": "", "categories": ["cs.CV", "cs.AI", "cs.RO"], "AI": {"tldr": "RoboEye是一种两阶段的物体识别框架，通过动态增强2D语义特征并结合领域自适应的3D推理，显著提升了在复杂仓库环境下的物体识别准确率，尤其是在存在视角变化、遮挡和包装差异的情况下。", "motivation": "随着电商产品目录的快速增长，仓库物体识别面临着更大的挑战，传统的仅依赖2D外观特征的方法在视角变化、遮挡、包装差异和长尾分布等因素的影响下性能下降。", "method": "RoboEye框架包含两个阶段：首先，使用大型视觉模型提取2D特征进行候选排序；其次，通过轻量级的3D特征质量评估模块判断是否需要3D重新排序，并在必要时使用机器人3D检索Transformer，利用几何感知的密集特征和基于关键点的匹配算法计算查询和参考图像之间的关键点对应关系。", "result": "实验结果表明，RoboEye在Recall@1指标上比之前的最佳模型RoboLLM提高了7.1%，并且仅使用RGB图像，无需显式3D输入，降低了部署成本。", "conclusion": "RoboEye通过结合2D语义特征和3D几何推理，有效解决了传统2D特征方法在复杂仓库环境下的泛化能力问题，为自动化仓库中的物体识别提供了更可靠的解决方案。"}}
{"id": "2509.15207", "title": "FlowRL: Matching Reward Distributions for LLM Reasoning", "authors": ["Xuekai Zhu", "Daixuan Cheng", "Dinghuai Zhang", "Hengli Li", "Kaiyan Zhang", "Che Jiang", "Youbang Sun", "Ermo Hua", "Yuxin Zuo", "Xingtai Lv", "Qizheng Zhang", "Lin Chen", "Fanghao Shao", "Bo Xue", "Yunchong Song", "Zhenjie Yang", "Ganqu Cui", "Ning Ding", "Jianfeng Gao", "Xiaodong Liu", "Bowen Zhou", "Hongyuan Mei", "Zhouhan Lin"], "summary": "We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning (RL). Recent advanced reasoning models adopt reward-maximizing methods (\\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, thus reducing diversity. In contrast, we transform scalar rewards into a normalized target distribution using a learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution. We implement this idea as a flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories. We conduct experiments on math and code reasoning tasks: FlowRL achieves a significant average improvement of $10.0\\%$ over GRPO and $5.1\\%$ over PPO on math benchmarks, and performs consistently better on code reasoning tasks. These results highlight reward distribution-matching as a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning.", "abs": "", "categories": ["cs.LG", "cs.AI", "cs.CL"], "AI": {"tldr": "FlowRL是一种新的LLM强化学习算法，它通过匹配奖励分布而不是最大化奖励来促进多样化的推理，从而避免了传统方法中的模式坍塌。", "motivation": "现有的基于奖励最大化的LLM强化学习方法（如PPO和GRPO）容易过度优化主导奖励信号，忽略其他有效的推理路径，导致推理路径多样性降低，泛化能力不足。", "method": "FlowRL将标量奖励转换为可学习的划分函数生成的归一化目标分布，并最小化策略与目标分布之间的逆KL散度，实现奖励分布匹配，从而促进多样化探索。", "result": "在数学和代码推理任务上，FlowRL相比于GRPO和PPO分别平均提升了10.0%和5.1%，并在代码推理任务中表现更佳。", "conclusion": "FlowRL的奖励分布匹配方法是实现LLM强化学习中高效探索和多样化推理的关键一步，有效解决了传统奖励最大化方法中的模式坍塌问题。"}}
{"id": "2509.15194", "title": "Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation", "authors": ["Yujun Zhou", "Zhenwen Liang", "Haolin Liu", "Wenhao Yu", "Kishan Panaganti", "Linfeng Song", "Dian Yu", "Xiangliang Zhang", "Haitao Mi", "Dong Yu"], "summary": "Large language models (LLMs) are increasingly trained with reinforcement learning from verifiable rewards (RLVR), yet real-world deployment demands models that can self-improve without labels or external judges. Existing label-free methods, confidence minimization, self-consistency, or majority-vote objectives, stabilize learning but steadily shrink exploration, causing an entropy collapse: generations become shorter, less diverse, and brittle. Unlike prior approaches such as Test-Time Reinforcement Learning (TTRL), which primarily adapt models to the immediate unlabeled dataset at hand, our goal is broader: to enable general improvements without sacrificing the model's inherent exploration capacity and generalization ability, i.e., evolving. We formalize this issue and propose EVolution-Oriented and Label-free Reinforcement Learning (EVOL-RL), a simple rule that couples stability with variation under a label-free setting. EVOL-RL keeps the majority-voted answer as a stable anchor (selection) while adding a novelty-aware reward that favors responses whose reasoning differs from what has already been produced (variation), measured in semantic space. Implemented with GRPO, EVOL-RL also uses asymmetric clipping to preserve strong signals and an entropy regularizer to sustain search. This majority-for-selection + novelty-for-variation design prevents collapse, maintains longer and more informative chains of thought, and improves both pass@1 and pass@n. EVOL-RL consistently outperforms the majority-only TTRL baseline; e.g., training on label-free AIME24 lifts Qwen3-4B-Base AIME25 pass@1 from TTRL's 4.6% to 16.4%, and pass@16 from 18.5% to 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks stronger generalization across domains (e.g., GPQA). Furthermore, we demonstrate that EVOL-RL also boosts performance in the RLVR setting, highlighting its broad applicability.", "abs": "", "categories": ["cs.LG", "cs.CL"], "AI": {"tldr": "本文提出了一种名为EVOL-RL的无标签强化学习方法，通过平衡选择（基于多数票）和变化（基于新颖性）来避免语言模型在自我进化过程中出现的熵塌缩问题，从而提升模型的泛化能力和性能。", "motivation": "现有无标签强化学习方法虽然能稳定学习，但容易导致熵塌缩，使得模型生成结果短、缺乏多样性且脆弱。研究旨在开发能够自主进化、持续学习，且不依赖标签或外部评判的语言模型。", "method": "EVOL-RL将选择（majority-voted answer）作为稳定锚点，并引入新颖性奖励（novelty-aware reward）来鼓励生成与已有答案不同的推理过程。该方法结合了GRPO算法、非对称裁剪和熵正则化，实现了“多数票选择 + 新颖性变化”的设计。", "result": "实验表明，EVOL-RL在MATH-500和AIME25等数据集上显著优于仅使用多数票的TTRL基线，例如在AIME25上，Qwen3-4B-Base模型的pass@1从4.6%提升至16.4%，pass@16从18.5%提升至37.9%。EVOL-RL不仅避免了多样性塌缩，还提升了跨领域泛化能力，并在RLVR设置下也能提升性能。", "conclusion": "EVOL-RL提供了一种有效的无标签强化学习框架，能够促进语言模型的自主进化，提升其泛化能力和性能，为构建更智能的语言模型奠定了基础。"}}
{"id": "2509.15188", "title": "Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning", "authors": ["Yeongbin Seo", "Dongha Lee", "Jaehyung Kim", "Jinyoung Yeo"], "summary": "Autoregressive (AR) language models generate text one token at a time, which limits their inference speed. Diffusion-based language models offer a promising alternative, as they can decode multiple tokens in parallel. However, we identify a key bottleneck in current diffusion LMs: the long decoding-window problem, where tokens generated far from the input context often become irrelevant or repetitive. Previous solutions like semi-autoregressive address this issue by splitting windows into blocks, but this sacrifices speed and bidirectionality, eliminating the main advantage of diffusion models. To overcome this, we propose Convolutional decoding (Conv), a normalization-based method that narrows the decoding window without hard segmentation, leading to better fluency and flexibility. Additionally, we introduce Rejecting Rule-based Fine-Tuning (R2FT), a post-hoc training scheme that better aligns tokens at positions far from context. Our methods achieve state-of-the-art results on open-ended generation benchmarks (e.g., AlpacaEval) among diffusion LM baselines, with significantly lower step size than previous works, demonstrating both speed and quality improvements.", "abs": "", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7"], "AI": {"tldr": "本文提出了一种新的扩散语言模型解码方法，通过卷积解码 (Conv) 缩小解码窗口并结合拒绝规则的微调 (R2FT)，解决了扩散语言模型中长解码窗口问题，实现了更快的速度和更高的生成质量。", "motivation": "现有的扩散语言模型存在长解码窗口问题，导致生成文本与上下文关联度低、重复性高。虽然半自回归方法可以缓解此问题，但牺牲了速度和双向性，失去了扩散模型的主要优势。", "method": "本文提出了两种方法：(1) 卷积解码 (Conv) 使用归一化方法缩小解码窗口，避免了硬分割带来的质量下降；(2) 拒绝规则的微调 (R2FT) 是一种后期训练方案，用于更好地对齐远离上下文的位置的token。", "result": "实验结果表明，结合 Conv 和 R2FT 的方法在 AlpacaEval 等开放式生成基准测试中取得了最先进的结果，并且在更少的步数下实现了速度和质量的提升。", "conclusion": "本文提出的方法有效解决了扩散语言模型中的长解码窗口问题，在保持速度和灵活性的同时提高了生成文本的流畅性和连贯性，为扩散语言模型的发展提供了新的思路。"}}
{"id": "2509.15157", "title": "Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning", "authors": ["Shiwan Zhao", "Xuyang Zhao", "Jiaming Zhou", "Aobo Kong", "Qicheng Li", "Yong Qin"], "summary": "Supervised fine-tuning (SFT) of large language models can be viewed as an off-policy learning problem, where expert demonstrations come from a fixed behavior policy while training aims to optimize a target policy. Importance sampling is the standard tool for correcting this distribution mismatch, but large policy gaps lead to high variance and training instability. Existing approaches mitigate this issue using KL penalties or clipping, which passively constrain updates rather than actively reducing the gap. We propose a simple yet effective data rewriting framework that proactively shrinks the policy gap by keeping correct solutions as on-policy data and rewriting incorrect ones with guided re-solving, falling back to expert demonstrations only when needed. This aligns the training distribution with the target policy before optimization, reducing importance sampling variance and stabilizing off-policy fine-tuning. Experiments on five mathematical reasoning benchmarks demonstrate consistent and significant gains over both vanilla SFT and the state-of-the-art Dynamic Fine-Tuning (DFT) approach. The data and code will be released at https://github.com/NKU-HLT/Off-Policy-SFT.", "abs": "", "categories": ["cs.LG", "cs.CL"], "AI": {"tldr": "本文提出了一种数据重写框架，通过主动缩小策略差距来稳定离策略监督微调，显著降低了训练不稳定性和方差。", "motivation": "传统的监督微调（SFT）存在策略差距问题，导致训练不稳定和过拟合。现有方法主要通过KL惩罚或裁剪来被动约束更新，而未能主动缩小策略差距。", "method": "该框架通过对数据进行重写来主动缩小策略差距：如果模型能正确解决问题，则保留；否则，利用正确答案引导模型重新解决问题（digest-and-retell），如果仍然失败，则回退到专家演示。", "result": "在五个数学推理基准测试中，该方法在原始SFT和最先进的动态微调（DFT）方法上都取得了显著且一致的性能提升。", "conclusion": "数据重写框架通过更好地对齐训练分布和目标策略，降低了重要性抽样方差，稳定了离策略微调，为独立的微调以及未来的RL或混合方法奠定了更稳定的基础。"}}
{"id": "2509.15110", "title": "TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference", "authors": ["Dan Zhang", "Min Cai", "Jonathan Li", "Ziniu Hu", "Yisong Yue", "Yuxiao Dong", "Jie Tang"], "summary": "Reward models are central to both reinforcement learning (RL) with language models and inference-time verification. However, existing reward models often lack temporal consistency, leading to ineffective policy updates and unstable RL training. We introduce TDRM, a method for learning smoother and more reliable reward models by minimizing temporal differences during training. This temporal-difference (TD) regularization produces smooth rewards and improves alignment with long-term objectives. Incorporating TDRM into the actor-critic style online RL loop yields consistent empirical gains. It is worth noting that TDRM is a supplement to verifiable reward methods, and both can be used in series. Experiments show that TD-trained process reward models (PRMs) improve performance across Best-of-N (up to 6.6%) and tree-search (up to 23.7%) settings. When combined with Reinforcement Learning with Verifiable Rewards (RLVR), TD-trained PRMs lead to more data-efficient RL -- achieving comparable performance with just 2.5k data to what baseline methods require 50.1k data to attain -- and yield higher-quality language model policies on 8 model variants (5 series), e.g., Qwen2.5-(0.5B, 1,5B), GLM4-9B-0414, GLM-Z1-9B-0414, Qwen2.5-Math-(1.5B, 7B), and DeepSeek-R1-Distill-Qwen-(1.5B, 7B). We release all code at https://github.com/THUDM/TDRM.", "abs": "", "categories": ["cs.LG", "cs.CL"], "AI": {"tldr": "本文提出了TDRM方法，通过最小化时间差正则化来学习更平滑、更可靠的奖励模型，从而提升LLM强化学习和推理的性能。", "motivation": "现有奖励模型缺乏时间一致性，导致策略更新无效和强化学习训练不稳定，难以区分推理过程中的不同步骤对最终结果的贡献。", "method": "TDRM采用时间差(TD)正则化，在训练过程中最小化奖励模型在相邻时间步之间的差异，从而产生平滑的奖励信号。该方法可以与可验证奖励方法结合使用，并应用于actor-critic风格的在线强化学习循环。", "result": "实验表明，TDRM能够提升Best-of-N和树搜索设置下的性能，并与RLVR结合使用时，显著提高数据效率，在8种LLM模型（包括Qwen2.5和GLM系列）上取得了更高质量的语言模型策略。", "conclusion": "TDRM提供了一种简单有效的方法来改善奖励模型的平滑性和一致性，从而提升LLM的强化学习和推理能力，为LLM的训练和应用提供了新的思路。"}}
{"id": "2509.14926", "title": "Patent Language Model Pretraining with ModernBERT", "authors": ["Amirhossein Yousefiramandi", "Ciaran Cooney"], "summary": "Transformer-based language models such as BERT have become foundational in NLP, yet their performance degrades in specialized domains like patents, which contain long, technical, and legally structured text. Prior approaches to patent NLP have primarily relied on fine-tuning general-purpose models or domain-adapted variants pretrained with limited data. In this work, we pretrain 3 domain-specific masked language models for patents, using the ModernBERT architecture and a curated corpus of over 60 million patent records. Our approach incorporates architectural optimizations, including FlashAttention, rotary embeddings, and GLU feed-forward layers. We evaluate our models on four downstream patent classification tasks. Our model, ModernBERT-base-PT, consistently outperforms the general-purpose ModernBERT baseline on three out of four datasets and achieves competitive performance with a baseline PatentBERT. Additional experiments with ModernBERT-base-VX and Mosaic-BERT-large demonstrate that scaling the model size and customizing the tokenizer further enhance performance on selected tasks. Notably, all ModernBERT variants retain substantially faster inference over - 3x that of PatentBERT - underscoring their suitability for time-sensitive applications. These results underscore the benefits of domain-specific pretraining and architectural improvements for patent-focused NLP tasks.", "abs": "", "categories": ["cs.CL", "cs.AI", "cs.LG"], "AI": {"tldr": "本文提出了针对专利领域的ModernBERT模型，通过在包含6000万专利记录的大型语料库上进行预训练，并在模型架构上进行优化，显著提升了专利NLP任务的性能，同时保持了更快的推理速度。", "motivation": "现有基于BERT的语言模型在专利等专业领域表现不佳，因为专利文本具有独特的法律和技术结构。为了解决这一问题，需要针对专利领域进行专门的预训练。", "method": "研究人员使用ModernBERT架构，结合FlashAttention、旋转嵌入和GLU前馈层等优化，在一个包含超过6000万专利记录的语料库上预训练了三个领域特定的掩码语言模型。在四个专利分类任务上进行评估。", "result": "ModernBERT-base-PT模型在三个数据集上优于通用ModernBERT基线模型，并在一个数据集上与PatentBERT基线模型表现出竞争力。更大的模型（ModernBERT-base-VX和Mosaic-BERT-large）和自定义的tokenizer进一步提升了性能，所有ModernBERT变体都具有更快的推理速度（超过PatentBERT 3倍）。", "conclusion": "本文的研究表明，领域特定的预训练和架构改进对于专利相关的NLP任务具有显著优势，并且ModernBERT模型在性能和效率之间取得了良好的平衡，适用于对时间敏感的应用。"}}
{"id": "2509.15174", "title": "SMARTER: A Data-efficient Framework to Improve Toxicity Detection with Explanation via Self-augmenting Large Language Models", "authors": ["Huy Nghiem", "Advik Sachdeva", "Hal Daumé III"], "summary": "WARNING: This paper contains examples of offensive materials. Toxic content has become pervasive on social media platforms. We introduce SMARTER, a data-efficient two-stage framework for explainable content moderation using Large Language Models (LLMs). In Stage 1, we leverage LLMs' own outputs to generate synthetic explanations for both correct and incorrect labels, enabling alignment via preference optimization with minimal human supervision. In Stage 2, we refine explanation quality through cross-model training, allowing weaker models to align stylistically and semantically with stronger ones. Experiments on three benchmark tasks -- HateXplain, Latent Hate, and Implicit Hate -- demonstrate that SMARTER enables LLMs to achieve up to a 13.5% macro-F1 improvement over standard few-shot baselines while using only a fraction of the full training data. Our framework offers a scalable strategy for low-resource settings by harnessing LLMs' self-improving capabilities for both classification and explanation.", "abs": "", "categories": ["cs.CL", "cs.AI"], "AI": {"tldr": "本文提出了SMARTER框架，利用大型语言模型（LLMs）进行数据高效且可解释的内容审核，通过自我增强和跨模型训练，在低资源环境下显著提升毒性检测的性能和可解释性。", "motivation": "社交媒体上毒性内容泛滥，传统的内容审核依赖人工，效率低且成本高。现有机器学习模型需要大量数据训练，且缺乏可解释性，难以获得用户的信任。", "method": "SMARTER框架包含两个阶段：第一阶段，利用LLMs自身输出生成合成解释，通过偏好优化进行对齐；第二阶段，通过跨模型训练，使较弱的模型在风格和语义上与较强的模型对齐。实验使用了HateXplain、Latent Hate和Implicit Hate三个基准数据集，并采用Llama-3.1-8B-Instruct和COT-T5-XL等开源LLMs。", "result": "实验结果表明，SMARTER框架在few-shot设置下，在三个基准数据集上分别实现了高达13.5%的macro-F1提升，并且只需要少量训练数据。", "conclusion": "SMARTER提供了一种可扩展且实用的方法，使低资源NLP从业者能够在内容审核中实现强大的性能和可解释性，为构建更透明、可信赖的内容审核系统提供了新的思路。"}}
{"id": "2509.15098", "title": "TextMine: LLM-Powered Knowledge Extraction for Humanitarian Mine Action", "authors": ["Chenyue Zhou", "Gürkan Solmaz", "Flavio Cirillo", "Kiril Gashteovski", "Jonathan Fürst"], "summary": "Humanitarian Mine Action has generated extensive best-practice knowledge, but much remains locked in unstructured reports. We introduce TextMine, an ontology-guided pipeline that uses Large Language Models to extract knowledge triples from HMA texts. TextMine integrates document chunking, domain-aware prompting, triple extraction, and both reference-based and LLM-as-a-Judge evaluation. We also create the first HMA ontology and a curated dataset of real-world demining reports. Experiments show ontology-aligned prompts boost extraction accuracy by 44.2%, cut hallucinations by 22.5%, and improve format conformance by 20.9% over baselines. While validated on Cambodian reports, TextMine can adapt to global demining efforts or other domains, transforming unstructured data into structured knowledge.", "abs": "", "categories": ["cs.CL", "cs.AI"], "AI": {"tldr": "本文提出了TextMine，一个基于大型语言模型(LLM)的知识抽取流程，用于从人道主义扫雷行动(HMA)报告中提取结构化知识。该流程结合了领域知识图谱引导和多维度评估，有效提升了知识抽取准确性和效率。", "motivation": "人道主义扫雷行动积累了大量最佳实践知识，但这些知识通常以非结构化报告形式存在，难以有效利用。本文旨在将这些非结构化数据转化为结构化知识，以支持决策制定和提高行动效率。", "method": "TextMine流程包括文档分块、领域知识图谱引导的提示词设计、三元组抽取以及基于参考和LLM-as-a-Judge的评估。该流程利用LLM进行推理，并使用专门构建的HMA知识图谱和数据集进行训练和评估。", "result": "实验结果表明，与基线方法相比，TextMine通过领域知识图谱引导的提示词，提取准确率提高了44.2%，幻觉减少了22.5%，格式符合性提高了20.9%。", "conclusion": "TextMine为将非结构化数据转化为结构化知识提供了一种有效的方法，可以应用于全球扫雷行动或其他领域，并有望促进人道主义扫雷行动的效率和效果。"}}
{"id": "2509.15027", "title": "CLEAR: A Comprehensive Linguistic Evaluation of Argument Rewriting by Large Language Models", "authors": ["Thomas Huber", "Christina Niklaus"], "summary": "While LLMs have been extensively studied on general text generation tasks, there is less research on text rewriting, a task related to general text generation, and particularly on the behavior of models on this task. In this paper we analyze what changes LLMs make in a text rewriting setting. We focus specifically on argumentative texts and their improvement, a task named Argument Improvement (ArgImp). We present CLEAR: an evaluation pipeline consisting of 57 metrics mapped to four linguistic levels: lexical, syntactic, semantic and pragmatic. This pipeline is used to examine the qualities of LLM-rewritten arguments on a broad set of argumentation corpora and compare the behavior of different LLMs on this task and analyze the behavior of different LLMs on this task in terms of linguistic levels. By taking all four linguistic levels into consideration, we find that the models perform ArgImp by shortening the texts while simultaneously increasing average word length and merging sentences. Overall we note an increase in the persuasion and coherence dimensions.", "abs": "", "categories": ["cs.CL", "cs.AI"], "AI": {"tldr": "本文提出了CLEAR评估框架，用于全面评估大型语言模型（LLMs）在论证改进（ArgImp）任务中的行为，并发现模型倾向于缩短文本、增加平均单词长度和合并句子，从而提高论证的说服力和连贯性。", "motivation": "尽管LLMs在通用文本生成任务中表现出色，但对其在文本改写，特别是论证改进方面的行为研究较少。本文旨在填补这一空白，深入分析LLMs在ArgImp任务中的具体改写策略。", "method": "研究团队构建了CLEAR评估框架，包含57个指标，涵盖词汇、句法、语义和语用四个语言层面。该框架应用于多个论证语料库，比较不同LLMs在ArgImp任务中的表现，并分析其在不同语言层面的行为。", "result": "研究发现，LLMs在进行ArgImp时，主要通过缩短文本、增加平均单词长度和合并句子来实现。同时，论证的说服力和连贯性也得到了提高。", "conclusion": "CLEAR框架为评估LLMs在论证改进任务中的表现提供了一个全面的工具，有助于更好地理解LLMs在论证写作中的作用，并为提升论证质量提供指导。"}}
{"id": "2509.14930", "title": "Cross-Modal Knowledge Distillation for Speech Large Language Models", "authors": ["Enzhi Wang", "Qicheng Li", "Zhiyuan Tang", "Yuhang Jia"], "summary": "In this work, we present the first systematic evaluation of catastrophic forgetting and modality inequivalence in speech large language models, showing that introducing speech capabilities can degrade knowledge and reasoning even when inputs remain textual, and performance further decreases with spoken queries. To address these challenges, we propose a cross-modal knowledge distillation framework that leverages both text-to-text and speech-to-text channels to transfer knowledge from a text-based teacher model to a speech LLM. Extensive experiments on dialogue and audio understanding tasks validate the effectiveness of our approach in preserving textual knowledge, improving cross-modal alignment, and enhancing reasoning in speech-based interactions.", "abs": "", "categories": ["cs.CL", "cs.AI"], "AI": {"tldr": "本文研究了语音大型语言模型中灾难性遗忘和跨模态不一致性问题，并提出了一种跨模态知识蒸馏框架，通过文本到文本和语音到文本通道，将文本模型的知识迁移到语音LLM，以提升语音交互中的推理能力。", "motivation": "语音LLM在引入语音能力后，知识和推理能力会下降，即使输入仍然是文本。此外，语音输入查询会导致性能进一步下降，这归因于灾难性遗忘和模态不一致性。", "method": "提出了一种跨模态知识蒸馏框架，利用文本到文本和语音到文本通道，将文本教师模型的知识迁移到语音LLM。该框架旨在解决灾难性遗忘和模态不一致性问题。", "result": "在对话和音频理解任务上进行了广泛实验，验证了该方法在保留文本知识、改善跨模态对齐和增强语音交互中的推理能力方面的有效性。", "conclusion": "该研究首次系统地评估了语音LLM中的灾难性遗忘和模态不一致性，并提供了一种有效的解决方案，有望提升语音LLM的性能和应用潜力。"}}
{"id": "2509.14886", "title": "A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation", "authors": ["Ye Shen", "Junying Wang", "Farong Wen", "Yijin Guo", "Qi Jia", "Zicheng Zhang", "Guangtao Zhai"], "summary": "The rapid progress of Multi-Modal Large Language Models (MLLMs) has spurred the creation of numerous benchmarks. However, conventional full-coverage Question-Answering evaluations suffer from high redundancy and low efficiency. Inspired by human interview processes, we propose a multi-to-one interview paradigm for efficient MLLM evaluation. Our framework consists of (i) a two-stage interview strategy with pre-interview and formal interview phases, (ii) dynamic adjustment of interviewer weights to ensure fairness, and (iii) an adaptive mechanism for question difficulty-level chosen. Experiments on different benchmarks show that the proposed paradigm achieves significantly higher correlation with full-coverage results than random sampling, with improvements of up to 17.6% in PLCC and 16.7% in SRCC, while reducing the number of required questions. These findings demonstrate that the proposed paradigm provides a reliable and efficient alternative for large-scale MLLM benchmarking.", "abs": "", "categories": ["cs.CL", "cs.AI"], "AI": {"tldr": "本文提出了一种“多对一”访谈范式，用于更高效地评估多模态大型语言模型（MLLM），通过两阶段访谈、动态权重调整和自适应难度机制，显著提高了评估效率和准确性。", "motivation": "现有MLLM评估方法（如全覆盖问答）存在冗余问题，效率低下。受现实世界招聘实践启发，需要更高效的评估范式。", "method": "提出了“多对一”访谈范式，包含：(1) 两阶段访谈策略（预访谈和正式访谈）；(2) 动态调整访谈者权重以保证公平性；(3) 自适应难度机制选择问题难度。", "result": "实验表明，该范式在PLCC和SRCC指标上分别提高了17.6%和16.7%，同时减少了所需问题数量，显著优于随机抽样，与全覆盖结果具有更高的相关性。", "conclusion": "该“多对一”访谈范式为大规模MLLM基准测试提供了一种可靠且高效的替代方案，能够全面、准确、公平地评估MLLM的能力。"}}
