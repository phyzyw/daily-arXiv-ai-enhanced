{"id": "2601.22009", "title": "MEIDNet: Multimodal generative AI framework for inverse materials design", "authors": ["Anand Babu", "Rogério Almeida Gouvêa", "Pierre Vandergheynst", "Gian-Marco Rignanese"], "summary": "In this work, we present Multimodal Equivariant Inverse Design Network (MEIDNet), a framework that jointly learns structural information and materials properties through contrastive learning, while encoding structures via an equivariant graph neural network (EGNN). By combining generative inverse design with multimodal learning, our approach accelerates the exploration of chemical-structural space and facilitates the discovery of materials that satisfy predefined property targets. MEIDNet exhibits strong latent-space alignment with cosine similarity 0.96 by fusion of three modalities through cross-modal learning. Through implementation of curriculum learning strategies, MEIDNet achieves ~60 times higher learning efficiency than conventional training techniques. The potential of our multimodal approach is demonstrated by generating low-bandgap perovskite structures at a stable, unique, and novel (SUN) rate of 13.6 %, which are further validated by ab initio methods. Our inverse design framework demonstrates both scalability and adaptability, paving the way for the universal learning of chemical space across diverse modalities.", "published": "2026-01-29", "categories": ["cond-mat.mtrl-sci", "cs.AI", "cs.LG", "physics.comp-ph"], "pdf_url": "https://arxiv.org/pdf/2601.22009v1", "primary_category": "cond-mat.mtrl-sci"}
{"id": "2601.21056", "title": "Better without U: Impact of Selective Hubbard U Correction on Foundational MLIPs", "authors": ["Thomas Warford", "Fabian L. Thiemann", "Gábor Csányi"], "summary": "The training of foundational machine learning interatomic potentials (fMLIPs) relies on diverse databases with energies and forces calculated using ab initio methods. We show that fMLIPs trained on large datasets such as MPtrj, Alexandria, and OMat24 encode inconsistencies from the Materials Project's selective use of the Hubbard U correction, which is applied to certain transition metals only if O or F atoms are present in the simulation cell. This inconsistent use of +U creates two incompatible potential-energy surfaces (PES): a lower-energy GGA surface and a higher-energy GGA+U one. When trained on both, MLIPs interpolate between them, leading to systematic underbinding, or even spurious repulsion, between U-corrected metals and oxygen- or fluorine-containing species. Models such as MACE-OMAT and -MPA exhibit repulsion between U-corrected metals and their oxides, limiting their value for studying catalysis and oxidation. We link the severity of this pathology to the oxygen number density in U-corrected training configurations. This explains why OMAT-trained models are most affected and suggests the issue might worsen as expanding future datasets increasingly include configurations with low oxygen content, such as those generated through combinatorial exploration of multi-element or defect-containing systems.   Our simple per-U-corrected-atom shift aligns PBE+U and PBE energies for identical structures, yielding a smoother PES compared to existing correction schemes, which target phase diagram accuracy. As a result, models trained on datasets with our shift applied exhibit smaller mean absolute errors for the adsorption energies of oxygen on U-corrected elemental slabs. Since datasets omitting +U entirely (e.g. MatPES, MP-ALOE) avoid these pathologies, we recommend excluding +U in future fMLIP datasets. For existing datasets, our post-hoc correction provides a low-cost improvement.", "published": "2026-01-28", "categories": ["physics.chem-ph", "cond-mat.mtrl-sci", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2601.21056v1", "primary_category": "physics.chem-ph"}
{"id": "2601.21945", "title": "Dependence of Equilibrium Propagation Training Success on Network Architecture", "authors": ["Qingshan Wang", "Clara C. Wanjura", "Florian Marquardt"], "summary": "The rapid rise of artificial intelligence has led to an unsustainable growth in energy consumption. This has motivated progress in neuromorphic computing and physics-based training of learning machines as alternatives to digital neural networks. Many theoretical studies focus on simple architectures like all-to-all or densely connected layered networks. However, these may be challenging to realize experimentally, e.g. due to connectivity constraints. In this work, we investigate the performance of the widespread physics-based training method of equilibrium propagation for more realistic architectural choices, specifically, locally connected lattices. We train an XY model and explore the influence of architecture on various benchmark tasks, tracking the evolution of spatially distributed responses and couplings during training. Our results show that sparse networks with only local connections can achieve performance comparable to dense networks. Our findings provide guidelines for further scaling up architectures based on equilibrium propagation in realistic settings.", "published": "2026-01-29", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.ET", "cs.NE"], "pdf_url": "https://arxiv.org/pdf/2601.21945v1", "primary_category": "cs.LG"}
{"id": "2601.21522", "title": "More Bang for the Buck: Improving the Inference of Large Language Models at a Fixed Budget using Reset and Discard (ReD)", "authors": ["Sagi Meir", "Tommer D. Keidar", "Noam Levi", "Shlomi Reuveni", "Barak Hirshberg"], "summary": "The performance of large language models (LLMs) on verifiable tasks is usually measured by pass@k, the probability of answering a question correctly at least once in k trials. At a fixed budget, a more suitable metric is coverage@cost, the average number of unique questions answered as a function of the total number of attempts. We connect the two metrics and show that the empirically-observed power-law behavior in pass@k leads to a sublinear growth of the coverage@cost (diminishing returns). To solve this problem, we propose Reset-and-Discard (ReD), a query method of LLMs that increases coverage@cost for any given budget, regardless of the pass@k form. Moreover, given a pass@k, we can quantitatively predict the savings in the total number of attempts using ReD. If pass@k is not available for the model, ReD can infer its power-law exponent. Experiments on three LLMs using HumanEval demonstrate that ReD substantially reduces the required attempts, tokens, and USD cost to reach a desired coverage, while also offering an efficient way to measure inference power-laws.", "published": "2026-01-29", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.AI", "stat.ML"], "pdf_url": "https://arxiv.org/pdf/2601.21522v1", "primary_category": "cs.LG"}
{"id": "2601.21219", "title": "Soft Quantization: Model Compression Via Weight Coupling", "authors": ["Daniel T. Bernstein", "Luca Di Carlo", "David Schwab"], "summary": "We show that introducing short-range attractive couplings between the weights of a neural network during training provides a novel avenue for model quantization. These couplings rapidly induce the discretization of a model's weight distribution, and they do so in a mixed-precision manner despite only relying on two additional hyperparameters. We demonstrate that, within an appropriate range of hyperparameters, our \"soft quantization'' scheme outperforms histogram-equalized post-training quantization on ResNet-20/CIFAR-10. Soft quantization provides both a new pipeline for the flexible compression of machine learning models and a new tool for investigating the trade-off between compression and generalization in high-dimensional loss landscapes.", "published": "2026-01-29", "categories": ["cs.LG", "cond-mat.dis-nn"], "pdf_url": "https://arxiv.org/pdf/2601.21219v1", "primary_category": "cs.LG"}
