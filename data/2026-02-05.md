<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 4]
- [physics.chem-ph](#physics.chem-ph) [Total: 3]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Generative Neural Operators through Diffusion Last Layer](https://arxiv.org/abs/2602.04139)
*Sungwon Park, Anthony Zhou, Hongjoong Kim, Amir Barati Farimani*

Main category: cs.LG

TL;DR: 本文提出了一种名为Diffusion Last Layer (DLL) 的轻量级概率头，可以附加到任何神经算子骨干网络，以对预测不确定性进行建模，从而提升泛化能力和不确定性感知预测。


<details>
  <summary>Details</summary>
Motivation: 许多实际系统本质上是随机的，因此对不确定性的量化对于可靠部署至关重要。现有的神经算子通常只能提供确定性预测，无法有效处理随机性。

Method: DLL通过低秩卡尔胡恩-洛维（K-L）展开，直接在函数空间中参数化条件输出分布。它作为一种附加模块，可以轻松地附加到现有的神经算子骨干网络，无需重新训练整个模型。

Result: 在随机PDE算子学习基准测试中，DLL提高了泛化能力和不确定性感知预测。即使在确定性的长时间演化设置中，DLL也能增强演化稳定性，并为骨干神经算子提供有意义的表观不确定性估计。

Conclusion: DLL提供了一种简单有效的方法，将现有的神经算子转化为生成模型，从而能够对预测不确定性进行建模，并为科学计算中的随机系统提供更可靠的预测。

Abstract: Neural operators have emerged as a powerful paradigm for learning discretization-invariant function-to-function mappings in scientific computing. However, many practical systems are inherently stochastic, making principled uncertainty quantification essential for reliable deployment. To address this, we introduce a simple add-on, the diffusion last layer (DLL), a lightweight probabilistic head that can be attached to arbitrary neural operator backbones to model predictive uncertainty. Motivated by the relative smoothness and low-dimensional structure often exhibited by PDE solution distributions, DLL parameterizes the conditional output distribution directly in function space through a low-rank Karhunen-Loève expansion, enabling efficient and expressive uncertainty modeling. Across stochastic PDE operator learning benchmarks, DLL improves generalization and uncertainty-aware prediction. Moreover, even in deterministic long-horizon rollout settings, DLL enhances rollout stability and provides meaningful estimates of epistemic uncertainty for backbone neural operators.

</details>


### [2] [From Evaluation to Design: Using Potential Energy Surface Smoothness Metrics to Guide Machine Learning Interatomic Potential Architectures](https://arxiv.org/abs/2602.04861)
*Ryan Liu, Eric Qu, Tobias Kreiman, Samuel M. Blau, Aditi S. Krishnapriyan*

Main category: cs.LG

TL;DR: 本文提出了一种名为Bond Smoothness Characterization Test (BSCT) 的新方法，用于评估机器学习原子势 (MLIP) 的势能面 (PES) 平滑性，并将其应用于指导 MLIP 模型设计，以提高其物理可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的 MLIP 评估方法（如微正则分子动力学模拟）计算成本高昂，且主要关注近平衡态。即使回归误差低，MLIP 也可能无法准确再现量子 PES 的物理平滑性，导致模拟不稳定和非物理行为。

Method: BSCT 通过控制键的变形来探测 PES，检测非平滑性，包括不连续性、人为极小值和虚假力。研究人员定义了力平滑偏差 (FSD) 作为 BSCT 的低成本指标，并将其与 MD 稳定性相关联。他们还使用 BSCT 作为“回路内”诊断工具，指导基于 Transformer 架构的 MLIP 模型设计，并引入了可微分 k 近邻算法和温度控制注意力等改进。

Result: BSCT 与 MD 稳定性高度相关，且计算成本远低于 MD。通过 BSCT 指导的模型设计，MLIP 能够同时实现低回归误差、稳定的 MD 模拟和鲁棒的原子性质预测。

Conclusion: BSCT 不仅是一种验证指标，也是一种“回路内”模型设计代理，可以提醒 MLIP 开发人员注意当前 MLIP 基准测试无法有效评估的物理挑战，为开发更可靠的 MLIP 提供了实用框架。

Abstract: Machine Learning Interatomic Potentials (MLIPs) sometimes fail to reproduce the physical smoothness of the quantum potential energy surface (PES), leading to erroneous behavior in downstream simulations that standard energy and force regression evaluations can miss. Existing evaluations, such as microcanonical molecular dynamics (MD), are computationally expensive and primarily probe near-equilibrium states. To improve evaluation metrics for MLIPs, we introduce the Bond Smoothness Characterization Test (BSCT). This efficient benchmark probes the PES via controlled bond deformations and detects non-smoothness, including discontinuities, artificial minima, and spurious forces, both near and far from equilibrium. We show that BSCT correlates strongly with MD stability while requiring a fraction of the cost of MD. To demonstrate how BSCT can guide iterative model design, we utilize an unconstrained Transformer backbone as a testbed, illustrating how refinements such as a new differentiable $k$-nearest neighbors algorithm and temperature-controlled attention reduce artifacts identified by our metric. By optimizing model design systematically based on BSCT, the resulting MLIP simultaneously achieves a low conventional E/F regression error, stable MD simulations, and robust atomistic property predictions. Our results establish BSCT as both a validation metric and as an "in-the-loop" model design proxy that alerts MLIP developers to physical challenges that cannot be efficiently evaluated by current MLIP benchmarks.

</details>


### [3] [Protein Autoregressive Modeling via Multiscale Structure Generation](https://arxiv.org/abs/2602.04883)
*Yanru Qu, Cheng-Yen Hsieh, Zaixiang Zheng, Ge Liu, Quanquan Gu*

Main category: cs.LG

TL;DR: 本文提出了PAR（Protein Autoregressive Modeling），一个多尺度自回归框架，用于蛋白质骨架生成，有效解决了自回归模型在蛋白质结构生成中的暴露偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质结构生成模型主要基于扩散模型，而自回归模型在其他领域表现出色。本文旨在探索如何将自回归建模应用于蛋白质骨架设计，克服其在连续数据建模和蛋白质双向依赖性方面的挑战。

Method: PAR框架通过多尺度降采样、自回归Transformer和基于流的骨架解码器实现。Transformer编码多尺度信息并生成条件嵌入，解码器则根据这些嵌入生成骨架原子。此外，PAR采用了噪声上下文学习和计划采样来缓解暴露偏差。

Result: PAR在无条件生成基准测试中表现出强大的零样本泛化能力，能够学习蛋白质分布并生成高质量的骨架，并具有良好的扩展性。它支持灵活的人工提示条件生成和motif骨架构建。

Conclusion: PAR作为一种有前景的蛋白质结构生成框架，通过利用蛋白质的多尺度层次结构，解锁了自回归模型在蛋白质设计中的潜力，为生物医学和纳米技术等领域提供了新的工具。

Abstract: We present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for protein backbone generation via coarse-to-fine next-scale prediction. Using the hierarchical nature of proteins, PAR generates structures that mimic sculpting a statue, forming a coarse topology and refining structural details over scales. To achieve this, PAR consists of three key components: (i) multi-scale downsampling operations that represent protein structures across multiple scales during training; (ii) an autoregressive transformer that encodes multi-scale information and produces conditional embeddings to guide structure generation; (iii) a flow-based backbone decoder that generates backbone atoms conditioned on these embeddings. Moreover, autoregressive models suffer from exposure bias, caused by the training and the generation procedure mismatch, and substantially degrades structure generation quality. We effectively alleviate this issue by adopting noisy context learning and scheduled sampling, enabling robust backbone generation. Notably, PAR exhibits strong zero-shot generalization, supporting flexible human-prompted conditional generation and motif scaffolding without requiring fine-tuning. On the unconditional generation benchmark, PAR effectively learns protein distributions and produces backbones of high design quality, and exhibits favorable scaling behavior. Together, these properties establish PAR as a promising framework for protein structure generation.

</details>


### [4] [Theory of Speciation Transitions in Diffusion Models with General Class Structure](https://arxiv.org/abs/2602.04404)
*Beatrice Achilli, Marco Benedetti, Giulio Biroli, Marc Mézard*

Main category: cs.LG

TL;DR: 本文提出了一种通用的扩散模型类化转变理论，该理论适用于任意目标分布，即使类之间无法通过一阶矩区分，也能预测类化时间。


<details>
  <summary>Details</summary>
Motivation: 现有类化转变理论仅限于一阶矩可区分的混合高斯模型，本文旨在扩展该理论，使其适用于更广泛的场景，并理解扩散模型中轨迹如何逐渐承诺于特定类别。

Method: 通过贝叶斯分类形式化类结构的概念，并利用自由熵差来表征类化时间。作者在混合一维伊辛模型和混合零均高斯模型上进行了验证，并使用复制方法求解伊辛模型。

Result: 该理论能够预测连续的类化时间，对应于越来越细粒度的类别承诺。在伊辛模型中，得到了类化时间的显式表达式。

Conclusion: 本文提供了一种统一且广泛适用的描述扩散模型中类化转变的框架，为理解和控制扩散生成模型提供了新的视角。

Abstract: Diffusion Models generate data by reversing a stochastic diffusion process, progressively transforming noise into structured samples drawn from a target distribution. Recent theoretical work has shown that this backward dynamics can undergo sharp qualitative transitions, known as speciation transitions, during which trajectories become dynamically committed to data classes. Existing theoretical analyses, however, are limited to settings where classes are identifiable through first moments, such as mixtures of Gaussians with well-separated means. In this work, we develop a general theory of speciation in diffusion models that applies to arbitrary target distributions admitting well-defined classes. We formalize the notion of class structure through Bayes classification and characterize speciation times in terms of free-entropy difference between classes. This criterion recovers known results in previously studied Gaussian-mixture models, while extending to situations in which classes are not distinguishable by first moments and may instead differ through higher-order or collective features. Our framework also accommodates multiple classes and predicts the existence of successive speciation times associated with increasingly fine-grained class commitment. We illustrate the theory on two analytically tractable examples: mixtures of one-dimensional Ising models at different temperatures and mixtures of zero-mean Gaussians with distinct covariance structures. In the Ising case, we obtain explicit expressions for speciation times by mapping the problem onto a random-field Ising model and solving it via the replica method. Our results provide a unified and broadly applicable description of speciation transitions in diffusion-based generative models.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [5] [Beyond Learning on Molecules by Weakly Supervising on Molecules](https://arxiv.org/abs/2602.04696)
*Gordan Prastalo, Kevin Maik Jablonka*

Main category: physics.chem-ph

TL;DR: 本文提出了一种名为ACE-Mol的新型分子表示模型，它利用程序化衍生的分子motif进行弱监督学习，从而实现任务相关的分子表示，并在分子性质预测基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有分子编码器通常是任务无关的，而手设计的特征又缺乏灵活性和可扩展性。为了克服任务相关性和表示学习之间的权衡，并避免昂贵的标注数据，需要一种能够有效适应不同任务的分子表示方法。

Method: ACE-Mol模型通过弱监督学习，利用成百上千个分子motif（结构单元）与自然语言描述的配对进行训练。这种方法将任务信息融入到分子表示中，使得模型能够立即将表示与任务对齐，而无需像传统编码器那样在嵌入空间中缓慢搜索。

Result: ACE-Mol在分子性质预测基准测试中实现了最先进的性能，并且生成的分子表示具有可解释性和化学意义。

Conclusion: ACE-Mol模型证明了弱监督学习在任务相关分子表示学习中的有效性，它能够快速适应任务，并生成稳定的、具有化学意义的分子表示，为分子性质预测和药物发现等领域提供了新的思路。

Abstract: Molecular representations are inherently task-dependent, yet most pre-trained molecular encoders are not. Task conditioning promises representations that reorganize based on task descriptions, but existing approaches rely on expensive labeled data. We show that weak supervision on programmatically derived molecular motifs is sufficient. Our Adaptive Chemical Embedding Model (ACE-Mol) learns from hundreds of motifs paired with natural language descriptors that are cheap to compute, trivial to scale. Conventional encoders slowly search the embedding space for task-relevant structure, whereas ACE-Mol immediately aligns its representations with the task. ACE-Mol achieves state-of-the-art performance across molecular property prediction benchmarks with interpretable, chemically meaningful representations.

</details>


### [6] [El Agente Quntur: A research collaborator agent for quantum chemistry](https://arxiv.org/abs/2602.04850)
*Juan B. Pérez-Sánchez, Yunheng Zou, Jorge A. Campos-Gonzalez-Angulo, Marcel Müller, Ignacio Gustin, Andrew Wang, Han Hao, Tsz Wai Ko, Changhyeok Choi, Eric S. Isbrandt, Mohammad Ghazi Vakili, Hanyong Xu, Chris Crebolder, Varinia Bernales, Alán Aspuru-Guzik*

Main category: physics.chem-ph

TL;DR: 本文介绍了一种名为El Agente Quntur的AI研究协作代理，旨在简化量子化学计算，使其更易于非专业人士使用，并扩展其在化学、材料科学等领域的应用。


<details>
  <summary>Details</summary>
Motivation: 量子化学是重要的研究工具，但其复杂性限制了其应用范围。为了弥合专业知识差距，并使更广泛的化学家能够利用这些工具，需要一种易于访问且智能的解决方案。

Method: Quntur是一个分层、多代理AI系统，它通过消除硬编码策略，采用基于推理的决策；构建通用且可组合的操作以提高效率；以及实施引导式深度研究来整合量子化学推理。该系统基于ORCA 6.0，并能够理解软件文档和科学文献，从而规划、执行、调整和分析体外化学实验。

Result: Quntur支持ORCA 6.0中的所有计算，并能够根据最佳实践进行体外化学实验的规划、执行、调整和分析。

Conclusion: Quntur的设计原则适用于更广泛的研究代理，并可轻松扩展到其他量子化学软件包。该研究为构建完全自主的端到端量子化学研究代理奠定了基础，并讨论了在研究级别上运行代理系统中的进展和瓶颈。

Abstract: Quantum chemistry is a foundational enabling tool for the fields of chemistry, materials science, computational biology and others. Despite of its power, the practical application of quantum chemistry simulations remains in the hands of qualified experts due to methodological complexity, software heterogeneity, and the need for informed interpretation of results. To bridge the accessibility gap for these tools and expand their reach to chemists with broader backgrounds, we introduce El Agente Quntur, a hierarchical, multi-agent AI system designed to operate not merely as an automation tool but as a research collaborator for computational quantum chemistry. Quntur was designed following three main strategies: i) elimination of hard-coded procedural policies in favour of reasoning-driven decisions, ii) construction of general and composable actions that facilitate generalization and efficiency, and iii) implementation of guided deep research to integrate abstract quantum-chemical reasoning across subdisciplines and a detailed understanding of the software's internal logic and syntax. Although instantiated in ORCA, these design principles are applicable to research agents more generally and easily expandable to additional quantum chemistry packages and beyond. Quntur supports the full range of calculations available in ORCA 6.0 and reasons over software documentation and scientific literature to plan, execute, adapt, and analyze in silico chemistry experiments following best practices. We discuss the advances and current bottlenecks in agentic systems operating at the research level in computational chemistry, and outline a roadmap toward a fully autonomous end-to-end computational chemistry research agent.

</details>


### [7] [El Agente Estructural: An Artificially Intelligent Molecular Editor](https://arxiv.org/abs/2602.04849)
*Changhyeok Choi, Yunheng Zou, Marcel Müller, Han Hao, Yeonghun Kang, Juan B. Pérez-Sánchez, Ignacio Gustin, Hanyong Xu, Mohammad Ghazi Vakili, Chris Crebolder, Alán Aspuru-Guzik, Varinia Bernales*

Main category: physics.chem-ph

TL;DR: 本文提出了El Agente Estructural，一个多模态、自然语言驱动的分子编辑智能体，它模仿人类专家直接操作分子系统，实现精确的分子几何操作。


<details>
  <summary>Details</summary>
Motivation: 现有的计算化学工作流程在处理复杂分子几何和修改方面仍面临挑战，而分子几何对化学性质至关重要，因此需要更灵活和可控的分子几何构建和操作方法。

Method: El Agente Estructural 集成了全面的领域知识工具和视觉-语言模型，通过直接操作分子系统，实现对原子或功能团替换、原子连接性和立体化学的精确控制，无需重建核心分子框架。它支持多种任务，包括位点选择性功能化、配体结合、配体交换、立体化学控制的结构构建、异构体互变、片段级结构分析、图像引导的结构生成以及机制驱动的几何生成和修改。

Result: 通过一系列案例研究，证明了 El Agente Estructural 能够在各种真实场景中进行有意义的分子几何操作，并可集成到 El Agente Quntur 自主量子化学平台中，进一步增强其能力。

Conclusion: 该研究表明，结合多模态推理和专门的几何感知工具，可以支持交互式和上下文感知的分子建模，超越了简单的结构生成，为计算化学研究和理解结构-性质关系提供了新的途径。

Abstract: We present El Agente Estructural, a multimodal, natural-language-driven geometry-generation and manipulation agent for autonomous chemistry and molecular modelling. Unlike molecular generation or editing via generative models, Estructural mimics how human experts directly manipulate molecular systems in three dimensions by integrating a comprehensive set of domain-informed tools and vision-language models. This design enables precise control over atomic or functional group replacements, atomic connectivity, and stereochemistry without the need to rebuild extensive core molecular frameworks. Through a series of representative case studies, we demonstrate that Estructural enables chemically meaningful geometry manipulation across a wide range of real-world scenarios. These include site-selective functionalization, ligand binding, ligand exchange, stereochemically controlled structure construction, isomer interconversion, fragment-level structural analysis, image-guided generation of structures from schematic reaction mechanisms, and mechanism-driven geometry generation and modification. These examples illustrate how multimodal reasoning, when combined with specialized geometry-aware tools, supports interactive and context-aware molecular modelling beyond structure generation. Looking forward, the integration of Estructural into El Agente Quntur, an autonomous multi-agent quantum chemistry platform, enhances its capabilities by adding sophisticated tools for the generation and editing of three-dimensional structures.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [8] [Theory of Optimal Learning Rate Schedules and Scaling Laws for a Random Feature Model](https://arxiv.org/abs/2602.04774)
*Blake Bordelon, Francesco Mori*

Main category: cond-mat.dis-nn

TL;DR: 本文研究了随机特征模型中随机梯度下降(SGD)的优化学习率策略，并揭示了学习率随训练迭代变化的规律，提出了易阶段和难阶段两种不同的学习率调度方案。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型训练中学习率的选择至关重要，但通常依赖经验试错。本文旨在建立理论框架，理解和预测学习率如何随训练迭代和模型规模变化，并探索学习率在不同训练周期之间的迁移。

Method: 本文使用最优控制方法，分析了随机特征模型中SGD动态，计算了最优学习率调度策略 η⋆(t)，并对学习率和批次大小进行了联合优化。通过数值和解析方法，研究了易阶段和难阶段的学习率行为。

Result: 研究发现，在易阶段，最优学习率调度策略呈多项式衰减；在难阶段，则类似于预热-稳定-衰减策略。此外，还推导了计算最优的缩放定律，并对学习率迁移依赖于模型和任务结构提出了理论。

Conclusion: 本文的理论工作为理解和设计更有效的学习率调度策略提供了指导，有助于提高深度学习模型的训练效率和泛化能力，并为学习率在不同训练周期之间的迁移提供了新的视角。

Abstract: Setting the learning rate for a deep learning model is a critical part of successful training, yet choosing this hyperparameter is often done empirically with trial and error. In this work, we explore a solvable model of optimal learning rate schedules for a powerlaw random feature model trained with stochastic gradient descent (SGD). We consider the optimal schedule $η_T^\star(t)$ where $t$ is the current iterate and $T$ is the total training horizon. This schedule is computed both numerically and analytically (when possible) using optimal control methods. Our analysis reveals two regimes which we term the easy phase and hard phase. In the easy phase the optimal schedule is a polynomial decay $η_T^\star(t) \simeq T^{-ξ} (1-t/T)^δ$ where $ξ$ and $δ$ depend on the properties of the features and task. In the hard phase, the optimal schedule resembles warmup-stable-decay with constant (in $T$) initial learning rate and annealing performed over a vanishing (in $T$) fraction of training steps. We investigate joint optimization of learning rate and batch size, identifying a degenerate optimality condition. Our model also predicts the compute-optimal scaling laws (where model size and training steps are chosen optimally) in both easy and hard regimes. Going beyond SGD, we consider optimal schedules for the momentum $β(t)$, where speedups in the hard phase are possible. We compare our optimal schedule to various benchmarks in our task including (1) optimal constant learning rates $η_T(t) \sim T^{-ξ}$ (2) optimal power laws $η_T(t) \sim T^{-ξ} t^{-χ}$, finding that our schedule achieves better rates than either of these. Our theory suggests that learning rate transfer across training horizon depends on the structure of the model and task. We explore these ideas in simple experimental pretraining setups.

</details>
