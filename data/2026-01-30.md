<div id=toc></div>

# Table of Contents

- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [1] [MEIDNet: Multimodal generative AI framework for inverse materials design](https://arxiv.org/abs/2601.22009)
*Anand Babu, Rogério Almeida Gouvêa, Pierre Vandergheynst, Gian-Marco Rignanese*

Main category: cond-mat.mtrl-sci

TL;DR: 本文提出了MEIDNet，一个多模态生成式AI框架，通过对比学习和等变图神经网络（EGNN）联合学习材料结构和性质，加速化学-结构空间探索，并实现目标性质材料的发现。


<details>
  <summary>Details</summary>
Motivation: 传统材料探索方法耗时耗力，AI辅助的逆向设计能够更高效地寻找满足预定义功能目标的候选材料。现有框架通常依赖单一信息模态，难以全面捕捉多维性质之间的复杂关系，因此需要多模态机器学习。

Method: MEIDNet框架结合生成式逆向设计和多模态学习，使用EGNN编码结构信息，并通过对比学习融合结构、电子和热力学三种模态。采用课程学习策略，提升训练效率。

Result: MEIDNet在三种数据集上表现出色，实现了三种模态的余弦相似度高达0.96，学习效率提升了约60倍。成功生成了13.6%的稳定、独特和新颖（SUN）的低带隙钙钛矿结构，并通过第一性原理方法验证。

Conclusion: MEIDNet证明了多模态方法在材料设计中的可扩展性和适应性，为通用化学空间学习铺平了道路，有望加速新材料的发现。

Abstract: In this work, we present Multimodal Equivariant Inverse Design Network (MEIDNet), a framework that jointly learns structural information and materials properties through contrastive learning, while encoding structures via an equivariant graph neural network (EGNN). By combining generative inverse design with multimodal learning, our approach accelerates the exploration of chemical-structural space and facilitates the discovery of materials that satisfy predefined property targets. MEIDNet exhibits strong latent-space alignment with cosine similarity 0.96 by fusion of three modalities through cross-modal learning. Through implementation of curriculum learning strategies, MEIDNet achieves ~60 times higher learning efficiency than conventional training techniques. The potential of our multimodal approach is demonstrated by generating low-bandgap perovskite structures at a stable, unique, and novel (SUN) rate of 13.6 %, which are further validated by ab initio methods. Our inverse design framework demonstrates both scalability and adaptability, paving the way for the universal learning of chemical space across diverse modalities.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [2] [Better without U: Impact of Selective Hubbard U Correction on Foundational MLIPs](https://arxiv.org/abs/2601.21056)
*Thomas Warford, Fabian L. Thiemann, Gábor Csányi*

Main category: physics.chem-ph

TL;DR: 本文发现，材料项目（MP）中对过渡金属选择性应用Hubbard U校正导致了机器学习原子间势（MLIP）训练数据的不一致性，从而影响了MLIP对U校正金属和含氧/氟物种的描述。本文提出了一种简单的后处理校正方法，可以改善MLIP的性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习原子间势（MLIP）模型在模拟涉及U校正金属和含氧/氟物种的体系时表现出问题，这源于训练数据中材料项目（MP）对Hubbard U校正的不一致使用。

Method: 研究人员分析了基于MPtrj、Alexandria和OMat24等大型数据集训练的MLIP模型，并发现这些模型在U校正金属和含氧/氟物种之间表现出排斥力。他们提出了一种简单的后处理校正方法，通过对每个U校正原子进行能量位移，使PBE+U和PBE能量一致。

Result: 研究表明，基于带有校正后的数据集训练的MLIP模型，在氧气在U校正金属板上的吸附能预测方面表现出更小的平均绝对误差。氧原子数密度与病理现象的严重程度相关。

Conclusion: 本文强调了在构建未来MLIP数据集时，应避免使用不一致的Hubbard U校正。提出的后处理校正方法为现有数据集提供了一种低成本的改进途径，有助于提高MLIP在模拟催化和氧化等重要过程中的准确性。

Abstract: The training of foundational machine learning interatomic potentials (fMLIPs) relies on diverse databases with energies and forces calculated using ab initio methods. We show that fMLIPs trained on large datasets such as MPtrj, Alexandria, and OMat24 encode inconsistencies from the Materials Project's selective use of the Hubbard U correction, which is applied to certain transition metals only if O or F atoms are present in the simulation cell. This inconsistent use of +U creates two incompatible potential-energy surfaces (PES): a lower-energy GGA surface and a higher-energy GGA+U one. When trained on both, MLIPs interpolate between them, leading to systematic underbinding, or even spurious repulsion, between U-corrected metals and oxygen- or fluorine-containing species. Models such as MACE-OMAT and -MPA exhibit repulsion between U-corrected metals and their oxides, limiting their value for studying catalysis and oxidation. We link the severity of this pathology to the oxygen number density in U-corrected training configurations. This explains why OMAT-trained models are most affected and suggests the issue might worsen as expanding future datasets increasingly include configurations with low oxygen content, such as those generated through combinatorial exploration of multi-element or defect-containing systems.   Our simple per-U-corrected-atom shift aligns PBE+U and PBE energies for identical structures, yielding a smoother PES compared to existing correction schemes, which target phase diagram accuracy. As a result, models trained on datasets with our shift applied exhibit smaller mean absolute errors for the adsorption energies of oxygen on U-corrected elemental slabs. Since datasets omitting +U entirely (e.g. MatPES, MP-ALOE) avoid these pathologies, we recommend excluding +U in future fMLIP datasets. For existing datasets, our post-hoc correction provides a low-cost improvement.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [3] [More Bang for the Buck: Improving the Inference of Large Language Models at a Fixed Budget using Reset and Discard (ReD)](https://arxiv.org/abs/2601.21522)
*Sagi Meir, Tommer D. Keidar, Noam Levi, Shlomi Reuveni, Barak Hirshberg*

Main category: cs.LG

TL;DR: 本文提出了一种新的LLM推理方法Reset-and-Discard (ReD)，能够在固定预算下显著提高覆盖率，避免了传统方法的边际效益递减问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM性能评估指标pass@k在固定预算下无法准确反映解决不同问题的能力，导致边际效益递减。实际应用中，需要在固定预算下解决尽可能多的问题，因此需要一种新的评估指标coverage@cost和相应的推理策略。

Method: ReD方法通过在固定尝试次数后重置采样并丢弃已解决的问题，实现了“广度优先”的推理策略。该方法避免了在困难问题上浪费过多计算资源，从而提高整体覆盖率。

Result: 实验表明，ReD方法在HumanEval数据集上，显著减少了所需的尝试次数、token数量和USD成本，以达到期望的覆盖率。同时，ReD方法可以有效测量LLM的幂律指数。

Conclusion: ReD方法提供了一种高效且实用的LLM推理策略，能够提高固定预算下的问题解决能力，并为评估LLM的推理能力提供新的视角。

Abstract: The performance of large language models (LLMs) on verifiable tasks is usually measured by pass@k, the probability of answering a question correctly at least once in k trials. At a fixed budget, a more suitable metric is coverage@cost, the average number of unique questions answered as a function of the total number of attempts. We connect the two metrics and show that the empirically-observed power-law behavior in pass@k leads to a sublinear growth of the coverage@cost (diminishing returns). To solve this problem, we propose Reset-and-Discard (ReD), a query method of LLMs that increases coverage@cost for any given budget, regardless of the pass@k form. Moreover, given a pass@k, we can quantitatively predict the savings in the total number of attempts using ReD. If pass@k is not available for the model, ReD can infer its power-law exponent. Experiments on three LLMs using HumanEval demonstrate that ReD substantially reduces the required attempts, tokens, and USD cost to reach a desired coverage, while also offering an efficient way to measure inference power-laws.

</details>


### [4] [Soft Quantization: Model Compression Via Weight Coupling](https://arxiv.org/abs/2601.21219)
*Daniel T. Bernstein, Luca Di Carlo, David Schwab*

Main category: cs.LG

TL;DR: 本文提出了一种名为“软量化”的新型模型压缩方法，通过在训练过程中引入权重之间的短程吸引力耦合，实现模型权重分布的快速离散化，并在压缩性能上优于传统量化方法。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络虽然性能出色，但内存和能耗成本高昂。量化是一种降低成本的有效手段，但传统量化方法通常需要仔细的参数调整或复杂的算法。

Method: 软量化通过在训练过程中引入权重之间的短程吸引力耦合来实现。具体地，在每一层中，权重会受到彼此之间的吸引力影响，导致它们聚集形成离散的簇。该方法仅依赖于两个全局超参数，并根据预训练状态的层统计信息自动调整耦合强度和范围。

Result: 在ResNet-20/CIFAR-10数据集上，软量化方案在适当的超参数范围内，优于基于直方图均衡的训练后量化（HEQ）。

Conclusion: 软量化提供了一种新的灵活的模型压缩途径，并为研究压缩与泛化之间的权衡提供了新的工具，有助于理解高维损失景观中的解决方案。

Abstract: We show that introducing short-range attractive couplings between the weights of a neural network during training provides a novel avenue for model quantization. These couplings rapidly induce the discretization of a model's weight distribution, and they do so in a mixed-precision manner despite only relying on two additional hyperparameters. We demonstrate that, within an appropriate range of hyperparameters, our "soft quantization'' scheme outperforms histogram-equalized post-training quantization on ResNet-20/CIFAR-10. Soft quantization provides both a new pipeline for the flexible compression of machine learning models and a new tool for investigating the trade-off between compression and generalization in high-dimensional loss landscapes.

</details>


### [5] [Dependence of Equilibrium Propagation Training Success on Network Architecture](https://arxiv.org/abs/2601.21945)
*Qingshan Wang, Clara C. Wanjura, Florian Marquardt*

Main category: cs.LG

TL;DR: 本文研究了平衡传播（Equilibrium Propagation，EP）方法在局部连接晶格等更现实网络架构中的性能，结果表明稀疏网络也能达到与密集网络相当的性能。


<details>
  <summary>Details</summary>
Motivation: 人工智能的快速发展导致能源消耗不可持续，因此需要探索神经形态计算和基于物理的机器学习方法作为替代方案。现有研究多集中于简单架构，而本文旨在研究更现实的架构，例如局部连接晶格，在EP训练中的表现。

Method: 研究人员训练了一个XY模型，并探索了架构对各种基准任务的影响，跟踪了训练过程中空间分布响应和耦合的演变。他们分析了局部连接晶格结构的平衡传播训练效果。

Result: 研究结果表明，即使是具有仅有局部连接的稀疏网络，也能达到与密集网络相当的性能水平。

Conclusion: 本文的研究结果为基于平衡传播方法在现实环境中扩展架构提供了指导，为神经形态计算的发展提供了参考。

Abstract: The rapid rise of artificial intelligence has led to an unsustainable growth in energy consumption. This has motivated progress in neuromorphic computing and physics-based training of learning machines as alternatives to digital neural networks. Many theoretical studies focus on simple architectures like all-to-all or densely connected layered networks. However, these may be challenging to realize experimentally, e.g. due to connectivity constraints. In this work, we investigate the performance of the widespread physics-based training method of equilibrium propagation for more realistic architectural choices, specifically, locally connected lattices. We train an XY model and explore the influence of architecture on various benchmark tasks, tracking the evolution of spatially distributed responses and couplings during training. Our results show that sparse networks with only local connections can achieve performance comparable to dense networks. Our findings provide guidelines for further scaling up architectures based on equilibrium propagation in realistic settings.

</details>
