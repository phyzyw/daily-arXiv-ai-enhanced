{"id": "2510.16591", "title": "Symmetry and Generalisation in Neural Approximations of Renormalisation Transformations", "authors": ["Cassidy Ashworth", "Pietro Liò", "Francesco Caso"], "summary": "Deep learning models have proven enormously successful at using multiple layers of representation to learn relevant features of structured data. Encoding physical symmetries into these models can improve performance on difficult tasks, and recent work has motivated the principle of parameter symmetry breaking and restoration as a unifying mechanism underlying their hierarchical learning dynamics. We evaluate the role of parameter symmetry and network expressivity in the generalisation behaviour of neural networks when learning a real-space renormalisation group (RG) transformation, using the central limit theorem (CLT) as a test case map. We consider simple multilayer perceptrons (MLPs) and graph neural networks (GNNs), and vary weight symmetries and activation functions across architectures. Our results reveal a competition between symmetry constraints and expressivity, with overly complex or overconstrained models generalising poorly. We analytically demonstrate this poor generalisation behaviour for certain constrained MLP architectures by recasting the CLT as a cumulant recursion relation and making use of an established framework to propagate cumulants through MLPs. We also empirically validate an extension of this framework from MLPs to GNNs, elucidating the internal information processing performed by these more complex models. These findings offer new insight into the learning dynamics of symmetric networks and their limitations in modelling structured physical transformations.", "abs": "", "categories": ["cs.LG", "cond-mat.stat-mech", "cs.AI", "stat.ML"], "AI": {"tldr": "本文研究了神经网络学习物理变换（以中心极限定理为例）时，参数对称性和网络表达能力之间的关系，揭示了两者之间的竞争关系，并分析了其对泛化性能的影响。", "motivation": "理解神经网络学习物理变换的机制，特别是参数对称性和网络表达能力如何影响其泛化能力，以及如何通过编码物理对称性来改进网络性能。", "method": "使用多层感知机（MLP）和图神经网络（GNN）作为模型，通过改变权重对称性和激活函数来研究不同架构下的泛化行为。分析性地将中心极限定理（CLT）转化为累积量递推关系，并通过累积量传播框架来分析MLP的泛化行为，并将其扩展到GNN进行经验验证。", "result": "研究发现参数对称性和网络表达能力之间存在竞争关系，过度复杂或过度约束的模型泛化性能较差。分析结果表明，对于某些受约束的MLP架构，这种泛化不良的行为可以通过累积量传播框架来解释。对GNN的经验验证进一步支持了这一发现。", "conclusion": "本文为对称网络的学习动态和在建模物理变换时的局限性提供了新的见解，强调了在设计神经网络时平衡对称性和表达能力的重要性，并为理解神经网络的内部信息处理机制提供了新的视角。"}}
{"id": "2510.17569", "title": "Semi-supervised Latent Bayesian Optimization for Designing Antimicrobial Peptides", "authors": ["Jyler Menard", "R. A. Mansbach"], "summary": "Antimicrobial peptides (AMPs) are a promising class of therapeutics to treat bacterial infections. Discovering and designing such peptides is difficult because of the vast number of possible sequences of amino acids. Deep generative models, such as variational autoencoders, have shown value in peptide design due to their ability to model sequence space with a continuous-valued latent space. Although such models have already been used to great effect in biomolecular design, they still suffer from a lack of interpretability and rigorous quantification of latent space quality as a search space. We investigate (1) whether further compression of the design space via dimensionality reduction may facilitate optimization, (2) the interpretability of the spaces, and (3) how organizing latent spaces with physicochemical properties may improve the efficiency of optimizing antimicrobial activity. We find that further reduction of the latent space via dimensionality reduction can be advantageous when organizing the space with more relevant information at data availability, that using the dimensionality reduction search space can be more interpretable, and that we can organize the latent space with different physicochemical properties even at different percentages of available labels.", "abs": "", "categories": ["cs.LG", "physics.comp-ph"], "AI": {"tldr": "本文研究了通过降维和组织潜在空间来优化抗菌肽设计，旨在提高抗菌肽发现的效率和可解释性。", "motivation": "抗菌药物耐药性日益严重，传统药物研发效率降低，迫切需要新型抗菌疗法。抗菌肽（AMPs）作为一种有前景的候选药物，但其庞大的序列空间使得传统方法难以有效探索。", "method": "利用变分自编码器等深度生成模型构建抗菌肽的潜在空间，并通过降维、组织潜在空间（基于物理化学性质）以及半监督学习进行优化。", "result": "研究发现，降维可以优化潜在空间，提高优化效率；降维后的搜索空间更具可解释性；可以通过不同的物理化学性质组织潜在空间，即使在标签数据有限的情况下也能提高优化效率。", "conclusion": "该研究为抗菌肽设计提供了一种更有效、更具可解释性的方法，有望加速新型抗菌药物的发现，应对日益严重的抗菌药物耐药性问题。"}}
{"id": "2510.16816", "title": "Efficient High-Accuracy PDEs Solver with the Linear Attention Neural Operator", "authors": ["Ming Zhong", "Zhenya Yan"], "summary": "Neural operators offer a powerful data-driven framework for learning mappings between function spaces, in which the transformer-based neural operator architecture faces a fundamental scalability-accuracy trade-off: softmax attention provides excellent fidelity but incurs quadratic complexity $\\mathcal{O}(N^2 d)$ in the number of mesh points $N$ and hidden dimension $d$, while linear attention variants reduce cost to $\\mathcal{O}(N d^2)$ but often suffer significant accuracy degradation. To address the aforementioned challenge, in this paper, we present a novel type of neural operators, Linear Attention Neural Operator (LANO), which achieves both scalability and high accuracy by reformulating attention through an agent-based mechanism. LANO resolves this dilemma by introducing a compact set of $M$ agent tokens $(M \\ll N)$ that mediate global interactions among $N$ tokens. This agent attention mechanism yields an operator layer with linear complexity $\\mathcal{O}(MN d)$ while preserving the expressive power of softmax attention. Theoretically, we demonstrate the universal approximation property, thereby demonstrating improved conditioning and stability properties. Empirically, LANO surpasses current state-of-the-art neural PDE solvers, including Transolver with slice-based softmax attention, achieving average $19.5\\%$ accuracy improvement across standard benchmarks. By bridging the gap between linear complexity and softmax-level performance, LANO establishes a scalable, high-accuracy foundation for scientific machine learning applications.", "abs": "", "categories": ["cs.LG", "cs.AI", "math-ph", "math.MP", "physics.comp-ph"], "AI": {"tldr": "本文提出了一种名为线性注意力神经网络算子（LANO）的新型神经网络算子，它通过引入代理机制实现了线性复杂度，同时保持了softmax注意力的表达能力，从而在可扩展性和精度之间取得了平衡。", "motivation": "传统的基于Transformer的神经网络算子在可扩展性和精度之间存在权衡，线性注意力变体虽然降低了计算成本，但通常会牺牲精度。为了解决这一问题，研究者们致力于开发一种既能保证可扩展性又能保持高精度的神经网络算子。", "method": "LANO通过引入少量代理token（M<<N）来介导N个token之间的全局交互，构建了一种基于代理的注意力机制。这种机制将算子层的复杂度降低到O(MNd)，同时保留了softmax注意力的表达能力。此外，研究者们还证明了LANO具有普适逼近性质。", "result": "实验结果表明，LANO在标准基准测试中平均比当前最先进的神经网络PDE求解器（如Transolver）提高了19.5%的精度，在可扩展性和精度方面都优于现有方法。", "conclusion": "LANO通过弥合线性复杂度与softmax级别性能之间的差距，为科学机器学习应用提供了一个可扩展、高精度的基础，有望推动实时模拟和科学计算的发展。"}}
{"id": "2510.17187", "title": "A Standardized Benchmark for Machine-Learned Molecular Dynamics using Weighted Ensemble Sampling", "authors": ["Alexander Aghili", "Andy Bruce", "Daniel Sabo", "Sanya Murdeshwar", "Kevin Bachelor", "Ionut Mistreanu", "Ashwin Lokapally", "Razvan Marinescu"], "summary": "The rapid evolution of molecular dynamics (MD) methods, including machine-learned dynamics, has outpaced the development of standardized tools for method validation. Objective comparison between simulation approaches is often hindered by inconsistent evaluation metrics, insufficient sampling of rare conformational states, and the absence of reproducible benchmarks. To address these challenges, we introduce a modular benchmarking framework that systematically evaluates protein MD methods using enhanced sampling analysis. Our approach uses weighted ensemble (WE) sampling via The Weighted Ensemble Simulation Toolkit with Parallelization and Analysis (WESTPA), based on progress coordinates derived from Time-lagged Independent Component Analysis (TICA), enabling fast and efficient exploration of protein conformational space. The framework includes a flexible, lightweight propagator interface that supports arbitrary simulation engines, allowing both classical force fields and machine learning-based models. Additionally, the framework offers a comprehensive evaluation suite capable of computing more than 19 different metrics and visualizations across a variety of domains. We further contribute a dataset of nine diverse proteins, ranging from 10 to 224 residues, that span a variety of folding complexities and topologies. Each protein has been extensively simulated at 300K for one million MD steps per starting point (4 ns). To demonstrate the utility of our framework, we perform validation tests using classic MD simulations with implicit solvent and compare protein conformational sampling using a fully trained versus under-trained CGSchNet model. By standardizing evaluation protocols and enabling direct, reproducible comparisons across MD approaches, our open-source platform lays the groundwork for consistent, rigorous benchmarking across the molecular simulation community.", "abs": "", "categories": ["cs.LG", "q-bio.BM", "92B20"], "AI": {"tldr": "本文提出了一种模块化的基准测试框架，用于系统地评估蛋白质分子动力学方法，特别是机器学习驱动的分子动力学方法，并提供了一个包含九种蛋白质的数据集。", "motivation": "现有的分子动力学方法验证工具缺乏标准化，难以客观比较不同方法，且在采样罕见构象状态方面存在不足。", "method": "该框架使用基于时间滞后独立成分分析（TICA）推导的进度坐标，通过加权集合（WE）采样（利用WESTPA工具包）高效探索蛋白质构象空间。框架具有灵活的传播器接口，支持各种模拟引擎，并提供超过19种评估指标和可视化工具。", "result": "通过对经典MD模拟和CGSchNet模型的验证测试，展示了该框架的实用性，并提供了一个包含九种不同蛋白质的数据集，用于评估不同MD方法的性能。", "conclusion": "该开源平台通过标准化评估协议，为分子模拟领域提供了一个进行一致、严格基准测试的基础，促进了不同MD方法之间的直接可重复比较。"}}
{"id": "2510.16612", "title": "Accelerated Learning on Large Scale Screens using Generative Library Models", "authors": ["Eli N. Weinstein", "Andrei Slabodkin", "Mattia G. Gollub", "Elizabeth B. Wood"], "summary": "Biological machine learning is often bottlenecked by a lack of scaled data. One promising route to relieving data bottlenecks is through high throughput screens, which can experimentally test the activity of $10^6-10^{12}$ protein sequences in parallel. In this article, we introduce algorithms to optimize high throughput screens for data creation and model training. We focus on the large scale regime, where dataset sizes are limited by the cost of measurement and sequencing. We show that when active sequences are rare, we maximize information gain if we only collect positive examples of active sequences, i.e. $x$ with $y>0$. We can correct for the missing negative examples using a generative model of the library, producing a consistent and efficient estimate of the true $p(y | x)$. We demonstrate this approach in simulation and on a large scale screen of antibodies. Overall, co-design of experiments and inference lets us accelerate learning dramatically.", "abs": "", "categories": ["stat.ML", "cs.LG", "q-bio.BM"], "AI": {"tldr": "本文提出了一种优化大规模生物学筛选实验的方法，通过仅收集活性序列的正例，并利用生成模型校正缺失的负例，从而在数据稀疏的情况下加速机器学习。", "motivation": "生物学发现常常受限于大规模高质量训练数据的缺乏，而大规模筛选实验是缓解这一瓶颈的潜在途径。然而，数据收集成本限制了数据集规模，需要优化实验设计以提高信息获取效率。", "method": "本文采用贝叶斯实验设计理论，提出了一种测量策略，仅收集活性序列的正例（y>0），并使用生成模型估计真实的p(y|x)分布，从而校正缺失的负例。该方法结合实验设计和推断，以最大限度地提高信息增益。", "result": "实验表明，在活性序列稀少的情况下，仅收集正例可以获得比标准方法更多的信息，并在模拟和实际抗体筛选实验中加速模型训练。", "conclusion": "本文提出的实验与推断的协同设计方法，能够显著加速机器学习过程，特别适用于活性序列极少的难题，为生物学发现提供了新的途径。"}}
{"id": "2510.16590", "title": "Atom-anchored LLMs speak Chemistry: A Retrosynthesis Demonstration", "authors": ["Alan Kai Hassen", "Andrius Bernatavicius", "Antonius P. A. Janssen", "Mike Preuss", "Gerard J. P. van Westen", "Djork-Arné Clevert"], "summary": "Applications of machine learning in chemistry are often limited by the scarcity and expense of labeled data, restricting traditional supervised methods. In this work, we introduce a framework for molecular reasoning using general-purpose Large Language Models (LLMs) that operates without requiring labeled training data. Our method anchors chain-of-thought reasoning to the molecular structure by using unique atomic identifiers. First, the LLM performs a one-shot task to identify relevant fragments and their associated chemical labels or transformation classes. In an optional second step, this position-aware information is used in a few-shot task with provided class examples to predict the chemical transformation. We apply our framework to single-step retrosynthesis, a task where LLMs have previously underperformed. Across academic benchmarks and expert-validated drug discovery molecules, our work enables LLMs to achieve high success rates in identifying chemically plausible reaction sites ($\\geq90\\%$), named reaction classes ($\\geq40\\%$), and final reactants ($\\geq74\\%$). Beyond solving complex chemical tasks, our work also provides a method to generate theoretically grounded synthetic datasets by mapping chemical knowledge onto the molecular structure and thereby addressing data scarcity.", "abs": "", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "AI": {"tldr": "本文提出了一种名为ATOM-ANCHOREDLLMS SPEAKCHEMISTRY的框架，利用通用大型语言模型（LLMs）进行分子推理，无需大量标注数据，并成功应用于逆合成任务。", "motivation": "化学领域机器学习应用受限于标注数据的稀缺和昂贵，传统监督方法受到限制。本文旨在利用通用LLMs解决这一问题。", "method": "该框架通过使用独特的原子标识符将链式思维推理锚定到分子结构上。首先，LLM进行一次性任务以识别相关片段及其化学标签或转化类别。然后，利用这些位置感知信息进行少量样本任务，预测化学转化。", "result": "在学术基准测试和专家验证的药物发现分子上，该框架实现了高成功率，在识别化学上可行的反应位点、命名反应类别和最终反应物方面分别达到≥90%、≥40%和≥74%。", "conclusion": "该工作不仅解决了复杂的化学任务，还提供了一种生成理论上合理的合成数据集的方法，从而解决了数据稀缺问题，并为化学领域机器学习提供了新的思路。"}}
{"id": "2510.16253", "title": "Protein Folding with Neural Ordinary Differential Equations", "authors": ["Arielle Sanford", "Shuo Sun", "Christian B. Mendl"], "summary": "Recent advances in protein structure prediction, such as AlphaFold, have demonstrated the power of deep neural architectures like the Evoformer for capturing complex spatial and evolutionary constraints on protein conformation. However, the depth of the Evoformer, comprising 48 stacked blocks, introduces high computational costs and rigid layerwise discretization. Inspired by Neural Ordinary Differential Equations (Neural ODEs), we propose a continuous-depth formulation of the Evoformer, replacing its 48 discrete blocks with a Neural ODE parameterization that preserves its core attention-based operations. This continuous-time Evoformer achieves constant memory cost (in depth) via the adjoint method, while allowing a principled trade-off between runtime and accuracy through adaptive ODE solvers. Benchmarking on protein structure prediction tasks, we find that the Neural ODE-based Evoformer produces structurally plausible predictions and reliably captures certain secondary structure elements, such as alpha-helices, though it does not fully replicate the accuracy of the original architecture. However, our model achieves this performance using dramatically fewer resources, just 17.5 hours of training on a single GPU, highlighting the promise of continuous-depth models as a lightweight and interpretable alternative for biomolecular modeling. This work opens new directions for efficient and adaptive protein structure prediction frameworks.", "abs": "", "categories": ["cs.LG", "cs.AI", "q-bio.BM", "q-bio.QM", "stat.ML", "I.2.1; J.3"], "AI": {"tldr": "本文提出了一种基于Neural ODE的Evoformer模型，旨在降低蛋白质结构预测的计算成本，同时保持一定的预测精度。", "motivation": "AlphaFold等蛋白质结构预测模型虽然强大，但其深度网络（Evoformer）计算成本高昂。研究动机在于探索一种更轻量级、可解释性更强的生物分子建模方法。", "method": "将Evoformer的48个离散层替换为Neural ODE参数化，使用ODE求解器在连续时间维度上模拟蛋白质结构的细微调整，并利用伴随敏感性方法实现常数内存成本。", "result": "基于Neural ODE的Evoformer在蛋白质结构预测任务中，能够产生结构上合理的预测，并可靠地捕捉到α-螺旋等二级结构元素，虽然精度略低于原始架构，但仅需单GPU 17.5小时即可训练。", "conclusion": "该研究表明，连续深度模型具有作为生物分子建模中轻量级和可解释替代方案的潜力，并为高效和自适应的蛋白质结构预测框架开辟了新的方向。"}}
