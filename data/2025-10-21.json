{"id": "2510.16591", "title": "Symmetry and Generalisation in Neural Approximations of Renormalisation Transformations", "authors": ["Cassidy Ashworth", "Pietro Liò", "Francesco Caso"], "summary": "Deep learning models have proven enormously successful at using multiple layers of representation to learn relevant features of structured data. Encoding physical symmetries into these models can improve performance on difficult tasks, and recent work has motivated the principle of parameter symmetry breaking and restoration as a unifying mechanism underlying their hierarchical learning dynamics. We evaluate the role of parameter symmetry and network expressivity in the generalisation behaviour of neural networks when learning a real-space renormalisation group (RG) transformation, using the central limit theorem (CLT) as a test case map. We consider simple multilayer perceptrons (MLPs) and graph neural networks (GNNs), and vary weight symmetries and activation functions across architectures. Our results reveal a competition between symmetry constraints and expressivity, with overly complex or overconstrained models generalising poorly. We analytically demonstrate this poor generalisation behaviour for certain constrained MLP architectures by recasting the CLT as a cumulant recursion relation and making use of an established framework to propagate cumulants through MLPs. We also empirically validate an extension of this framework from MLPs to GNNs, elucidating the internal information processing performed by these more complex models. These findings offer new insight into the learning dynamics of symmetric networks and their limitations in modelling structured physical transformations.", "published": "2025-10-18", "categories": ["cs.LG", "cond-mat.stat-mech", "cs.AI", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2510.16591v1", "primary_category": "cs.LG"}
{"id": "2510.17569", "title": "Semi-supervised Latent Bayesian Optimization for Designing Antimicrobial Peptides", "authors": ["Jyler Menard", "R. A. Mansbach"], "summary": "Antimicrobial peptides (AMPs) are a promising class of therapeutics to treat bacterial infections. Discovering and designing such peptides is difficult because of the vast number of possible sequences of amino acids. Deep generative models, such as variational autoencoders, have shown value in peptide design due to their ability to model sequence space with a continuous-valued latent space. Although such models have already been used to great effect in biomolecular design, they still suffer from a lack of interpretability and rigorous quantification of latent space quality as a search space. We investigate (1) whether further compression of the design space via dimensionality reduction may facilitate optimization, (2) the interpretability of the spaces, and (3) how organizing latent spaces with physicochemical properties may improve the efficiency of optimizing antimicrobial activity. We find that further reduction of the latent space via dimensionality reduction can be advantageous when organizing the space with more relevant information at data availability, that using the dimensionality reduction search space can be more interpretable, and that we can organize the latent space with different physicochemical properties even at different percentages of available labels.", "published": "2025-10-20", "categories": ["cs.LG", "physics.comp-ph"], "pdf_url": "http://arxiv.org/pdf/2510.17569v1", "primary_category": "cs.LG"}
{"id": "2510.16816", "title": "Efficient High-Accuracy PDEs Solver with the Linear Attention Neural Operator", "authors": ["Ming Zhong", "Zhenya Yan"], "summary": "Neural operators offer a powerful data-driven framework for learning mappings between function spaces, in which the transformer-based neural operator architecture faces a fundamental scalability-accuracy trade-off: softmax attention provides excellent fidelity but incurs quadratic complexity $\\mathcal{O}(N^2 d)$ in the number of mesh points $N$ and hidden dimension $d$, while linear attention variants reduce cost to $\\mathcal{O}(N d^2)$ but often suffer significant accuracy degradation. To address the aforementioned challenge, in this paper, we present a novel type of neural operators, Linear Attention Neural Operator (LANO), which achieves both scalability and high accuracy by reformulating attention through an agent-based mechanism. LANO resolves this dilemma by introducing a compact set of $M$ agent tokens $(M \\ll N)$ that mediate global interactions among $N$ tokens. This agent attention mechanism yields an operator layer with linear complexity $\\mathcal{O}(MN d)$ while preserving the expressive power of softmax attention. Theoretically, we demonstrate the universal approximation property, thereby demonstrating improved conditioning and stability properties. Empirically, LANO surpasses current state-of-the-art neural PDE solvers, including Transolver with slice-based softmax attention, achieving average $19.5\\%$ accuracy improvement across standard benchmarks. By bridging the gap between linear complexity and softmax-level performance, LANO establishes a scalable, high-accuracy foundation for scientific machine learning applications.", "published": "2025-10-19", "categories": ["cs.LG", "cs.AI", "math-ph", "math.MP", "physics.comp-ph"], "pdf_url": "http://arxiv.org/pdf/2510.16816v1", "primary_category": "cs.LG"}
{"id": "2510.17187", "title": "A Standardized Benchmark for Machine-Learned Molecular Dynamics using Weighted Ensemble Sampling", "authors": ["Alexander Aghili", "Andy Bruce", "Daniel Sabo", "Sanya Murdeshwar", "Kevin Bachelor", "Ionut Mistreanu", "Ashwin Lokapally", "Razvan Marinescu"], "summary": "The rapid evolution of molecular dynamics (MD) methods, including machine-learned dynamics, has outpaced the development of standardized tools for method validation. Objective comparison between simulation approaches is often hindered by inconsistent evaluation metrics, insufficient sampling of rare conformational states, and the absence of reproducible benchmarks. To address these challenges, we introduce a modular benchmarking framework that systematically evaluates protein MD methods using enhanced sampling analysis. Our approach uses weighted ensemble (WE) sampling via The Weighted Ensemble Simulation Toolkit with Parallelization and Analysis (WESTPA), based on progress coordinates derived from Time-lagged Independent Component Analysis (TICA), enabling fast and efficient exploration of protein conformational space. The framework includes a flexible, lightweight propagator interface that supports arbitrary simulation engines, allowing both classical force fields and machine learning-based models. Additionally, the framework offers a comprehensive evaluation suite capable of computing more than 19 different metrics and visualizations across a variety of domains. We further contribute a dataset of nine diverse proteins, ranging from 10 to 224 residues, that span a variety of folding complexities and topologies. Each protein has been extensively simulated at 300K for one million MD steps per starting point (4 ns). To demonstrate the utility of our framework, we perform validation tests using classic MD simulations with implicit solvent and compare protein conformational sampling using a fully trained versus under-trained CGSchNet model. By standardizing evaluation protocols and enabling direct, reproducible comparisons across MD approaches, our open-source platform lays the groundwork for consistent, rigorous benchmarking across the molecular simulation community.", "published": "2025-10-20", "categories": ["cs.LG", "q-bio.BM", "92B20"], "pdf_url": "http://arxiv.org/pdf/2510.17187v1", "primary_category": "cs.LG"}
{"id": "2510.16612", "title": "Accelerated Learning on Large Scale Screens using Generative Library Models", "authors": ["Eli N. Weinstein", "Andrei Slabodkin", "Mattia G. Gollub", "Elizabeth B. Wood"], "summary": "Biological machine learning is often bottlenecked by a lack of scaled data. One promising route to relieving data bottlenecks is through high throughput screens, which can experimentally test the activity of $10^6-10^{12}$ protein sequences in parallel. In this article, we introduce algorithms to optimize high throughput screens for data creation and model training. We focus on the large scale regime, where dataset sizes are limited by the cost of measurement and sequencing. We show that when active sequences are rare, we maximize information gain if we only collect positive examples of active sequences, i.e. $x$ with $y>0$. We can correct for the missing negative examples using a generative model of the library, producing a consistent and efficient estimate of the true $p(y | x)$. We demonstrate this approach in simulation and on a large scale screen of antibodies. Overall, co-design of experiments and inference lets us accelerate learning dramatically.", "published": "2025-10-18", "categories": ["stat.ML", "cs.LG", "q-bio.BM"], "pdf_url": "http://arxiv.org/pdf/2510.16612v1", "primary_category": "stat.ML"}
{"id": "2510.16590", "title": "Atom-anchored LLMs speak Chemistry: A Retrosynthesis Demonstration", "authors": ["Alan Kai Hassen", "Andrius Bernatavicius", "Antonius P. A. Janssen", "Mike Preuss", "Gerard J. P. van Westen", "Djork-Arné Clevert"], "summary": "Applications of machine learning in chemistry are often limited by the scarcity and expense of labeled data, restricting traditional supervised methods. In this work, we introduce a framework for molecular reasoning using general-purpose Large Language Models (LLMs) that operates without requiring labeled training data. Our method anchors chain-of-thought reasoning to the molecular structure by using unique atomic identifiers. First, the LLM performs a one-shot task to identify relevant fragments and their associated chemical labels or transformation classes. In an optional second step, this position-aware information is used in a few-shot task with provided class examples to predict the chemical transformation. We apply our framework to single-step retrosynthesis, a task where LLMs have previously underperformed. Across academic benchmarks and expert-validated drug discovery molecules, our work enables LLMs to achieve high success rates in identifying chemically plausible reaction sites ($\\geq90\\%$), named reaction classes ($\\geq40\\%$), and final reactants ($\\geq74\\%$). Beyond solving complex chemical tasks, our work also provides a method to generate theoretically grounded synthetic datasets by mapping chemical knowledge onto the molecular structure and thereby addressing data scarcity.", "published": "2025-10-18", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "pdf_url": "http://arxiv.org/pdf/2510.16590v1", "primary_category": "cs.LG"}
{"id": "2510.16253", "title": "Protein Folding with Neural Ordinary Differential Equations", "authors": ["Arielle Sanford", "Shuo Sun", "Christian B. Mendl"], "summary": "Recent advances in protein structure prediction, such as AlphaFold, have demonstrated the power of deep neural architectures like the Evoformer for capturing complex spatial and evolutionary constraints on protein conformation. However, the depth of the Evoformer, comprising 48 stacked blocks, introduces high computational costs and rigid layerwise discretization. Inspired by Neural Ordinary Differential Equations (Neural ODEs), we propose a continuous-depth formulation of the Evoformer, replacing its 48 discrete blocks with a Neural ODE parameterization that preserves its core attention-based operations. This continuous-time Evoformer achieves constant memory cost (in depth) via the adjoint method, while allowing a principled trade-off between runtime and accuracy through adaptive ODE solvers. Benchmarking on protein structure prediction tasks, we find that the Neural ODE-based Evoformer produces structurally plausible predictions and reliably captures certain secondary structure elements, such as alpha-helices, though it does not fully replicate the accuracy of the original architecture. However, our model achieves this performance using dramatically fewer resources, just 17.5 hours of training on a single GPU, highlighting the promise of continuous-depth models as a lightweight and interpretable alternative for biomolecular modeling. This work opens new directions for efficient and adaptive protein structure prediction frameworks.", "published": "2025-10-17", "categories": ["cs.LG", "cs.AI", "q-bio.BM", "q-bio.QM", "stat.ML", "I.2.1; J.3"], "pdf_url": "http://arxiv.org/pdf/2510.16253v1", "primary_category": "cs.LG"}
