<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Teaching Language Models Mechanistic Explainability Through Arrow-Pushing](https://arxiv.org/abs/2512.05722)
*Théo A. Neukomm, Zlatko Jončev, Philippe Schwaller*

Main category: cs.LG

TL;DR: 本文提出了一种利用语言模型预测化学反应机制的方法，通过“箭头推动”形式化来追踪电子流动，并开发了MechSMILES文本格式来表示反应机制。


<details>
  <summary>Details</summary>
Motivation: 现有的计算机辅助合成规划（CASP）系统缺乏可解释性，且常提出化学上不可行的反应。为了解决这些问题，需要一种能够理解反应机制并确保化学合理性的方法。

Method: 研究人员开发了MechSMILES文本格式，用于编码分子结构和电子流动，并利用该格式训练语言模型，在四个不同复杂度的机制预测任务上进行训练，包括mech-USPTO-31k和FlowER数据集。

Result: 模型在 elementary step 预测任务中达到 95% 的 top-3 准确率，在 mech-USPTO-31k 和 FlowER 数据集上分别达到 73% 和 93% 的完整反应机制检索准确率。

Conclusion: 该研究提供了一种通往更具可解释性和化学有效性的计算合成规划的途径，并提供了一个架构无关的机制预测基准框架，能够验证CASP系统，进行原子到原子的映射，并提取催化剂相关的反应模板。

Abstract: Chemical reaction mechanisms provide crucial insight into synthesizability, yet current Computer-Assisted Synthesis Planning (CASP) systems lack mechanistic grounding. We introduce a computational framework for teaching language models to predict chemical reaction mechanisms through arrow pushing formalism, a century-old notation that tracks electron flow while respecting conservation laws. We developed MechSMILES, a compact textual format encoding molecular structure and electron flow, and trained language models on four mechanism prediction tasks of increasing complexity using mechanistic reaction datasets, such as mech-USPTO-31k and FlowER. Our models achieve more than 95\% top-3 accuracy on elementary step prediction and scores that surpass 73\% on mech-USPTO-31k, and 93\% on FlowER dataset for the retrieval of complete reaction mechanisms on our hardest task. This mechanistic understanding enables three key applications. First, our models serve as post-hoc validators for CASP systems, filtering chemically implausible transformations. Second, they enable holistic atom-to-atom mapping that tracks all atoms, including hydrogens. Third, they extract catalyst-aware reaction templates that distinguish recycled catalysts from spectator species. By grounding predictions in physically meaningful electron moves that ensure conservation of mass and charge, this work provides a pathway toward more explainable and chemically valid computational synthesis planning, while providing an architecture-agnostic framework for the benchmarking of mechanism prediction.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [2] [FieldSeer I: Physics-Guided World Models for Long-Horizon Electromagnetic Dynamics under Partial Observability](https://arxiv.org/abs/2512.05361)
*Ziheng Guo, Fang Wu, Maoxiong Zhao, Chaoqun Fang, Yang Bu*

Main category: physics.optics

TL;DR: FieldSeer I是一种基于物理引导的世界模型，能够根据部分观测预测2D TE波导中的电磁场动态，并支持在预测过程中修改几何结构。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法在预测电磁场动态、处理部分可观测性和快速适应结构变化方面存在局限性，缺乏在交互式设计中预览未来行为的能力。

Method: FieldSeer I利用几何感知、动作条件的世界模型，结合对称对数域训练以保证数值稳定性，并支持在预测过程中进行几何结构编辑，无需重新同化。

Result: 在可重复的FDTD基准测试中，FieldSeer I在软件-环内过滤、离线单文件展开和离线多结构展开三种场景下，都优于GRU和确定性基线，实现了更高的后缀保真度。

Conclusion: FieldSeer I为光子设计中的交互式数字孪生提供了一条可行路径，展示了基于几何条件的世模型在动态预测和交互式设计中的潜力。

Abstract: We introduce FieldSeer I, a geometry-aware world model that forecasts electromagnetic field dynamics from partial observations in 2-D TE waveguides. The model assimilates a short prefix of observed fields, conditions on a scalar source action and structure/material map, and generates closed-loop rollouts in the physical domain. Training in a symmetric-log domain ensures numerical stability. Evaluated on a reproducible FDTD benchmark (200 unique simulations, structure-wise split), FieldSeer I achieves higher suffix fidelity than GRU and deterministic baselines across three practical settings: (i) software-in-the-loop filtering (64x64, P=80->Q=80), (ii) offline single-file rollouts (80x140, P=240->Q=40), and (iii) offline multi-structure rollouts (80x140, P=180->Q=100). Crucially, it enables edit-after-prefix geometry modifications without re-assimilation. Results demonstrate that geometry-conditioned world models provide a practical path toward interactive digital twins for photonic design.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [3] [STAR-GO: Improving Protein Function Prediction by Learning to Hierarchically Integrate Ontology-Informed Semantic Embeddings](https://arxiv.org/abs/2512.05245)
*Mehmet Efe Akça, Gökçe Uludoğan, Arzucan Özgür, İnci M. Baytaş*

Main category: q-bio.BM

TL;DR: STAR-GO是一种基于Transformer的框架，通过联合建模GO术语的语义和结构特征，以提升零样本蛋白质功能预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 实验蛋白质功能注释滞后于蛋白质序列数据的快速增长，现有模型通常侧重于单一模态，难以泛化到新出现的GO术语，导致模型过时。

Method: STAR-GO将文本定义与本体图结构结合，学习统一的GO表示，并以层级顺序处理这些表示以传播信息，同时将这些表示与蛋白质序列嵌入对齐，以捕获序列-功能关系。

Result: STAR-GO在蛋白质功能预测任务上实现了最先进的性能，并表现出优越的零样本泛化能力。

Conclusion: STAR-GO的成功表明，整合语义和结构信息对于构建鲁棒且适应性强的蛋白质功能预测模型至关重要。

Abstract: Accurate prediction of protein function is essential for elucidating molecular mechanisms and advancing biological and therapeutic discovery. Yet experimental annotation lags far behind the rapid growth of protein sequence data. Computational approaches address this gap by associating proteins with Gene Ontology (GO) terms, which encode functional knowledge through hierarchical relations and textual definitions. However, existing models often emphasize one modality over the other, limiting their ability to generalize, particularly to unseen or newly introduced GO terms that frequently arise as the ontology evolves, and making the previously trained models outdated. We present STAR-GO, a Transformer-based framework that jointly models the semantic and structural characteristics of GO terms to enhance zero-shot protein function prediction. STAR-GO integrates textual definitions with ontology graph structure to learn unified GO representations, which are processed in hierarchical order to propagate information from general to specific terms. These representations are then aligned with protein sequence embeddings to capture sequence-function relationships. STAR-GO achieves state-of-the-art performance and superior zero-shot generalization, demonstrating the utility of integrating semantics and structure for robust and adaptable protein function prediction. Code is available at https://github.com/boun-tabi-lifelu/stargo.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [4] [Comparing the latent features of universal machine-learning interatomic potentials](https://arxiv.org/abs/2512.05717)
*Sofiia Chorna, Davide Tisi, Cesare Malosso, Wei Bin How, Michele Ceriotti, Sanggyu Chong*

Main category: physics.chem-ph

TL;DR: 本文系统分析了不同通用机器学习原子间势 (uMLIP) 如何学习和编码化学空间，发现它们在潜在特征的表示方式上存在显著差异，且模型之间特征重建误差较大。


<details>
  <summary>Details</summary>
Motivation: 尽管 uMLIP 在材料建模中取得了显著进展，但不同 uMLIP 之间化学空间表示方式的差异性尚不明确。本文旨在深入研究 uMLIP 如何感知和组织化学空间，从而更好地理解和利用这些模型。

Method: 研究人员选取了 MACE-MP-03b、PET-MAD、DPA-3.1 和 UMA-S-1P1 四种 uMLIP，并使用特征重建误差作为指标，定量评估了它们潜在特征的信息内容和相互可重建性。此外，还研究了不同数据集和训练策略对模型的影响，以及在微调过程中潜在空间的演变。

Result: 研究发现，不同的 uMLIP 以显著不同的方式编码化学空间，模型之间的特征重建误差较大。同一模型架构的变体，其特征表现受数据集、目标和训练策略的影响。微调后的 uMLIP 仍然保留了较强的预训练偏差。

Conclusion: 本文的研究结果揭示了 uMLIP 在化学空间编码方面的差异性，为理解和改进 uMLIP 提供了新的视角。通过将原子级特征压缩为全局结构级特征，可以更有效地利用 uMLIP 的信息内容，为材料建模和化学研究提供更强大的工具。

Abstract: The past few years have seen the development of ``universal'' machine-learning interatomic potentials (uMLIPs) capable of approximating the ground-state potential energy surface across a wide range of chemical structures and compositions with reasonable accuracy. While these models differ in the architecture and the dataset used, they share the ability to compress a staggering amount of chemical information into descriptive latent features. Herein, we systematically analyze what the different uMLIPs have learned by quantitatively assessing the relative information content of their latent features with feature reconstruction errors as metrics, and observing how the trends are affected by the choice of training set and training protocol. We find that the uMLIPs encode chemical space in significantly distinct ways, with substantial cross-model feature reconstruction errors. When variants of the same model architecture are considered, trends become dependent on the dataset, target, and training protocol of choice. We also observe that fine-tuning of a uMLIP retains a strong pre-training bias in the latent features. Finally, we discuss how atom-level features, which are directly output by MLIPs, can be compressed into global structure-level features via concatenation of progressive cumulants, each adding significantly new information about the variability across the atomic environments within a given system.

</details>
