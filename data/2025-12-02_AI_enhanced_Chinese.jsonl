{"id": "2512.00168", "title": "Tuning Universality in Deep Neural Networks", "authors": ["Arsham Ghavasieh"], "summary": "Deep neural networks (DNNs) exhibit crackling-like avalanches whose origin lacks a mechanistic explanation. Here, I derive a stochastic theory of deep information propagation (DIP) by incorporating Central Limit Theorem (CLT)-level fluctuations. Four effective couplings $(r, h, D_1, D_2)$ characterize the dynamics, yielding a Landau description of the static exponents and a Directed Percolation (DP) structure of activity cascades. Tuning the couplings selects between avalanche dynamics generated by a Brownian Motion (BM) in a logarithmic trap and an absorbed free BM, each corresponding to a distinct universality classes. Numerical simulations confirm the theory and demonstrate that activation function design controls the collective dynamics in random DNNs.", "abs": "", "categories": ["cond-mat.dis-nn", "cond-mat.stat-mech", "cs.AI", "nlin.AO", "physics.bio-ph"], "AI": {"tldr": "本文提出了一种随机深度神经网络（DNN）的信息传播随机理论，揭示了激活函数设计如何控制网络中的集体动力学，并将其导向不同的普适性类别。", "motivation": "尽管深度神经网络（DNN）中观察到类似临界现象的雪崩行为，但其机制尚不明确。现有研究主要集中在单一激活函数，缺乏对架构和激活函数对临界行为影响的理解。因此，需要一个分析框架来识别关键自由度，隔离雪崩生成机制，并确定可能的普适性类别。", "method": "作者通过将中心极限定理（CLT）级别的涨落纳入深度信息传播（DIP）模型，将动力学简化为四个有效耦合（r, h, D1, D2）。这些耦合产生一个 Landau 描述，并揭示了类似于定向渗透（DP）的随机结构，从而获得静态和动态临界指数。作者将神经网络增益动力学映射到不同的雪崩生成过程，包括布朗运动和吸收自由布朗运动。", "result": "研究结果表明，通过调整激活函数，可以控制随机 DNN 之间的雪崩动力学，使其在不同的普适性类别之间切换。数值模拟验证了该理论，并证实了雪崩统计与预测的普适性类别一致，确认了 DP 和 RW 标度之间的交叉。", "conclusion": "本文建立了一个统一的随机理论，阐明了 DNN 中临界动力学的机制，并揭示了激活函数设计对集体行为的影响。该理论为理解生物和人工神经网络如何利用相似的宏观原理优化计算提供了新的视角。"}}
{"id": "2512.01888", "title": "Domain-Decomposed Graph Neural Network Surrogate Modeling for Ice Sheets", "authors": ["Adrienne M. Propp", "Mauro Perego", "Eric C. Cyr", "Anthony Gruber", "Amanda A. Howard", "Alexander Heinlein", "Panos Stinis", "Daniel M. Tartakovsky"], "summary": "Accurate yet efficient surrogate models are essential for large-scale simulations of partial differential equations (PDEs), particularly for uncertainty quantification (UQ) tasks that demand hundreds or thousands of evaluations. We develop a physics-inspired graph neural network (GNN) surrogate that operates directly on unstructured meshes and leverages the flexibility of graph attention. To improve both training efficiency and generalization properties of the model, we introduce a domain decomposition (DD) strategy that partitions the mesh into subdomains, trains local GNN surrogates in parallel, and aggregates their predictions. We then employ transfer learning to fine-tune models across subdomains, accelerating training and improving accuracy in data-limited settings. Applied to ice sheet simulations, our approach accurately predicts full-field velocities on high-resolution meshes, substantially reduces training time relative to training a single global surrogate model, and provides a ripe foundation for UQ objectives. Our results demonstrate that graph-based DD, combined with transfer learning, provides a scalable and reliable pathway for training GNN surrogates on massive PDE-governed systems, with broad potential for application beyond ice sheet dynamics.", "abs": "", "categories": ["cs.LG", "math-ph", "math.NA", "physics.comp-ph"], "AI": {"tldr": "本文提出了一种基于图神经网络（GNN）的领域分解（DD）代理模型，用于加速大规模偏微分方程（PDE）的模拟，特别是在不确定性量化（UQ）任务中，并在冰盖模拟中取得了良好的效果。", "motivation": "传统的PDE求解器在进行需要大量模型评估的不确定性量化（UQ）任务时计算成本过高。因此，需要高效且尊重底层物理的建模技术，以加速模拟并推广到不同的网格分辨率、输入参数和域几何形状。", "method": "该方法采用物理感知的GNN代理模型，将网格分解为子域，并行训练局部GNN代理，并利用迁移学习在子域之间微调模型。这种基于图的DD结合迁移学习，为训练GNN代理提供了一种可扩展且可靠的途径。", "result": "该方法准确预测了高分辨率网格上的全场速度，显著减少了相对于单个全局代理模型的训练时间，并为UQ目标提供了良好的基础。实验结果表明，该方法在冰盖模拟中表现出色。", "conclusion": "该研究表明，基于图的DD结合迁移学习是一种可扩展且可靠的训练GNN代理的途径，具有广泛的应用潜力，超越冰盖动力学。"}}
{"id": "2512.01517", "title": "Neural Networks for Predicting Permeability Tensors of 2D Porous Media: Comparison of Convolution- and Transformer-based Architectures", "authors": ["Sigurd Vargdal", "Paula Reis", "Henrik Andersen Sveinsson", "Gaute Linga"], "summary": "Permeability is a central concept in the macroscopic description of flow through porous media, with applications spanning from oil recovery to hydrology. Traditional methods for determining the permeability tensor involving flow simulations or experiments can be time consuming and resource-intensive, while analytical methods, e.g., based on the Kozeny-Carman equation, may be too simplistic for accurate prediction based on pore-scale features. In this work, we explore deep learning as a more efficient alternative for predicting the permeability tensor based on two-dimensional binary images of porous media, segmented into solid ($1$) and void ($0$) regions. We generate a dataset of 24,000 synthetic random periodic porous media samples with specified porosity and characteristic length scale. Using Lattice-Boltzmann simulations, we compute the permeability tensor for flow through these samples with values spanning three orders of magnitude. We evaluate three families of image-based deep learning models: ResNet (ResNet-$50$ and ResNet-$101$), Vision Transformers (ViT-T$16$ and ViT-S$16$) and ConvNeXt (Tiny and Small). To improve model generalisation, we employ techniques such as weight decay, learning rate scheduling, and data augmentation. The effect of data augmentation and dataset size on model performance is studied, and we find that they generally increase the accuracy of permeability predictions. We also show that ConvNeXt and ResNet converge faster than ViT and degrade in performance if trained for too long. ConvNeXt-Small achieved the highest $R^2$ score of $0.99460$ on $4,000$ unseen test samples. These findings underscore the potential to use image-based neural networks to predict permeability tensors accurately.", "abs": "", "categories": ["physics.flu-dyn", "cs.LG", "physics.comp-ph", "physics.geo-ph"], "AI": {"tldr": "本文研究了利用深度学习模型，基于二维多孔介质的图像预测渗透率张量，并比较了卷积神经网络（ResNet, ConvNeXt）和Transformer网络（ViT）的性能。", "motivation": "传统的多孔介质渗透率确定方法（实验、模拟、理论计算）耗时且精度有限，因此探索深度学习作为一种更高效且准确的替代方案。", "method": "生成了24,000个合成随机多孔介质样本，使用Lattice-Boltzmann模拟计算渗透率张量。评估了ResNet、Vision Transformers (ViT) 和 ConvNeXt 三种类型的深度学习模型，并研究了数据增强和数据集大小对模型性能的影响。", "result": "ConvNeXt-Small模型在4,000个未见测试样本上取得了最高的R2分数（0.99460），并且ConvNeXt和ResNet比ViT收敛更快。", "conclusion": "研究结果表明，基于图像的神经网络可以准确地预测渗透率张量，为多孔介质流特性预测提供了一种有潜力的替代方法。"}}
{"id": "2512.01010", "title": "Chain of Unit-Physics: A Primitive-Centric Approach to Scientific Code Synthesis", "authors": ["Vansh Sharma", "Venkat Raman"], "summary": "Agentic large language models are proposed as autonomous code generators for scientific computing, yet their reliability in high-stakes problems remains unclear. Developing computational scientific software from natural-language queries remains challenging broadly due to (a) sparse representation of domain codes during training and (b) the limited feasibility of RLHF with a small expert community. To address these limitations, this work conceptualizes an inverse approach to code design, embodied in the Chain of Unit-Physics framework: a first-principles (or primitives)-centric, multi-agent system in which human expert knowledge is encoded as unit-physics tests that explicitly constrain code generation. The framework is evaluated on a nontrivial combustion task, used here as a representative benchmark for scientific problem with realistic physical constraints. Closed-weight systems and code-focused agentic variants fail to produce correct end-to-end solvers, despite tool and web access, exhibiting four recurrent error classes: interface (syntax/API) hallucinations, overconfident assumptions, numerical/physical incoherence, and configuration fragility. Open-weight models with chain-of-thought (CoT) decoding reduce interface errors but still yield incorrect solutions. On the benchmark task, the proposed framework converges within 5-6 iterations, matches the human-expert implementation (mean error of $3.1\\times10^{-3}$ %), with a $\\sim$33.4 % faster runtime and a $\\sim$30 % efficient memory usage at a cost comparable to mid-sized commercial APIs, yielding a practical template for physics-grounded scientific code generation. As datasets and models evolve, zero-shot code accuracy will improve; however, the Chain of Unit-Physics framework goes further by embedding first-principles analysis that is foundational to scientific codes.", "abs": "", "categories": ["cs.MA", "cs.AI", "cs.LG", "cs.SE", "physics.comp-ph", "physics.flu-dyn"], "AI": {"tldr": "本文提出了Chain of Unit-Physics框架，通过将人类专家知识编码为单元物理测试来指导科学代码生成，从而提高代码的可靠性和可解释性。该框架在燃烧任务上表现出与专家实现相当的精度，并具有更快的运行时间和更高效的内存使用。", "motivation": "现有的基于大型语言模型（LLM）的科学代码生成方法存在可靠性问题，例如幻觉、过度自信、数值/物理不一致和配置脆弱性。由于训练数据稀疏和RLHF的局限性，直接从自然语言查询生成科学代码仍然具有挑战性。", "method": "Chain of Unit-Physics框架是一种以基本原理（或基本元素）为中心的、多智能体系统。它将人类专家的知识编码为单元物理测试，这些测试明确地约束代码生成过程。该框架在12自由度的燃烧任务上进行评估，并与闭环系统和代码聚焦智能体变体进行对比。", "result": "与闭环系统和CoT解码方法相比，Chain of Unit-Physics框架在燃烧任务上收敛速度更快（5-6次迭代），精度更高（平均误差3.1×10⁻³%），运行速度更快（33.4%），内存使用效率更高（30%）。", "conclusion": "Chain of Unit-Physics框架提供了一种实用的、基于物理的科学代码生成模板，通过嵌入基本原理分析，指导更可靠和可解释的人工智能协作。即使在数据集和模型不断发展的情况下，该框架也能确保代码的准确性和可解释性。"}}
{"id": "2512.00384", "title": "Efficient and Programmable Exploration of Synthesizable Chemical Space", "authors": ["Shitong Luo", "Connor W. Coley"], "summary": "The constrained nature of synthesizable chemical space poses a significant challenge for sampling molecules that are both synthetically accessible and possess desired properties. In this work, we present PrexSyn, an efficient and programmable model for molecular discovery within synthesizable chemical space. PrexSyn is based on a decoder-only transformer trained on a billion-scale datastream of synthesizable pathways paired with molecular properties, enabled by a real-time, high-throughput C++-based data generation engine. The large-scale training data allows PrexSyn to reconstruct the synthesizable chemical space nearly perfectly at a high inference speed and learn the association between properties and synthesizable molecules. Based on its learned property-pathway mappings, PrexSyn can generate synthesizable molecules that satisfy not only single-property conditions but also composite property queries joined by logical operators, thereby allowing users to ``program'' generation objectives. Moreover, by exploiting this property-based querying capability, PrexSyn can efficiently optimize molecules against black-box oracle functions via iterative query refinement, achieving higher sampling efficiency than even synthesis-agnostic baselines, making PrexSyn a powerful general-purpose molecular optimization tool. Overall, PrexSyn pushes the frontier of synthesizable molecular design by setting a new state of the art in synthesizable chemical space coverage, molecular sampling efficiency, and inference speed.", "abs": "", "categories": ["cs.LG", "q-bio.BM"], "AI": {"tldr": "PrexSyn是一种高效且可编程的模型，用于在可合成化学空间中发现分子。它通过大规模训练数据和基于属性的查询能力，实现了对可合成化学空间的完美重建和高效的分子优化。", "motivation": "现有分子生成模型通常忽略了合成可行性，导致生成的分子难以实验验证。为了解决这个问题，该研究旨在开发一个保证合成可行性的分子生成模型，并能够根据属性进行编程。", "method": "PrexSyn基于一个仅解码器Transformer模型，在由C++引擎实时生成的大规模可合成路径和分子属性数据集上进行训练。该模型利用属性-路径映射关系，支持复合属性查询和黑盒优化。", "result": "PrexSyn实现了对可合成化学空间的近乎完美重建，具有高推理速度，并且在分子采样效率和优化方面优于现有方法，能够高效地优化分子以满足复杂的属性条件。", "conclusion": "PrexSyn在可合成分子设计领域取得了突破，在可合成化学空间覆盖率、分子采样效率和推理速度方面达到了新的状态，为大规模分子优化提供了强大的工具。"}}
{"id": "2512.00708", "title": "Towards Precision Protein-Ligand Affinity Prediction Benchmark: A Complete and Modification-Aware DAVIS Dataset", "authors": ["Ming-Hsiu Wu", "Ziqian Xie", "Shuiwang Ji", "Degui Zhi"], "summary": "Advancements in AI for science unlocks capabilities for critical drug discovery tasks such as protein-ligand binding affinity prediction. However, current models overfit to existing oversimplified datasets that does not represent naturally occurring and biologically relevant proteins with modifications. In this work, we curate a complete and modification-aware version of the widely used DAVIS dataset by incorporating 4,032 kinase-ligand pairs involving substitutions, insertions, deletions, and phosphorylation events. This enriched dataset enables benchmarking of predictive models under biologically realistic conditions. Based on this new dataset, we propose three benchmark settings-Augmented Dataset Prediction, Wild-Type to Modification Generalization, and Few-Shot Modification Generalization-designed to assess model robustness in the presence of protein modifications. Through extensive evaluation of both docking-free and docking-based methods, we find that docking-based model generalize better in zero-shot settings. In contrast, docking-free models tend to overfit to wild-type proteins and struggle with unseen modifications but show notable improvement when fine-tuned on a small set of modified examples. We anticipate that the curated dataset and benchmarks offer a valuable foundation for developing models that better generalize to protein modifications, ultimately advancing precision medicine in drug discovery. The benchmark is available at: https://github.com/ZhiGroup/DAVIS-complete", "abs": "", "categories": ["cs.LG", "q-bio.BM"], "AI": {"tldr": "本文构建了一个包含蛋白质修饰信息的完整DAVIS数据集（DAVIS-complete），并提出了三个基准测试框架，用于评估蛋白质-配体亲和力预测模型在生物学上更现实的场景下的泛化能力。", "motivation": "现有AI模型在蛋白质-配体亲和力预测中存在问题，它们过度拟合于简化数据集，忽略了蛋白质修饰（如取代、插入、缺失和磷酸化）对蛋白质结构和配体相互作用的显著影响，导致模型难以泛化到实际生物环境中。", "method": "研究人员对DAVIS数据集进行了补充，纳入了4032个包含蛋白质修饰的激酶-配体对，构建了DAVIS-complete数据集。在此基础上，设计了三个基准测试：增强数据集预测、野生型到修饰的泛化和少量修饰泛化，用于评估模型的鲁棒性。", "result": "实验结果表明，基于对接的方法在零样本设置下泛化能力更强，而无对接的方法容易过度拟合野生型蛋白质，难以处理未见的修饰，但经过少量修饰样本的微调后性能显著提升。", "conclusion": "DAVIS-complete数据集和提出的基准测试为开发能够更好地泛化到蛋白质修饰的模型奠定了基础，有望推动药物发现领域中的精准医疗。"}}
{"id": "2512.01870", "title": "Testing Transformer Learnability on the Arithmetic Sequence of Rooted Trees", "authors": ["Alessandro Breccia", "Federica Gerace", "Marco Lippi", "Gabriele Sicuro", "Pierluigi Contucci"], "summary": "We study whether a Large Language Model can learn the deterministic sequence of trees generated by the iterated prime factorization of the natural numbers. Each integer is mapped into a rooted planar tree and the resulting sequence $ \\mathbb{N}\\mathcal{T}$ defines an arithmetic text with measurable statistical structure. A transformer network (the GPT-2 architecture) is trained from scratch on the first $10^{11}$ elements to subsequently test its predictive ability under next-word and masked-word prediction tasks. Our results show that the model partially learns the internal grammar of $\\mathbb{N}\\mathcal{T}$, capturing non-trivial regularities and correlations. This suggests that learnability may extend beyond empirical data to the very structure of arithmetic.", "abs": "", "categories": ["cs.AI", "cond-mat.dis-nn", "math-ph", "math.NT"], "AI": {"tldr": "本文研究了大型语言模型是否能够学习由自然数迭代素数分解生成的树序列，结果表明模型部分学习了该序列的内部语法，捕捉到了一些非平凡的规律和相关性。", "motivation": "素数分解的计算复杂度高，且大数分解序列稀疏，本文旨在探索大型语言模型是否能从机器学习的角度提供优势，并验证算术结构是否能被学习。", "method": "将自然数映射到根树，形成一个无限序列NT，该序列被视为一个确定性的文本。使用GPT-2架构的Transformer网络，在包含前10^11个元素的序列上进行从头训练，并测试其“下一个词”和“掩码词”预测能力。", "result": "实验结果表明，模型部分学习了NT的内部语法，捕捉到了一些非平凡的规律和相关性，表明模型能够识别序列中的结构。", "conclusion": "研究表明，学习能力可能超越经验数据，扩展到算术本身的结构，暗示了算术结构可能具有类似于自然语言的语法特征，为利用机器学习方法研究算术结构提供了新的思路。"}}
{"id": "2512.00379", "title": "EnzyCLIP: A Cross-Attention Dual Encoder Framework with Contrastive Learning for Predicting Enzyme Kinetic Constants", "authors": ["Anas Aziz Khan", "Md Shah Fahad", "Priyanka", "Ramesh Chandra", "Guransh Singh"], "summary": "Accurate prediction of enzyme kinetic parameters is crucial for drug discovery, metabolic engineering, and synthetic biology applications. Current computational approaches face limitations in capturing complex enzyme-substrate interactions and often focus on single parameters while neglecting the joint prediction of catalytic turnover numbers (Kcat) and Michaelis-Menten constants (Km). We present EnzyCLIP, a novel dual-encoder framework that leverages contrastive learning and cross-attention mechanisms to predict enzyme kinetic parameters from protein sequences and substrate molecular structures. Our approach integrates ESM-2 protein language model embeddings with ChemBERTa chemical representations through a CLIP-inspired architecture enhanced with bidirectional cross-attention for dynamic enzyme-substrate interaction modeling. EnzyCLIP combines InfoNCE contrastive loss with Huber regression loss to learn aligned multimodal representations while predicting log10-transformed kinetic parameters. The model is trained on the CatPred-DB database containing 23,151 Kcat and 41,174 Km experimentally validated measurements, and achieved competitive performance with R2 scores of 0.593 for Kcat and 0.607 for Km prediction. XGBoost ensemble methods applied to the learned embeddings further improved Km prediction (R2 = 0.61) while maintaining robust Kcat performance.", "abs": "", "categories": ["q-bio.BM", "cs.LG"], "AI": {"tldr": "EnzyCLIP是一个新的双编码器框架，利用对比学习和交叉注意力机制，能够根据蛋白质序列和底物分子结构预测酶动力学常数，在酶动力学参数预测方面取得了有竞争力的性能。", "motivation": "现有计算方法在捕获复杂的酶-底物相互作用方面存在局限性，并且通常侧重于单个参数，而忽略了催化转化数 (Kcat) 和米氏常数 (Km) 的联合预测。", "method": "EnzyCLIP 采用 CLIP 架构，结合 ESM-2 蛋白质语言模型嵌入和 ChemBERTa 化学表示，通过双向交叉注意力动态建模酶-底物相互作用。它使用 InfoNCE 对比损失和 Huber 回归损失来学习对齐的多模态表示，并预测对数转换的动力学参数。", "result": "EnzyCLIP 在 CatPred-DB 数据库上训练，实现了 Kcat 的 R2 值为 0.593，Km 的 R2 值为 0.607。使用学习到的嵌入的 XGBoost 集群方法进一步提高了 Km 预测 (R2=0.61)，同时保持了 Kcat 的鲁棒性能。", "conclusion": "EnzyCLIP 提供了一种有效的方法来预测酶动力学常数，有望促进药物发现、代谢工程和合成生物学等领域的发展。"}}
{"id": "2512.01317", "title": "Data-Driven Learnability Transition of Measurement-Induced Entanglement", "authors": ["Dongheng Qian", "Jing Wang"], "summary": "Measurement-induced entanglement (MIE) captures how local measurements generate long-range quantum correlations and drive dynamical phase transitions in many-body systems. Yet estimating MIE experimentally remains challenging: direct evaluation requires extensive post-selection over measurement outcomes, raising the question of whether MIE is accessible with only polynomial resources. We address this challenge by reframing MIE detection as a data-driven learning problem that assumes no prior knowledge of state preparation. Using measurement records alone, we train a neural network in a self-supervised manner to predict the uncertainty metric for MIE--the gap between upper and lower bounds of the average post-measurement bipartite entanglement. Applied to random circuits with one-dimensional all-to-all connectivity and two-dimensional nearest-neighbor coupling, our method reveals a learnability transition with increasing circuit depth: below a threshold, the uncertainty is small and decreases with polynomial measurement data and model parameters, while above it the uncertainty remains large despite increasing resources. We further verify this transition experimentally on current noisy quantum devices, demonstrating its robustness to realistic noise. These results highlight the power of data-driven approaches for learning MIE and delineate the practical limits of its classical learnability.", "abs": "", "categories": ["quant-ph", "cond-mat.dis-nn", "cs.AI"], "AI": {"tldr": "本文提出了一种数据驱动的方法，利用测量记录训练神经网络来预测测量诱导纠缠(MIE)的指标，无需后选择和预先知识。研究揭示了MIE的可学习性转变，并验证了其在真实量子设备上的鲁棒性。", "motivation": "直接表征MIE在实验中面临挑战，需要大量的后选择操作。本文旨在解决这个问题，探索在仅使用测量记录的情况下，利用数据驱动方法学习MIE的可能性，并确定其经典可学习性的实际限制。", "method": "研究人员将MIE检测重新定义为数据驱动学习问题，使用测量记录训练基于Transformer的神经网络，以自监督的方式预测MIE的指标——平均后测量双足纠缠的上界和下界之间的差距。该方法应用于具有一维全互连和二维最近邻耦合的随机电路。", "result": "研究发现，随着电路深度的增加，存在可学习性转变：在阈值以下，不确定性较小，并且随着多项式测量数据和模型参数的增加而减小；在阈值以上，不确定性保持较大，即使资源增加也无法改善。该转变在当前的噪声量子设备上得到了实验验证。", "conclusion": "本文的成果突出了数据驱动方法在学习MIE方面的潜力，并明确了其经典可学习性的实际限制。该研究为在实验中探测和利用MIE提供了新的思路，并为量子信息科学的发展提供了有价值的参考。"}}
