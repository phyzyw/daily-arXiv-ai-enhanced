{"id": "2602.10637", "title": "Coarse-Grained Boltzmann Generators", "authors": ["Weilong Chen", "Bojun Zhao", "Jan Eckwert", "Julija Zavadlav"], "summary": "Sampling equilibrium molecular configurations from the Boltzmann distribution is a longstanding challenge. Boltzmann Generators (BGs) address this by combining exact-likelihood generative models with importance sampling, but their practical scalability is limited. Meanwhile, coarse-grained surrogates enable the modeling of larger systems by reducing effective dimensionality, yet often lack the reweighting process required to ensure asymptotically correct statistics. In this work, we propose Coarse-Grained Boltzmann Generators (CG-BGs), a principled framework that unifies scalable reduced-order modeling with the exactness of importance sampling. CG-BGs act in a coarse-grained coordinate space, using a learned potential of mean force (PMF) to reweight samples generated by a flow-based model. Crucially, we show that this PMF can be efficiently learned from rapidly converged data via force matching. Our results demonstrate that CG-BGs faithfully capture complex interactions mediated by explicit solvent within highly reduced representations, establishing a scalable pathway for the unbiased sampling of larger molecular systems.", "published": "2026-02-11", "categories": ["cs.LG", "cond-mat.stat-mech", "physics.chem-ph", "stat.ML"], "pdf_url": "https://arxiv.org/pdf/2602.10637v1", "primary_category": "cs.LG"}
{"id": "2602.11097", "title": "Statistical Learning Analysis of Physics-Informed Neural Networks", "authors": ["David A. Barajas-Solano"], "summary": "We study the training and performance of physics-informed learning for initial and boundary value problems (IBVP) with physics-informed neural networks (PINNs) from a statistical learning perspective. Specifically, we restrict ourselves to parameterizations with hard initial and boundary condition constraints and reformulate the problem of estimating PINN parameters as a statistical learning problem. From this perspective, the physics penalty on the IBVP residuals can be better understood not as a regularizing term bus as an infinite source of indirect data, and the learning process as fitting the PINN distribution of residuals $p(y \\mid x, t, w) q(x, t) $ to the true data-generating distribution $δ(0) q(x, t)$ by minimizing the Kullback-Leibler divergence between the true and PINN distributions. Furthermore, this analysis show that physics-informed learning with PINNs is a singular learning problem, and we employ singular learning theory tools, namely the so-called Local Learning Coefficient (Lau et al., 2025) to analyze the estimates of PINN parameters obtained via stochastic optimization for a heat equation IBVP. Finally, we discuss implications of this analysis on the quantification of predictive uncertainty of PINNs and the extrapolation capacity of PINNs.", "published": "2026-02-11", "categories": ["cs.LG", "physics.comp-ph"], "pdf_url": "https://arxiv.org/pdf/2602.11097v1", "primary_category": "cs.LG"}
{"id": "2602.10611", "title": "On the Role of Consistency Between Physics and Data in Physics-Informed Neural Networks", "authors": ["Nicolás Becerra-Zuniga", "Lucas Lacasa", "Eusebio Valero", "Gonzalo Rubio"], "summary": "Physics-informed neural networks (PINNs) have gained significant attention as a surrogate modeling strategy for partial differential equations (PDEs), particularly in regimes where labeled data are scarce and physical constraints can be leveraged to regularize the learning process. In practice, however, PINNs are frequently trained using experimental or numerical data that are not fully consistent with the governing equations due to measurement noise, discretization errors, or modeling assumptions. The implications of such data-to-PDE inconsistencies on the accuracy and convergence of PINNs remain insufficiently understood. In this work, we systematically analyze how data inconsistency fundamentally limits the attainable accuracy of PINNs. We introduce the concept of a consistency barrier, defined as an intrinsic lower bound on the error that arises from mismatches between the fidelity of the data and the exact enforcement of the PDE residual. To isolate and quantify this effect, we consider the 1D viscous Burgers equation with a manufactured analytical solution, which enables full control over data fidelity and residual errors. PINNs are trained using datasets of progressively increasing numerical accuracy, as well as perfectly consistent analytical data. Results show that while the inclusion of the PDE residual allows PINNs to partially mitigate low-fidelity data and recover the dominant physical structure, the training process ultimately saturates at an error level dictated by the data inconsistency. When high-fidelity numerical data are employed, PINN solutions become indistinguishable from those trained on analytical data, indicating that the consistency barrier is effectively removed. These findings clarify the interplay between data quality and physics enforcement in PINNs providing practical guidance for the construction and interpretation of physics-informed surrogate models.", "published": "2026-02-11", "categories": ["cs.LG", "physics.comp-ph", "stat.ML"], "pdf_url": "https://arxiv.org/pdf/2602.10611v1", "primary_category": "cs.LG"}
{"id": "2602.10451", "title": "A Multimodal Conditional Mixture Model with Distribution-Level Physics Priors", "authors": ["Jinkyo Han", "Bahador Bahmani"], "summary": "Many scientific and engineering systems exhibit intrinsically multimodal behavior arising from latent regime switching and non-unique physical mechanisms. In such settings, learning the full conditional distribution of admissible outcomes in a physically consistent and interpretable manner remains a challenge. While recent advances in machine learning have enabled powerful multimodal generative modeling, their integration with physics-constrained scientific modeling remains nontrivial, particularly when physical structure must be preserved or data are limited. This work develops a physics-informed multimodal conditional modeling framework based on mixture density representations. Mixture density networks (MDNs) provide an explicit and interpretable parameterization of multimodal conditional distributions. Physical knowledge is embedded through component-specific regularization terms that penalize violations of governing equations or physical laws. This formulation naturally accommodates non-uniqueness and stochasticity while remaining computationally efficient and amenable to conditioning on contextual inputs. The proposed framework is evaluated across a range of scientific problems in which multimodality arises from intrinsic physical mechanisms rather than observational noise, including bifurcation phenomena in nonlinear dynamical systems, stochastic partial differential equations, and atomistic-scale shock dynamics. In addition, the proposed method is compared with a conditional flow matching (CFM) model, a representative state-of-the-art generative modeling approach, demonstrating that MDNs can achieve competitive performance while offering a simpler and more interpretable formulation.", "published": "2026-02-11", "categories": ["cs.LG", "physics.comp-ph"], "pdf_url": "https://arxiv.org/pdf/2602.10451v1", "primary_category": "cs.LG"}
{"id": "2602.10158", "title": "NMRTrans: Structure Elucidation from Experimental NMR Spectra via Set Transformers", "authors": ["Liujia Yang", "Zhuo Yang", "Jiaqing Xie", "Yubin Wang", "Ben Gao", "Tianfan Fu", "Xingjian Wei", "Jiaxing Sun", "Jiang Wu", "Conghui He", "Yuqiang Li", "Qinying Gu"], "summary": "Nuclear Magnetic Resonance (NMR) spectroscopy is fundamental for molecular structure elucidation, yet interpreting spectra at scale remains time-consuming and highly expertise-dependent. While recent spectrum-as-language modeling and retrieval-based methods have shown promise, they rely heavily on large corpora of computed spectra and exhibit notable performance drops when applied to experimental measurements. To address these issues, we build NMRSpec, a large-scale corpus of experimental $^1$H and $^{13}$C spectra mined from chemical literature, and propose NMRTrans, which models spectra as unordered peak sets and aligns the model's inductive bias with the physical nature of NMR. To our best knowledge, NMRTrans is the first NMR Transformer trained solely on large-scale experimental spectra and achieves state-of-the-art performance on experimental benchmarks, improving Top-10 Accuracy over the strongest baseline by +17.82 points (61.15% vs. 43.33%), and underscoring the importance of experimental data and structure-aware architectures for reliable NMR structure elucidation.", "published": "2026-02-10", "categories": ["physics.chem-ph", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2602.10158v1", "primary_category": "physics.chem-ph"}
{"id": "2602.10680", "title": "A solvable high-dimensional model where nonlinear autoencoders learn structure invisible to PCA while test loss misaligns with generalization", "authors": ["Vicente Conde Mendes", "Lorenzo Bardone", "Cédric Koller", "Jorge Medina Moreira", "Vittorio Erba", "Emanuele Troiani", "Lenka Zdeborová"], "summary": "Many real-world datasets contain hidden structure that cannot be detected by simple linear correlations between input features. For example, latent factors may influence the data in a coordinated way, even though their effect is invisible to covariance-based methods such as PCA. In practice, nonlinear neural networks often succeed in extracting such hidden structure in unsupervised and self-supervised learning. However, constructing a minimal high-dimensional model where this advantage can be rigorously analyzed has remained an open theoretical challenge. We introduce a tractable high-dimensional spiked model with two latent factors: one visible to covariance, and one statistically dependent yet uncorrelated, appearing only in higher-order moments. PCA and linear autoencoders fail to recover the latter, while a minimal nonlinear autoencoder provably extracts both. We analyze both the population risk, and empirical risk minimization. Our model also provides a tractable example where self-supervised test loss is poorly aligned with representation quality: nonlinear autoencoders recover latent structure that linear methods miss, even though their reconstruction loss is higher.", "published": "2026-02-11", "categories": ["stat.ML", "cond-mat.dis-nn", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2602.10680v1", "primary_category": "stat.ML"}
{"id": "2602.10209", "title": "Neural Network Quantum Field Theory from Transformer Architectures", "authors": ["Dmitry S. Ageev", "Yulia A. Ageeva"], "summary": "We propose a neural-network construction of Euclidean scalar quantum field theories from transformer attention heads, defining $n$-point correlators by averaging over random network parameters in the NN-QFT framework. For a single attention head, shared random softmax weights couple different width coordinates and induce non-Gaussian field statistics that persist in the infinite-width limit $d_k\\to\\infty$. We compute the two-point function in an attention-weight representation and show how Euclidean-invariant kernels can be engineered via random-feature token embeddings. We then analyze the connected four-point function and identify an \"independence-breaking\" contribution, expressible as a covariance over query-key weights, which remains finite at infinite width. Finally, we show that summing many independent heads with standard $1/N_h$ normalization suppresses connected non-Gaussian correlators as $1/N_h$, yielding a Gaussian NN-QFT in the large-head limit.", "published": "2026-02-10", "categories": ["cs.LG", "cond-mat.dis-nn", "hep-th"], "pdf_url": "https://arxiv.org/pdf/2602.10209v1", "primary_category": "cs.LG"}
