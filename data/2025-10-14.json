{"id": "2510.09784", "title": "Combined Representation and Generation with Diffusive State Predictive Information Bottleneck", "authors": ["Richard John", "Yunrui Qiu", "Lukas Herron", "Pratyush Tiwary"], "summary": "Generative modeling becomes increasingly data-intensive in high-dimensional spaces. In molecular science, where data collection is expensive and important events are rare, compression to lower-dimensional manifolds is especially important for various downstream tasks, including generation. We combine a time-lagged information bottleneck designed to characterize molecular important representations and a diffusion model in one joint training objective. The resulting protocol, which we term Diffusive State Predictive Information Bottleneck (D-SPIB), enables the balancing of representation learning and generation aims in one flexible architecture. Additionally, the model is capable of combining temperature information from different molecular simulation trajectories to learn a coherent and useful internal representation of thermodynamics. We benchmark D-SPIB on multiple molecular tasks and showcase its potential for exploring physical conditions outside the training set.", "published": "2025-10-10", "categories": ["cs.LG", "cond-mat.stat-mech", "q-bio.QM"], "pdf_url": "http://arxiv.org/pdf/2510.09784v1", "primary_category": "cs.LG"}
{"id": "2510.11209", "title": "Cross-Scale Reservoir Computing for large spatio-temporal forecasting and modeling", "authors": ["Nicola Alboré", "Gabriele Di Antonio", "Fabrizio Coccetti", "Andrea Gabrielli"], "summary": "We propose a new reservoir computing method for forecasting high-resolution spatiotemporal datasets. By combining multi-resolution inputs from coarser to finer layers, our architecture better captures both local and global dynamics. Applied to Sea Surface Temperature data, it outperforms standard parallel reservoir models in long-term forecasting, demonstrating the effectiveness of cross-layers coupling in improving predictive accuracy. Finally, we show that the optimal network dynamics in each layer become increasingly linear, revealing the slow modes propagated to subsequent layers.", "published": "2025-10-13", "categories": ["cs.LG", "physics.comp-ph"], "pdf_url": "http://arxiv.org/pdf/2510.11209v1", "primary_category": "cs.LG"}
{"id": "2510.11148", "title": "Enhanced Sampling for Efficient Learning of Coarse-Grained Machine Learning Potentials", "authors": ["Weilong Chen", "Franz Görlich", "Paul Fuchs", "Julija Zavadlav"], "summary": "Coarse-graining (CG) enables molecular dynamics (MD) simulations of larger systems and longer timescales that are otherwise infeasible with atomistic models. Machine learning potentials (MLPs), with their capacity to capture many-body interactions, can provide accurate approximations of the potential of mean force (PMF) in CG models. Current CG MLPs are typically trained in a bottom-up manner via force matching, which in practice relies on configurations sampled from the unbiased equilibrium Boltzmann distribution to ensure thermodynamic consistency. This convention poses two key limitations: first, sufficiently long atomistic trajectories are needed to reach convergence; and second, even once equilibrated, transition regions remain poorly sampled. To address these issues, we employ enhanced sampling to bias along CG degrees of freedom for data generation, and then recompute the forces with respect to the unbiased potential. This strategy simultaneously shortens the simulation time required to produce equilibrated data and enriches sampling in transition regions, while preserving the correct PMF. We demonstrate its effectiveness on the M\\\"uller-Brown potential and capped alanine, achieving notable improvements. Our findings support the use of enhanced sampling for force matching as a promising direction to improve the accuracy and reliability of CG MLPs.", "published": "2025-10-13", "categories": ["physics.chem-ph", "cs.LG", "physics.comp-ph"], "pdf_url": "http://arxiv.org/pdf/2510.11148v1", "primary_category": "physics.chem-ph"}
{"id": "2510.10483", "title": "Gradient Enhanced Self-Training Physics-Informed Neural Network (gST-PINN) for Solving Nonlinear Partial Differential Equations", "authors": ["Narayan S Iyer", "Bivas Bhaumik", "Ram S Iyer", "Satyasaran Changdar"], "summary": "Partial differential equations (PDEs) provide a mathematical foundation for simulating and understanding intricate behaviors in both physical sciences and engineering. With the growing capabilities of deep learning, data$-$driven approaches like Physics$-$Informed Neural Networks (PINNs) have been developed, offering a mesh$-$free, analytic type framework for efficiently solving PDEs across a wide range of applications. However, traditional PINNs often struggle with challenges such as limited precision, slow training dynamics, lack of labeled data availability, and inadequate handling of multi$-$physics interactions. To overcome these challenging issues of PINNs, we proposed a Gradient Enhanced Self$-$Training PINN (gST$-$PINN) method that specifically introduces a gradient based pseudo point self$-$learning algorithm for solving PDEs. We tested the proposed method on three different types of PDE problems from various fields, each representing distinct scenarios. The effectiveness of the proposed method is evident, as the PINN approach for solving the Burgers$'$ equation attains a mean square error (MSE) on the order of $10^{-3}$, while the diffusion$-$sorption equation achieves an MSE on the order of $10^{-4}$ after 12,500 iterations, with no further improvement as the iterations increase. In contrast, the MSE for both PDEs in the gST$-$PINN model continues to decrease, demonstrating better generalization and reaching an MSE on the order of $10^{-5}$ after 18,500 iterations. Furthermore, the results show that the proposed purely semi$-$supervised gST$-$PINN consistently outperforms the standard PINN method in all cases, even when solution of the PDEs are unavailable. It generalizes both PINN and Gradient$-$enhanced PINN (gPINN), and can be effectively applied in scenarios prone to low accuracy and convergence issues, particularly in the absence of labeled data.", "published": "2025-10-12", "categories": ["cs.LG", "physics.comp-ph"], "pdf_url": "http://arxiv.org/pdf/2510.10483v1", "primary_category": "cs.LG"}
{"id": "2510.09768", "title": "Scaling Laws and Symmetry, Evidence from Neural Force Fields", "authors": ["Khang Ngo", "Siamak Ravanbakhsh"], "summary": "We present an empirical study in the geometric task of learning interatomic potentials, which shows equivariance matters even more at larger scales; we show a clear power-law scaling behaviour with respect to data, parameters and compute with ``architecture-dependent exponents''. In particular, we observe that equivariant architectures, which leverage task symmetry, scale better than non-equivariant models. Moreover, among equivariant architectures, higher-order representations translate to better scaling exponents. Our analysis also suggests that for compute-optimal training, the data and model sizes should scale in tandem regardless of the architecture. At a high level, these results suggest that, contrary to common belief, we should not leave it to the model to discover fundamental inductive biases such as symmetry, especially as we scale, because they change the inherent difficulty of the task and its scaling laws.", "published": "2025-10-10", "categories": ["cs.LG", "cs.AI", "physics.comp-ph"], "pdf_url": "http://arxiv.org/pdf/2510.09768v1", "primary_category": "cs.LG"}
{"id": "2510.11188", "title": "Protein as a Second Language for LLMs", "authors": ["Xinhui Chen", "Zuchao Li", "Mengqi Gao", "Yufeng Zhang", "Chak Tou Leong", "Haoyang Li", "Jiaqi Chen"], "summary": "Deciphering the function of unseen protein sequences is a fundamental challenge with broad scientific impact, yet most existing methods depend on task-specific adapters or large-scale supervised fine-tuning. We introduce the \"Protein-as-Second-Language\" framework, which reformulates amino-acid sequences as sentences in a novel symbolic language that large language models can interpret through contextual exemplars. Our approach adaptively constructs sequence-question-answer triples that reveal functional cues in a zero-shot setting, without any further training. To support this process, we curate a bilingual corpus of 79,926 protein-QA instances spanning attribute prediction, descriptive understanding, and extended reasoning. Empirically, our method delivers consistent gains across diverse open-source LLMs and GPT-4, achieving up to 17.2% ROUGE-L improvement (average +7%) and even surpassing fine-tuned protein-specific language models. These results highlight that generic LLMs, when guided with protein-as-language cues, can outperform domain-specialized models, offering a scalable pathway for protein understanding in foundation models.", "published": "2025-10-13", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "pdf_url": "http://arxiv.org/pdf/2510.11188v1", "primary_category": "cs.LG"}
{"id": "2510.10020", "title": "Calibrating Generative Models", "authors": ["Henry D. Smith", "Nathaniel L. Diamant", "Brian L. Trippe"], "summary": "Generative models frequently suffer miscalibration, wherein class probabilities and other statistics of the sampling distribution deviate from desired values. We frame calibration as a constrained optimization problem and seek the closest model in Kullback-Leibler divergence satisfying calibration constraints. To address the intractability of imposing these constraints exactly, we introduce two surrogate objectives for fine-tuning: (1) the relax loss, which replaces the constraint with a miscalibration penalty, and (2) the reward loss, which converts calibration into a reward fine-tuning problem. We demonstrate that these approaches substantially reduce calibration error across hundreds of simultaneous constraints and models with up to one billion parameters, spanning applications in protein design, image generation, and language modeling.", "published": "2025-10-11", "categories": ["stat.ML", "cs.LG", "q-bio.BM"], "pdf_url": "http://arxiv.org/pdf/2510.10020v1", "primary_category": "stat.ML"}
{"id": "2510.10693", "title": "High-Dimensional Learning Dynamics of Quantized Models with Straight-Through Estimator", "authors": ["Yuma Ichikawa", "Shuhei Kashiwamura", "Ayaka Sakata"], "summary": "Quantized neural network training optimizes a discrete, non-differentiable objective. The straight-through estimator (STE) enables backpropagation through surrogate gradients and is widely used. While previous studies have primarily focused on the properties of surrogate gradients and their convergence, the influence of quantization hyperparameters, such as bit width and quantization range, on learning dynamics remains largely unexplored. We theoretically show that in the high-dimensional limit, STE dynamics converge to a deterministic ordinary differential equation. This reveals that STE training exhibits a plateau followed by a sharp drop in generalization error, with plateau length depending on the quantization range. A fixed-point analysis quantifies the asymptotic deviation from the unquantized linear model. We also extend analytical techniques for stochastic gradient descent to nonlinear transformations of weights and inputs.", "published": "2025-10-12", "categories": ["stat.ML", "cond-mat.dis-nn", "cs.AI", "cs.LG", "math.ST", "stat.TH"], "pdf_url": "http://arxiv.org/pdf/2510.10693v1", "primary_category": "stat.ML"}
