{"id": "2511.01976", "title": "Stability of mixed-state phases under weak decoherence", "authors": ["Yifan F. Zhang", "Sarang Gopalakrishnan"], "summary": "We prove that the Gibbs states of classical, and commuting-Pauli, Hamiltonians are stable under weak local decoherence: i.e., we show that the effect of the decoherence can be locally reversed. In particular, our conclusions apply to finite-temperature equilibrium critical points and ordered low-temperature phases. In these systems the unconditional spatio-temporal correlations are long-range, and local (e.g., Metropolis) dynamics exhibits critical slowing down. Nevertheless, our results imply the existence of local \"decoders\" that undo the decoherence, when the decoherence strength is below a critical value. An implication of these results is that thermally stable quantum memories have a threshold against decoherence that remains nonzero as one approaches the critical temperature. Analogously, in diffusion models, stability of data distributions implies the existence of computationally-efficent local denoisers in the late-time generation dynamics.", "published": "2025-11-03", "categories": ["quant-ph", "cond-mat.stat-mech", "cs.LG", "math-ph", "math.MP"], "pdf_url": "http://arxiv.org/pdf/2511.01976v1", "primary_category": "quant-ph"}
{"id": "2511.02584", "title": "Redundancy Maximization as a Principle of Associative Memory Learning", "authors": ["Mark Blümel", "Andreas C. Schneider", "Valentin Neuhaus", "David A. Ehrlich", "Marcel Graetz", "Michael Wibral", "Abdullah Makkeh", "Viola Priesemann"], "summary": "Associative memory, traditionally modeled by Hopfield networks, enables the retrieval of previously stored patterns from partial or noisy cues. Yet, the local computational principles which are required to enable this function remain incompletely understood. To formally characterize the local information processing in such systems, we employ a recent extension of information theory - Partial Information Decomposition (PID). PID decomposes the contribution of different inputs to an output into unique information from each input, redundant information across inputs, and synergistic information that emerges from combining different inputs. Applying this framework to individual neurons in classical Hopfield networks we find that below the memory capacity, the information in a neuron's activity is characterized by high redundancy between the external pattern input and the internal recurrent input, while synergy and unique information are close to zero until the memory capacity is surpassed and performance drops steeply. Inspired by this observation, we use redundancy as an information-theoretic learning goal, which is directly optimized for each neuron, dramatically increasing the network's memory capacity to 1.59, a more than tenfold improvement over the 0.14 capacity of classical Hopfield networks and even outperforming recent state-of-the-art implementations of Hopfield networks. Ultimately, this work establishes redundancy maximization as a new design principle for associative memories and opens pathways for new associative memory models based on information-theoretic goals.", "published": "2025-11-04", "categories": ["cs.IT", "cs.LG", "cs.NE", "math.IT", "physics.comp-ph"], "pdf_url": "http://arxiv.org/pdf/2511.02584v1", "primary_category": "cs.IT"}
{"id": "2511.02087", "title": "Energy Loss Functions for Physical Systems", "authors": ["Sékou-Oumar Kaba", "Kusha Sareen", "Daniel Levy", "Siamak Ravanbakhsh"], "summary": "Effectively leveraging prior knowledge of a system's physics is crucial for applications of machine learning to scientific domains. Previous approaches mostly focused on incorporating physical insights at the architectural level. In this paper, we propose a framework to leverage physical information directly into the loss function for prediction and generative modeling tasks on systems like molecules and spins. We derive energy loss functions assuming that each data sample is in thermal equilibrium with respect to an approximate energy landscape. By using the reverse KL divergence with a Boltzmann distribution around the data, we obtain the loss as an energy difference between the data and the model predictions. This perspective also recasts traditional objectives like MSE as energy-based, but with a physically meaningless energy. In contrast, our formulation yields physically grounded loss functions with gradients that better align with valid configurations, while being architecture-agnostic and computationally efficient. The energy loss functions also inherently respect physical symmetries. We demonstrate our approach on molecular generation and spin ground-state prediction and report significant improvements over baselines.", "published": "2025-11-03", "categories": ["cs.LG", "cs.AI", "physics.comp-ph"], "pdf_url": "http://arxiv.org/pdf/2511.02087v1", "primary_category": "cs.LG"}
{"id": "2511.01464", "title": "Split-Flows: Measure Transport and Information Loss Across Molecular Resolutions", "authors": ["Sander Hummerich", "Tristan Bereau", "Ullrich Köthe"], "summary": "By reducing resolution, coarse-grained models greatly accelerate molecular simulations, unlocking access to long-timescale phenomena, though at the expense of microscopic information. Recovering this fine-grained detail is essential for tasks that depend on atomistic accuracy, making backmapping a central challenge in molecular modeling. We introduce split-flows, a novel flow-based approach that reinterprets backmapping as a continuous-time measure transport across resolutions. Unlike existing generative strategies, split-flows establish a direct probabilistic link between resolutions, enabling expressive conditional sampling of atomistic structures and -- for the first time -- a tractable route to computing mapping entropies, an information-theoretic measure of the irreducible detail lost in coarse-graining. We demonstrate these capabilities on diverse molecular systems, including chignolin, a lipid bilayer, and alanine dipeptide, highlighting split-flows as a principled framework for accurate backmapping and systematic evaluation of coarse-grained models.", "published": "2025-11-03", "categories": ["physics.chem-ph", "cs.LG", "physics.comp-ph"], "pdf_url": "http://arxiv.org/pdf/2511.01464v1", "primary_category": "physics.chem-ph"}
{"id": "2511.01913", "title": "Delta-learned force fields for nonbonded interactions: Addressing the strength mismatch between covalent-nonbonded interaction for global models", "authors": ["Leonardo Cázares-Trejo", "Marco Loreto-Silva", "Huziel E. Sauceda"], "summary": "Noncovalent interactions--vdW dispersion, hydrogen/halogen bonding, ion-$\\pi$, and $\\pi$-stacking--govern structure, dynamics, and emergent phenomena in materials and molecular systems, yet accurately learning them alongside covalent forces remains a core challenge for machine-learned force fields (MLFFs). This challenge is acute for global models that use Coulomb-matrix (CM) descriptors compared under Euclidean/Frobenius metrics in multifragment settings. We show that the mismatch between predominantly covalent force labels and the CM's overrepresentation of intermolecular features biases single-model training and degrades force-field fidelity. To address this, we introduce \\textit{$\\Delta$-sGDML}, a scale-aware formulation within the sGDML framework that explicitly decouples intra- and intermolecular physics by training fragment-specific models alongside a dedicated binding model, then composing them at inference. Across benzene dimers, host-guest complexes (C$_{60}$@buckycatcher, NO$_3^-$@i-corona[6]arene), benzene-water, and benzene-Na$^+$, \\mbox{$\\Delta$-sGDML} delivers consistent gains over a single global model, with fragment-resolved force-error reductions up to \\textbf{75\\%}, without loss of energy accuracy. Furthermore, molecular-dynamics simulations further confirm that the $\\Delta$-model yields a reliable force field for C$_{60}$@buckycatcher, producing stable trajectories across a wide range of temperatures (10-400~K), unlike the single global model, which loses stability above $\\sim$200~K. The method offers a practical route to homogenize per-fragment errors and recover reliable noncovalent physics in global MLFFs.", "published": "2025-11-01", "categories": ["physics.chem-ph", "cond-mat.mtrl-sci", "cs.LG", "physics.comp-ph"], "pdf_url": "http://arxiv.org/pdf/2511.01913v1", "primary_category": "physics.chem-ph"}
{"id": "2511.01946", "title": "COFAP: A Universal Framework for COFs Adsorption Prediction through Designed Multi-Modal Extraction and Cross-Modal Synergy", "authors": ["Zihan Li", "Mingyang Wan", "Mingyu Gao", "Zhongshan Chen", "Xiangke Wang", "Feifan Zhang"], "summary": "Covalent organic frameworks (COFs) are promising adsorbents for gas adsorption and separation, while identifying the optimal structures among their vast design space requires efficient high-throughput screening. Conventional machine-learning predictors rely heavily on specific gas-related features. However, these features are time-consuming and limit scalability, leading to inefficiency and labor-intensive processes. Herein, a universal COFs adsorption prediction framework (COFAP) is proposed, which can extract multi-modal structural and chemical features through deep learning, and fuse these complementary features via cross-modal attention mechanism. Without Henry coefficients or adsorption heat, COFAP sets a new SOTA by outperforming previous approaches on hypoCOFs dataset. Based on COFAP, we also found that high-performing COFs for separation concentrate within a narrow range of pore size and surface area. A weight-adjustable prioritization scheme is also developed to enable flexible, application-specific ranking of candidate COFs for researchers. Superior efficiency and accuracy render COFAP directly deployable in crystalline porous materials.", "published": "2025-11-03", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI", "physics.chem-ph"], "pdf_url": "http://arxiv.org/pdf/2511.01946v1", "primary_category": "cs.LG"}
{"id": "2511.01671", "title": "Spin-Adapted Neural Network Wavefunctions in Real Space", "authors": ["Ruichen Li", "Yuzhi Liu", "Du Jiang", "Yixiao Chen", "Xuelan Wen", "Wenrui Li", "Di He", "Liwei Wang", "Ji Chen", "Weiluo Ren"], "summary": "Spin plays a fundamental role in understanding electronic structure, yet many real-space wavefunction methods fail to adequately consider it. We introduce the Spin-Adapted Antisymmetrization Method (SAAM), a general procedure that enforces exact total spin symmetry for antisymmetric many-electron wavefunctions in real space. In the context of neural network-based quantum Monte Carlo (NNQMC), SAAM leverages the expressiveness of deep neural networks to capture electron correlation while enforcing exact spin adaptation via group representation theory. This framework provides a principled route to embed physical priors into otherwise black-box neural network wavefunctions, yielding a compact representation of correlated system with neural network orbitals. Compared with existing treatments of spin in NNQMC, SAAM is more accurate and efficient, achieving exact spin purity without any additional tunable hyperparameters. To demonstrate its effectiveness, we apply SAAM to study the spin ladder of iron-sulfur clusters, a long-standing challenge for many-body methods due to their dense spectrum of nearly degenerate spin states. Our results reveal accurate resolution of low-lying spin states and spin gaps in [Fe$_2$S$_2$] and [Fe$_4$S$_4$] clusters, offering new insights into their electronic structures. In sum, these findings establish SAAM as a robust, hyperparameter-free standard for spin-adapted NNQMC, particularly for strongly correlated systems.", "published": "2025-11-03", "categories": ["physics.chem-ph", "cs.AI"], "pdf_url": "http://arxiv.org/pdf/2511.01671v1", "primary_category": "physics.chem-ph"}
{"id": "2511.02769", "title": "STAR-VAE: Latent Variable Transformers for Scalable and Controllable Molecular Generation", "authors": ["Bum Chul Kwon", "Ben Shapira", "Moshiko Raboh", "Shreyans Sethi", "Shruti Murarka", "Joseph A Morrone", "Jianying Hu", "Parthasarathy Suryanarayanan"], "summary": "The chemical space of drug-like molecules is vast, motivating the development of generative models that must learn broad chemical distributions, enable conditional generation by capturing structure-property representations, and provide fast molecular generation. Meeting the objectives depends on modeling choices, including the probabilistic modeling approach, the conditional generative formulation, the architecture, and the molecular input representation. To address the challenges, we present STAR-VAE (Selfies-encoded, Transformer-based, AutoRegressive Variational Auto Encoder), a scalable latent-variable framework with a Transformer encoder and an autoregressive Transformer decoder. It is trained on 79 million drug-like molecules from PubChem, using SELFIES to guarantee syntactic validity. The latent-variable formulation enables conditional generation: a property predictor supplies a conditioning signal that is applied consistently to the latent prior, the inference network, and the decoder. Our contributions are: (i) a Transformer-based latent-variable encoder-decoder model trained on SELFIES representations; (ii) a principled conditional latent-variable formulation for property-guided generation; and (iii) efficient finetuning with low-rank adapters (LoRA) in both encoder and decoder, enabling fast adaptation with limited property and activity data. On the GuacaMol and MOSES benchmarks, our approach matches or exceeds baselines, and latent-space analyses reveal smooth, semantically structured representations that support both unconditional exploration and property-aware generation. On the Tartarus benchmarks, the conditional model shifts docking-score distributions toward stronger predicted binding. These results suggest that a modernized, scale-appropriate VAE remains competitive for molecular generation when paired with principled conditioning and parameter-efficient finetuning.", "published": "2025-11-04", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "pdf_url": "http://arxiv.org/pdf/2511.02769v1", "primary_category": "cs.LG"}
{"id": "2511.02003", "title": "Bulk-boundary decomposition of neural networks", "authors": ["Donghee Lee", "Hye-Sung Lee", "Jaeok Yi"], "summary": "We present the bulk-boundary decomposition as a new framework for understanding the training dynamics of deep neural networks. Starting from the stochastic gradient descent formulation, we show that the Lagrangian can be reorganized into a data-independent bulk term and a data-dependent boundary term. The bulk captures the intrinsic dynamics set by network architecture and activation functions, while the boundary reflects stochastic interactions from training samples at the input and output layers. This decomposition exposes the local and homogeneous structure underlying deep networks. As a natural extension, we develop a field-theoretic formulation of neural dynamics based on this decomposition.", "published": "2025-11-03", "categories": ["cs.LG", "cond-mat.dis-nn", "hep-ph"], "pdf_url": "http://arxiv.org/pdf/2511.02003v1", "primary_category": "cs.LG"}
{"id": "2511.01037", "title": "Binary perceptron computational gap -- a parametric fl RDT view", "authors": ["Mihailo Stojnic"], "summary": "Recent studies suggest that asymmetric binary perceptron (ABP) likely exhibits the so-called statistical-computational gap characterized with the appearance of two phase transitioning constraint density thresholds: \\textbf{\\emph{(i)}} the \\emph{satisfiability threshold} $\\alpha_c$, below/above which ABP succeeds/fails to operate as a storage memory; and \\textbf{\\emph{(ii)}} \\emph{algorithmic threshold} $\\alpha_a$, below/above which one can/cannot efficiently determine ABP's weight so that it operates as a storage memory.   We consider a particular parametric utilization of \\emph{fully lifted random duality theory} (fl RDT) [85] and study its potential ABP's algorithmic implications. A remarkable structural parametric change is uncovered as one progresses through fl RDT lifting levels. On the first two levels, the so-called $\\c$ sequence -- a key parametric fl RDT component -- is of the (natural) decreasing type. A change of such phenomenology on higher levels is then connected to the $\\alpha_c$ -- $\\alpha_a$ threshold change. Namely, on the second level concrete numerical values give for the critical constraint density $\\alpha=\\alpha_c\\approx 0.8331$. While progressing through higher levels decreases this estimate, already on the fifth level we observe a satisfactory level of convergence and obtain $\\alpha\\approx 0.7764$. This allows to draw two striking parallels: \\textbf{\\emph{(i)}} the obtained constraint density estimate is in a remarkable agrement with range $\\alpha\\in (0.77,0.78)$ of clustering defragmentation (believed to be responsible for failure of locally improving algorithms) [17,88]; and \\textbf{\\emph{(ii)}} the observed change of $\\c$ sequence phenomenology closely matches the one of the negative Hopfield model for which the existence of efficient algorithms that closely approach similar type of threshold has been demonstrated recently [87].", "published": "2025-11-02", "categories": ["stat.ML", "cond-mat.dis-nn", "cs.IT", "cs.LG", "math.IT", "math.PR"], "pdf_url": "http://arxiv.org/pdf/2511.01037v1", "primary_category": "stat.ML"}
{"id": "2511.00746", "title": "Correspondence Between Ising Machines and Neural Networks", "authors": ["Andrew G. Moore"], "summary": "Computation with the Ising model is central to future computing technologies like quantum annealing, adiabatic quantum computing, and thermodynamic classical computing. Traditionally, computed values have been equated with ground states. This paper generalizes computation with ground states to computation with spin averages, allowing computations to take place at high temperatures. It then introduces a systematic correspondence between Ising devices and neural networks and a simple method to run trained feed-forward neural networks on Ising-type hardware. Finally, a mathematical proof is offered that these implementations are always successful.", "published": "2025-11-02", "categories": ["cond-mat.dis-nn", "cs.ET", "cs.LG", "quant-ph"], "pdf_url": "http://arxiv.org/pdf/2511.00746v1", "primary_category": "cond-mat.dis-nn"}
