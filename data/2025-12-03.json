{"id": "2512.02904", "title": "Towards a fully differentiable digital twin for solar cells", "authors": ["Marie Louise Schubert", "Houssam Metni", "Jan David Fischbach", "Benedikt Zerulla", "Marjan Krstić", "Ulrich W. Paetzold", "Seyedamir Orooji", "Olivier J. J. Ronsin", "Yasin Ameslon", "Jens Harting", "Thomas Kirchartz", "Sandheep Ravishankar", "Chris Dreessen", "Eunchi Kim", "Christian Sprau", "Mohamed Hussein", "Alexander Colsmann", "Karen Forberich", "Klaus Jäger", "Pascal Friederich", "Carsten Rockstuhl"], "summary": "Maximizing energy yield (EY) - the total electric energy generated by a solar cell within a year at a specific location - is crucial in photovoltaics (PV), especially for emerging technologies. Computational methods provide the necessary insights and guidance for future research. However, existing simulations typically focus on only isolated aspects of solar cells. This lack of consistency highlights the need for a framework unifying all computational levels, from material to cell properties, for accurate prediction and optimization of EY prediction. To address this challenge, a differentiable digital twin, Sol(Di)$^2$T, is introduced to enable comprehensive end-to-end optimization of solar cells. The workflow starts with material properties and morphological processing parameters, followed by optical and electrical simulations. Finally, climatic conditions and geographic location are incorporated to predict the EY. Each step is either intrinsically differentiable or replaced with a machine-learned surrogate model, enabling not only accurate EY prediction but also gradient-based optimization with respect to input parameters. Consequently, Sol(Di)$^2$T extends EY predictions to previously unexplored conditions. Demonstrated for an organic solar cell, the proposed framework marks a significant step towards tailoring solar cells for specific applications while ensuring maximal performance.", "published": "2025-12-02", "categories": ["physics.comp-ph", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2512.02904v1", "primary_category": "physics.comp-ph"}
{"id": "2512.02315", "title": "Few-shot Protein Fitness Prediction via In-context Learning and Test-time Training", "authors": ["Felix Teufel", "Aaron W. Kollasch", "Yining Huang", "Ole Winther", "Kevin K. Yang", "Pascal Notin", "Debora S. Marks"], "summary": "Accurately predicting protein fitness with minimal experimental data is a persistent challenge in protein engineering. We introduce PRIMO (PRotein In-context Mutation Oracle), a transformer-based framework that leverages in-context learning and test-time training to adapt rapidly to new proteins and assays without large task-specific datasets. By encoding sequence information, auxiliary zero-shot predictions, and sparse experimental labels from many assays as a unified token set in a pre-training masked-language modeling paradigm, PRIMO learns to prioritize promising variants through a preference-based loss function. Across diverse protein families and properties-including both substitution and indel mutations-PRIMO outperforms zero-shot and fully supervised baselines. This work underscores the power of combining large-scale pre-training with efficient test-time adaptation to tackle challenging protein design tasks where data collection is expensive and label availability is limited.", "published": "2025-12-02", "categories": ["q-bio.BM", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2512.02315v1", "primary_category": "q-bio.BM"}
{"id": "2512.02303", "title": "Training Dynamics of Learning 3D-Rotational Equivariance", "authors": ["Max W. Shen", "Ewa Nowara", "Michael Maser", "Kyunghyun Cho"], "summary": "While data augmentation is widely used to train symmetry-agnostic models, it remains unclear how quickly and effectively they learn to respect symmetries. We investigate this by deriving a principled measure of equivariance error that, for convex losses, calculates the percent of total loss attributable to imperfections in learned symmetry. We focus our empirical investigation to 3D-rotation equivariance on high-dimensional molecular tasks (flow matching, force field prediction, denoising voxels) and find that models reduce equivariance error quickly to $\\leq$2\\% held-out loss within 1k-10k training steps, a result robust to model and dataset size. This happens because learning 3D-rotational equivariance is an easier learning task, with a smoother and better-conditioned loss landscape, than the main prediction task. For 3D rotations, the loss penalty for non-equivariant models is small throughout training, so they may achieve lower test loss than equivariant models per GPU-hour unless the equivariant ``efficiency gap'' is narrowed. We also experimentally and theoretically investigate the relationships between relative equivariance error, learning gradients, and model parameters.", "published": "2025-12-02", "categories": ["cs.LG", "q-bio.BM"], "pdf_url": "https://arxiv.org/pdf/2512.02303v1", "primary_category": "cs.LG"}
