{"id": "2602.12449", "title": "Computationally sufficient statistics for Ising models", "authors": ["Abhijith Jayakumar", "Shreya Shukla", "Marc Vuffray", "Andrey Y. Lokhov", "Sidhant Misra"], "summary": "Learning Gibbs distributions using only sufficient statistics has long been recognized as a computationally hard problem. On the other hand, computationally efficient algorithms for learning Gibbs distributions rely on access to full sample configurations generated from the model. For many systems of interest that arise in physical contexts, expecting a full sample to be observed is not practical, and hence it is important to look for computationally efficient methods that solve the learning problem with access to only a limited set of statistics. We examine the trade-offs between the power of computation and observation within this scenario, employing the Ising model as a paradigmatic example. We demonstrate that it is feasible to reconstruct the model parameters for a model with $\\ell_1$ width $γ$ by observing statistics up to an order of $O(γ)$. This approach allows us to infer the model's structure and also learn its couplings and magnetic fields. We also discuss a setting where prior information about structure of the model is available and show that the learning problem can be solved efficiently with even more limited observational power.", "published": "2026-02-12", "categories": ["cs.LG", "cond-mat.stat-mech", "stat.ML"], "pdf_url": "https://arxiv.org/pdf/2602.12449v1", "primary_category": "cs.LG"}
{"id": "2602.13194", "title": "Semantic Chunking and the Entropy of Natural Language", "authors": ["Weishun Zhong", "Doron Sivan", "Tankut Can", "Mikhail Katkov", "Misha Tsodyks"], "summary": "The entropy rate of printed English is famously estimated to be about one bit per character, a benchmark that modern large language models (LLMs) have only recently approached. This entropy rate implies that English contains nearly 80 percent redundancy relative to the five bits per character expected for random text. We introduce a statistical model that attempts to capture the intricate multi-scale structure of natural language, providing a first-principles account of this redundancy level. Our model describes a procedure of self-similarly segmenting text into semantically coherent chunks down to the single-word level. The semantic structure of the text can then be hierarchically decomposed, allowing for analytical treatment. Numerical experiments with modern LLMs and open datasets suggest that our model quantitatively captures the structure of real texts at different levels of the semantic hierarchy. The entropy rate predicted by our model agrees with the estimated entropy rate of printed English. Moreover, our theory further reveals that the entropy rate of natural language is not fixed but should increase systematically with the semantic complexity of corpora, which are captured by the only free parameter in our model.", "published": "2026-02-13", "categories": ["cs.CL", "cond-mat.dis-nn", "cond-mat.stat-mech", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2602.13194v1", "primary_category": "cs.CL"}
