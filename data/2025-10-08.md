<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 4]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Thermodynamic Performance Limits for Score-Based Diffusion Models](https://arxiv.org/abs/2510.06174)
*Nathan X. Kodama, Michael Hinczewski*

Main category: cs.LG

TL;DR: 本文将基于得分的扩散模型与非平衡热力学联系起来，推导了基于熵率的性能下限，并对该下限进行了数值验证。


<details>
  <summary>Details</summary>
Motivation: 现有研究已将深度学习与非平衡热力学联系起来，但缺乏对扩散模型热力学操作的正式表述和性能下限。本文旨在填补这一空白，并借鉴麦克斯韦恶魔的思想。

Method: 本文通过推导与熵率相关的下限，建立了基于得分的扩散模型与非平衡热力学之间的联系。具体地，推导了一个关于负对数似然（NLL）的热力学下限，该下限依赖于系统熵率。

Result: 推导出了一个热力学下限，即 NLL≥S0+S1/2−1/2(2Z1/0˙Sθ(t)dt)，其中S0和S1分别是数据的熵和平衡分布的熵。该下限比基于ELBO和KL的上下限更具优势。

Conclusion: 本文将生成模型性能与基本物理原理联系起来，为理解扩散模型的热力学操作提供了新的见解，并对热力学计算硬件的设计具有潜在意义。

Abstract: We establish a fundamental connection between score-based diffusion models and non-equilibrium thermodynamics by deriving performance limits based on entropy rates. Our main theoretical contribution is a lower bound on the negative log-likelihood of the data that relates model performance to entropy rates of diffusion processes. We numerically validate this bound on a synthetic dataset and investigate its tightness. By building a bridge to entropy rates - system, intrinsic, and exchange entropy - we provide new insights into the thermodynamic operation of these models, drawing parallels to Maxwell's demon and implications for thermodynamic computing hardware. Our framework connects generative modeling performance to fundamental physical principles through stochastic thermodynamics.

</details>


### [2] [The Physics of Data and Tasks: Theories of Locality and Compositionality in Deep Learning](https://arxiv.org/abs/2510.06106)
*Alessandro Favero*

Main category: cs.LG

TL;DR: 本论文研究了深度学习中数据和任务的局部性和组合性，旨在理解深度神经网络如何克服维度灾难并实现泛化。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络取得了显著成功，但其学习机制尚不明确。维度灾难使得高维任务的学习在统计上难以处理，这表明可学习数据存在潜在结构，本研究旨在揭示这种结构并分析其对性能的影响。

Method: 论文结合统计物理和学习理论，分析了无限宽度卷积神经网络的泛化能力；利用概率上下文无关文法对数据进行建模，研究了扩散生成模型如何通过组合特征生成新数据；并探索了任务和技能在模型参数空间中的组合性。

Result: 研究表明，卷积神经网络可以通过适应目标函数的空间尺度克服维度灾难；扩散生成模型存在生成过程中的相变，支持自然数据的组合结构；学习这些文法的样本复杂度随数据维度多项式增长；大型预训练模型权重空间存在与特定任务相关的局部区域，支持任务算术和模型编辑。

Conclusion: 本研究为理解生成模型如何学习泛化和创造性提供了理论基础，并揭示了深度学习中数据、任务和模型表示的组合结构，为进一步研究深度学习的泛化能力和可解释性提供了新的视角。

Abstract: Deep neural networks have achieved remarkable success, yet our understanding of how they learn remains limited. These models can learn high-dimensional tasks, which is generally statistically intractable due to the curse of dimensionality. This apparent paradox suggests that learnable data must have an underlying latent structure. What is the nature of this structure? How do neural networks encode and exploit it, and how does it quantitatively impact performance - for instance, how does generalization improve with the number of training examples? This thesis addresses these questions by studying the roles of locality and compositionality in data, tasks, and deep learning representations.

</details>


### [3] [Physics-Informed Neural Networks with Fourier Features and Attention-Driven Decoding](https://arxiv.org/abs/2510.05385)
*Rohan Arni, Carlos Blanco*

Main category: cs.LG

TL;DR: 本文提出了Spectral PINNsformer (S-Pformer)架构，通过移除编码器并引入傅里叶特征嵌入，有效解决了传统PINN的频谱偏差问题，同时降低了参数量。


<details>
  <summary>Details</summary>
Motivation: 传统PINN（尤其是基于MLP的PINN）存在频谱偏差和泛化能力不足的问题，而基于Transformer的PINNsformer虽然改进了性能，但编码器存在冗余。因此，需要一种更高效、更准确的PINN架构。

Method: S-Pformer架构通过以下方式改进了PINNsformer：1) 移除编码器，直接使用自注意力机制；2) 引入傅里叶特征嵌入，在频域自适应地编码多尺度行为。

Result: 实验结果表明，S-Pformer在所有基准测试中优于传统的编码器-解码器PINNsformer架构，并且在参数量显著减少的情况下，达到了或超过了MLP的性能。

Conclusion: S-Pformer提供了一种更高效、更准确的PINN架构，能够更好地解决偏微分方程，并有望在科学和工程领域得到广泛应用。

Abstract: Physics-Informed Neural Networks (PINNs) are a useful framework for approximating partial differential equation solutions using deep learning methods. In this paper, we propose a principled redesign of the PINNsformer, a Transformer-based PINN architecture. We present the Spectral PINNSformer (S-Pformer), a refinement of encoder-decoder PINNSformers that addresses two key issues; 1. the redundancy (i.e. increased parameter count) of the encoder, and 2. the mitigation of spectral bias. We find that the encoder is unnecessary for capturing spatiotemporal correlations when relying solely on self-attention, thereby reducing parameter count. Further, we integrate Fourier feature embeddings to explicitly mitigate spectral bias, enabling adaptive encoding of multiscale behaviors in the frequency domain. Our model outperforms encoder-decoder PINNSformer architectures across all benchmarks, achieving or outperforming MLP performance while reducing parameter count significantly.

</details>


### [4] [Computing frustration and near-monotonicity in deep neural networks](https://arxiv.org/abs/2510.05286)
*Joel Wendin, Erik G. Larsson, Claudio Altafini*

Main category: cs.LG

TL;DR: 本文研究了深度卷积神经网络的内部组织结构，通过计算网络中的“沮丧度”（frustration）并分析其与结构平衡的关系，发现这些网络表现出比随机模型更接近单调的行为。


<details>
  <summary>Details</summary>
Motivation: 尽管深度神经网络在图像分类等领域表现出色，但其成功背后的原因和性质尚不清楚。本文旨在探索训练好的深度神经网络的组织原则，以理解其泛化能力。

Method: 本文将统计物理中的“沮丧度”和结构平衡概念应用于深度神经网络的图表示，并结合函数单调性的概念。通过计算网络的沮丧度，评估其与结构平衡的接近程度，并分析这种接近程度与网络行为的关系。

Result: 研究发现，所有考虑的预训练深度卷积神经网络的沮丧度都低于随机模型。这表明网络中编码的无序程度小于随机模型，并且网络表现出接近单调的行为。

Conclusion: 本文结果表明，深度卷积神经网络倾向于表现出比随机模型更有序的行为，这可能是一种新的隐式正则化形式，有助于理解深度神经网络的泛化能力和内部组织结构。

Abstract: For the signed graph associated to a deep neural network, one can compute the frustration level, i.e., test how close or distant the graph is to structural balance. For all the pretrained deep convolutional neural networks we consider, we find that the frustration is always less than expected from null models. From a statistical physics point of view, and in particular in reference to an Ising spin glass model, the reduced frustration indicates that the amount of disorder encoded in the network is less than in the null models. From a functional point of view, low frustration (i.e., proximity to structural balance) means that the function representing the network behaves near-monotonically, i.e., more similarly to a monotone function than in the null models. Evidence of near-monotonic behavior along the partial order determined by frustration is observed for all networks we consider. This confirms that the class of deep convolutional neural networks tends to have a more ordered behavior than expected from null models, and suggests a novel form of implicit regularization.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [5] [Adaptive Pruning for Increased Robustness and Reduced Computational Overhead in Gaussian Process Accelerated Saddle Point Searches](https://arxiv.org/abs/2510.06030)
*Rohit Goswami, Hannes Jónsson*

Main category: physics.chem-ph

TL;DR: 本文提出了一种自适应剪枝策略，用于加速高斯过程（GP）加速的鞍点搜索，旨在降低计算成本并提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的GP加速鞍点搜索方法在超参数优化和模型更新时计算成本高昂，且容易因模型覆盖不足而失败。因此，需要一种更高效且稳定的方法。

Method: 该方法结合了感知几何的最优输运度量和主动剪枝策略，通过Wasserstein-1距离进行远点采样，选择固定大小的几何多样化配置子集。此外，还引入了置换不变度量、早停信任半径和信号方差对数势垒惩罚，以增强稳定性。

Result: 实验结果表明，该方法将238个具有挑战性的化学反应配置的平均计算时间减少了一半多。

Conclusion: 该研究表明，改进后的GP方法是一种强大且可扩展的算法，可用于加速需要大量计算才能评估能量和原子力的鞍点搜索，为计算化学领域提供了新的解决方案。

Abstract: Gaussian process (GP) regression provides a strategy for accelerating saddle point searches on high-dimensional energy surfaces by reducing the number of times the energy and its derivatives with respect to atomic coordinates need to be evaluated. The computational overhead in the hyperparameter optimization can, however, be large and make the approach inefficient. Failures can also occur if the search ventures too far into regions that are not represented well enough by the GP model. Here, these challenges are resolved by using geometry-aware optimal transport measures and an active pruning strategy using a summation over Wasserstein-1 distances for each atom-type in farthest-point sampling, selecting a fixed-size subset of geometrically diverse configurations to avoid rapidly increasing cost of GP updates as more observations are made. Stability is enhanced by permutation-invariant metric that provides a reliable trust radius for early-stopping and a logarithmic barrier penalty for the growth of the signal variance. These physically motivated algorithmic changes prove their efficacy by reducing to less than a half the mean computational time on a set of 238 challenging configurations from a previously published data set of chemical reactions. With these improvements, the GP approach is established as, a robust and scalable algorithm for accelerating saddle point searches when the evaluation of the energy and atomic forces requires significant computational effort.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [6] [Bilevel optimization for learning hyperparameters: Application to solving PDEs and inverse problems with Gaussian processes](https://arxiv.org/abs/2510.05568)
*Nicholas H. Nelsen, Houman Owhadi, Andrew M. Stuart, Xianjin Yang, Zongren Zou*

Main category: stat.ML

TL;DR: 本文提出了一种高效的双层优化方法，用于学习解决偏微分方程(PDE)和逆问题的超参数，通过高斯-牛顿线性化内层优化步骤，显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 科学计算和推断问题，如PDE和逆问题，对超参数的选择高度敏感。传统的随机超参数初始化方法准确性和鲁棒性不足，需要一种更有效的超参数优化策略。

Method: 该方法采用双层优化框架，外层优化超参数以最小化验证损失，内层优化模型以平衡数据保真度和平滑度。关键创新在于使用高斯-牛顿线性化内层优化，避免了重复求解PDE，从而实现了显式梯度计算和快速迭代。

Result: 通过高斯过程模型应用于非线性PDE和PDE逆问题，实验结果表明该方法在准确性和鲁棒性方面取得了显著改进，尤其在高维超参数优化中表现出良好的可扩展性。

Conclusion: 本文提出的方法为基于高斯过程的PDE求解和超参数学习提供了一种高效且可扩展的解决方案，具有广泛的应用前景。

Abstract: Methods for solving scientific computing and inference problems, such as kernel- and neural network-based approaches for partial differential equations (PDEs), inverse problems, and supervised learning tasks, depend crucially on the choice of hyperparameters. Specifically, the efficacy of such methods, and in particular their accuracy, stability, and generalization properties, strongly depends on the choice of hyperparameters. While bilevel optimization offers a principled framework for hyperparameter tuning, its nested optimization structure can be computationally demanding, especially in PDE-constrained contexts. In this paper, we propose an efficient strategy for hyperparameter optimization within the bilevel framework by employing a Gauss-Newton linearization of the inner optimization step. Our approach provides closed-form updates, eliminating the need for repeated costly PDE solves. As a result, each iteration of the outer loop reduces to a single linearized PDE solve, followed by explicit gradient-based hyperparameter updates. We demonstrate the effectiveness of the proposed method through Gaussian process models applied to nonlinear PDEs and to PDE inverse problems. Extensive numerical experiments highlight substantial improvements in accuracy and robustness compared to conventional random hyperparameter initialization. In particular, experiments with additive kernels and neural network-parameterized deep kernels demonstrate the method's scalability and effectiveness for high-dimensional hyperparameter optimization.

</details>
