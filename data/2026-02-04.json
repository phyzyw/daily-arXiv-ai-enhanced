{"id": "2602.02788", "title": "Structure-Preserving Learning Improves Geometry Generalization in Neural PDEs", "authors": ["Benjamin D. Shaffer", "Shawn Koohy", "Brooks Kinch", "M. Ani Hsieh", "Nathaniel Trask"], "summary": "We aim to develop physics foundation models for science and engineering that provide real-time solutions to Partial Differential Equations (PDEs) which preserve structure and accuracy under adaptation to unseen geometries. To this end, we introduce General-Geometry Neural Whitney Forms (Geo-NeW): a data-driven finite element method. We jointly learn a differential operator and compatible reduced finite element spaces defined on the underlying geometry. The resulting model is solved to generate predictions, while exactly preserving physical conservation laws through Finite Element Exterior Calculus. Geometry enters the model as a discretized mesh both through a transformer-based encoding and as the basis for the learned finite element spaces. This explicitly connects the underlying geometry and imposed boundary conditions to the solution, providing a powerful inductive bias for learning neural PDEs, which we demonstrate improves generalization to unseen domains. We provide a novel parameterization of the constitutive model ensuring the existence and uniqueness of the solution. Our approach demonstrates state-of-the-art performance on several steady-state PDE benchmarks, and provides a significant improvement over conventional baselines on out-of-distribution geometries.", "published": "2026-02-02", "categories": ["cs.LG", "cs.AI", "physics.comp-ph"], "pdf_url": "https://arxiv.org/pdf/2602.02788v1", "primary_category": "cs.LG"}
{"id": "2602.03546", "title": "How to Train Your Resistive Network: Generalized Equilibrium Propagation and Analytical Learning", "authors": ["Jonathan Lin", "Aman Desai", "Frank Barrows", "Francesco Caravelli"], "summary": "Machine learning is a powerful method of extracting meaning from data; unfortunately, current digital hardware is extremely energy-intensive. There is interest in an alternative analog computing implementation that could match the performance of traditional machine learning while being significantly more energy-efficient. However, it remains unclear how to train such analog computing systems while adhering to locality constraints imposed by the physical (as opposed to digital) nature of these systems. Local learning algorithms such as Equilibrium Propagation and Coupled Learning have been proposed to address this issue. In this paper, we develop an algorithm to exactly calculate gradients using a graph theoretic and analytical framework for Kirchhoff's laws. We also introduce Generalized Equilibrium Propagation, a framework encompassing a broad class of Hebbian learning algorithms, including Coupled Learning and Equilibrium Propagation, and show how our algorithm compares. We demonstrate our algorithm using numerical simulations and show that we can train resistor networks without the need for a replica or readout over all resistors, only at the output layer. We also show that under the analytical gradient approach, it is possible to update only a subset of the resistance values without a strong degradation in performance.", "published": "2026-02-03", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.mes-hall", "cond-mat.soft", "cs.ET"], "pdf_url": "https://arxiv.org/pdf/2602.03546v1", "primary_category": "cs.LG"}
{"id": "2602.03031", "title": "Physics-inspired transformer quantum states via latent imaginary-time evolution", "authors": ["Kimihiro Yamazaki", "Itsushi Sakata", "Takuya Konishi", "Yoshinobu Kawahara"], "summary": "Neural quantum states (NQS) are powerful ansätze in the variational Monte Carlo framework, yet their architectures are often treated as black boxes. We propose a physically transparent framework in which NQS are treated as neural approximations to latent imaginary-time evolution. This viewpoint suggests that standard Transformer-based NQS (TQS) architectures correspond to physically unmotivated effective Hamiltonians dependent on imaginary time in a latent space. Building on this interpretation, we introduce physics-inspired transformer quantum states (PITQS), which enforce a static effective Hamiltonian by sharing weights across layers and improve propagation accuracy via Trotter-Suzuki decompositions without increasing the number of variational parameters. For the frustrated $J_1$-$J_2$ Heisenberg model, our ansätze achieve accuracies comparable to or exceeding state-of-the-art TQS while using substantially fewer variational parameters. This study demonstrates that reinterpreting the deep network structure as a latent cooling process enables a more physically grounded, systematic, and compact design, thereby bridging the gap between black-box expressivity and physically transparent construction.", "published": "2026-02-03", "categories": ["cond-mat.dis-nn", "cs.LG", "quant-ph"], "pdf_url": "https://arxiv.org/pdf/2602.03031v1", "primary_category": "cond-mat.dis-nn"}
{"id": "2602.02855", "title": "When pre-training hurts LoRA fine-tuning: a dynamical analysis via single-index models", "authors": ["Gibbs Nwemadji", "Bruno Loureiro", "Jean Barbier"], "summary": "Pre-training on a source task is usually expected to facilitate fine-tuning on similar downstream problems. In this work, we mathematically show that this naive intuition is not always true: excessive pre-training can computationally slow down fine-tuning optimization. We study this phenomenon for low-rank adaptation (LoRA) fine-tuning on single-index models trained under one-pass SGD. Leveraging a summary statistics description of the fine-tuning dynamics, we precisely characterize how the convergence rate depends on the initial fine-tuning alignment and the degree of non-linearity of the target task. The key take away is that even when the pre-training and down- stream tasks are well aligned, strong pre-training can induce a prolonged search phase and hinder convergence. Our theory thus provides a unified picture of how pre-training strength and task difficulty jointly shape the dynamics and limitations of LoRA fine-tuning in a nontrivial tractable model.", "published": "2026-02-02", "categories": ["cs.LG", "cond-mat.dis-nn", "math.ST"], "pdf_url": "https://arxiv.org/pdf/2602.02855v1", "primary_category": "cs.LG"}
