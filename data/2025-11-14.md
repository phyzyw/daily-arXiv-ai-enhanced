<div id=toc></div>

# Table of Contents

- [physics.bio-ph](#physics.bio-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]


<div id='physics.bio-ph'></div>

# physics.bio-ph [[Back]](#toc)

### [1] [Completion of partial structures using Patterson maps with the CrysFormer machine learning model](https://arxiv.org/abs/2511.10440)
*Tom Pan, Evan Dramko, Mitchell D. Miller, Anastasios Kyrillidis, George N. Phillips*

Main category: physics.bio-ph

TL;DR: 本文提出了一种结合了传统结晶学和深度学习的混合方法，利用 Patterson 图谱和 AlphaFold 预测的“部分结构”模板来预测电子密度图，从而辅助蛋白质结构测定。


<details>
  <summary>Details</summary>
Motivation: 现有的基于深度学习的蛋白质结构预测模型通常不直接利用实验测量数据，如 X 射线衍射数据。因此，需要一种更紧密地耦合传统结晶学和深度学习的方法。

Method: 该方法训练了一个混合的 3D 视觉 Transformer 和卷积网络，该网络接受 Patterson 图谱和 AlphaFold 数据库中预测结构的“部分结构”模板作为输入，预测电子密度图，并通过标准的结晶学精修流程将其转化为原子模型。

Result: 实验表明，该方法能够有效改善结晶学结构因子的相位，完成“部分结构”模板中缺失的区域，并提高电子密度图与真实原子结构的吻合度。

Conclusion: 该研究表明，将深度学习与传统结晶学相结合，可以有效提高蛋白质结构测定的效率和准确性，为结构生物学领域提供了一种新的方法。

Abstract: Protein structure determination has long been one of the primary challenges of structural biology, to which deep machine learning (ML)-based approaches have increasingly been applied. However, these ML models generally do not incorporate the experimental measurements directly, such as X-ray crystallographic diffraction data. To this end, we explore an approach that more tightly couples these traditional crystallographic and recent ML-based methods, by training a hybrid 3-d vision transformer and convolutional network on inputs from both domains. We make use of two distinct input constructs / Patterson maps, which are directly obtainable from crystallographic data, and ``partial structure'' template maps derived from predicted structures deposited in the AlphaFold Protein Structure Database with subsequently omitted residues. With these, we predict electron density maps that are then post-processed into atomic models through standard crystallographic refinement processes. Introducing an initial dataset of small protein fragments taken from Protein Data Bank entries and placing them in hypothetical crystal settings, we demonstrate that our method is effective at both improving the phases of the crystallographic structure factors and completing the regions missing from partial structure templates, as well as improving the agreement of the electron density maps with the ground truth atomic structures.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [2] [HeatGen: A Guided Diffusion Framework for Multiphysics Heat Sink Design Optimization](https://arxiv.org/abs/2511.09578)
*Hadi Keramati, Morteza Sadeghi, Rajeev K. Jaiman*

Main category: cs.LG

TL;DR: 本文提出了一种基于引导降噪扩散概率模型（DDPM）的生成优化框架，用于设计散热器，旨在在保持表面温度阈值以下的同时，最小化压降。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法在散热器设计中存在可扩展性差、计算成本高以及难以快速适应新约束等问题，因此需要一种更高效、可扩展的优化方法。

Method: 该方法利用多保真度方法生成训练数据，并基于边界表示的多片散热器几何形状，训练一个降噪扩散概率模型。通过训练两个残差神经网络预测压降和表面温度，并利用这些模型的梯度引导几何生成过程，以满足压降和表面温度的约束。

Result: 实验结果表明，使用引导扩散模型生成的散热器压降比传统黑盒优化方法（如CMA-ES）降低了高达10%。

Conclusion: 该研究为构建电子散热的基础生成模型迈出了重要一步，提供了一种计算效率高、可快速适应新约束的散热器设计方法。

Abstract: This study presents a generative optimization framework based on a guided denoising diffusion probabilistic model (DDPM) that leverages surrogate gradients to generate heat sink designs minimizing pressure drop while maintaining surface temperatures below a specified threshold. Geometries are represented using boundary representations of multiple fins, and a multi-fidelity approach is employed to generate training data. Using this dataset, along with vectors representing the boundary representation geometries, we train a denoising diffusion probabilistic model to generate heat sinks with characteristics consistent with those observed in the data. We train two different residual neural networks to predict the pressure drop and surface temperature for each geometry. We use the gradients of these surrogate models with respect to the design variables to guide the geometry generation process toward satisfying the low-pressure and surface temperature constraints. This inference-time guidance directs the generative process toward heat sink designs that not only prevent overheating but also achieve lower pressure drops compared to traditional optimization methods such as CMA-ES. In contrast to traditional black-box optimization approaches, our method is scalable, provided sufficient training data is available. Unlike traditional topology optimization methods, once the model is trained and the heat sink world model is saved, inference under new constraints (e.g., temperature) is computationally inexpensive and does not require retraining. Samples generated using the guided diffusion model achieve pressure drops up to 10 percent lower than the limits obtained by traditional black-box optimization methods. This work represents a step toward building a foundational generative model for electronics cooling.

</details>


### [3] [Fractional neural attention for efficient multiscale sequence processing](https://arxiv.org/abs/2511.10208)
*Cheng Kevin Qu, Andrew Ly, Pulin Gong*

Main category: cs.LG

TL;DR: 本文提出了Fractional Neural Attention (FNA) 框架，它受到生物注意力机制和动力系统理论的启发，通过分数阶拉普拉斯控制的 Lévy 扩散模拟token交互，从而提升Transformer模型的表达能力和信息混合速度。


<details>
  <summary>Details</summary>
Motivation: 理解和扩展自注意力机制的原理是推进人工智能的关键挑战。本文旨在借鉴生物学多尺度注意力动态和动力系统理论，构建更强大的Transformer模型。

Method: FNA模型通过分数阶拉普拉斯控制的Lévy扩散模拟token交互，实现多尺度信息处理。利用几何谐波中的扩散图算法进行FNA权重降维，同时保留嵌入和隐藏状态的内在结构。

Result: FNA在文本分类、图像处理和神经机器翻译等任务中均表现出竞争力，即使在单层单头的情况下也能取得良好性能。理论分析表明，FNA的动态由分数阶扩散方程控制，且注意力网络具有更大的谱间隙和更短的路径长度。

Conclusion: FNA将自注意力、随机动力学和几何学联系起来，提供了一个可解释、生物学基础的强大、受神经科学启发的AI机制，为构建更高效的Transformer模型奠定了基础。

Abstract: Attention mechanisms underpin the computational power of Transformer models, which have achieved remarkable success across diverse domains. Yet understanding and extending the principles underlying self-attention remains a key challenge for advancing artificial intelligence. Drawing inspiration from the multiscale dynamics of biological attention and from dynamical systems theory, we introduce Fractional Neural Attention (FNA), a principled, neuroscience-inspired framework for multiscale information processing. FNA models token interactions through Lévy diffusion governed by the fractional Laplacian, intrinsically realizing simultaneous short- and long-range dependencies across multiple scales. This mechanism yields greater expressivity and faster information mixing, advancing the foundational capacity of Transformers. Theoretically, we show that FNA's dynamics are governed by the fractional diffusion equation, and that the resulting attention networks exhibit larger spectral gaps and shorter path lengths -- mechanistic signatures of enhanced computational efficiency. Empirically, FNA achieves competitive text-classification performance even with a single layer and a single head; it also improves performance in image processing and neural machine translation. Finally, the diffusion map algorithm from geometric harmonics enables dimensionality reduction of FNA weights while preserving the intrinsic structure of embeddings and hidden states. Together, these results establish FNA as a principled mechanism connecting self-attention, stochastic dynamics, and geometry, providing an interpretable, biologically grounded foundation for powerful, neuroscience-inspired AI.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [4] [VEDA: 3D Molecular Generation via Variance-Exploding Diffusion with Annealing](https://arxiv.org/abs/2511.09568)
*Peining Zhang, Jinbo Bi, Minghu Song*

Main category: physics.chem-ph

TL;DR: 本文提出了一种名为VEDA的新框架，它结合了变分扩散和退火策略，以高效地生成符合化学准确性的3D分子结构。


<details>
  <summary>Details</summary>
Motivation: 现有的3D分子生成模型在采样效率和构象准确性之间存在权衡：基于流的模型速度快但几何不准确，而去噪扩散模型准确但速度慢。本文旨在解决扩散模型采样速度慢的问题。

Method: VEDA框架结合了变分扩散（VE diffusion）和退火（annealing），并包含三个关键技术贡献：(1) VE时间表，模拟退火过程以提高3D准确性；(2) 新型预处理方案，协调SE(3)-等变网络与残差扩散目标；(3) 基于反正弦的新调度器，将采样集中在对数信噪比的关键区间。

Result: 在QM9和GEOM-DRUGS数据集上，VEDA实现了与基于流的模型相当的采样效率，仅用100步就达到了最先进的价键稳定性和有效性。生成的结构非常稳定，GFN2-xTB优化后的平均能量变化仅为1.72 kcal/mol，显著低于基线模型SemlaFlow的32.3 kcal/mol。

Conclusion: VEDA框架证明了将VE扩散与SE(3)-等变架构有原则地结合可以同时实现高化学准确性和计算效率，为3D分子生成提供了一种新的有效途径。

Abstract: Diffusion models show promise for 3D molecular generation, but face a fundamental trade-off between sampling efficiency and conformational accuracy. While flow-based models are fast, they often produce geometrically inaccurate structures, as they have difficulty capturing the multimodal distributions of molecular conformations. In contrast, denoising diffusion models are more accurate but suffer from slow sampling, a limitation attributed to sub-optimal integration between diffusion dynamics and SE(3)-equivariant architectures. To address this, we propose VEDA, a unified SE(3)-equivariant framework that combines variance-exploding diffusion with annealing to efficiently generate conformationally accurate 3D molecular structures. Specifically, our key technical contributions include: (1) a VE schedule that enables noise injection functionally analogous to simulated annealing, improving 3D accuracy and reducing relaxation energy; (2) a novel preconditioning scheme that reconciles the coordinate-predicting nature of SE(3)-equivariant networks with a residual-based diffusion objective, and (3) a new arcsin-based scheduler that concentrates sampling in critical intervals of the logarithmic signal-to-noise ratio. On the QM9 and GEOM-DRUGS datasets, VEDA matches the sampling efficiency of flow-based models, achieving state-of-the-art valency stability and validity with only 100 sampling steps. More importantly, VEDA's generated structures are remarkably stable, as measured by their relaxation energy during GFN2-xTB optimization. The median energy change is only 1.72 kcal/mol, significantly lower than the 32.3 kcal/mol from its architectural baseline, SemlaFlow. Our framework demonstrates that principled integration of VE diffusion with SE(3)-equivariant architectures can achieve both high chemical accuracy and computational efficiency.

</details>


### [5] [Solvaformer: an SE(3)-equivariant graph transformer for small molecule solubility prediction](https://arxiv.org/abs/2511.09774)
*Jonathan Broadbent, Michael Bailey, Mingxuan Li, Abhishek Paul, Louis De Lescure, Paul Chauvin, Lorenzo Kogler-Anele, Yasser Jangjou, Sven Jager*

Main category: physics.chem-ph

TL;DR: 本文提出了Solvaformer，一种基于图transformer的几何感知模型，用于准确预测小分子溶解度，并具有良好的可解释性。


<details>
  <summary>Details</summary>
Motivation: 实验测定溶解度成本高昂，现有机器学习方法要么依赖量子描述符，要么缺乏可解释性。因此，需要一种准确、可扩展且可解释的溶解度预测方法。

Method: Solvaformer采用几何感知图transformer架构，结合分子内SE(3)-等变注意力机制和分子间标量注意力机制，实现分子间的通信。模型在CombiSolv-QM（量子力学数据）和BigSolDB 2.0（实验数据）混合数据集上进行多任务训练，预测溶解度（log S）和溶剂化自由能。

Result: Solvaformer在溶解度预测方面表现出最强的整体性能，接近DFT-辅助梯度提升基线，并优于EquiformerV2消融实验和序列模型。此外，token级别的注意力机制能够产生化学上合理的归因，能够恢复控制位置异构体溶解度差异的分子内和分子间氢键模式。

Conclusion: Solvaformer通过结合几何归纳偏差和混合数据集训练策略，为溶液相性质预测提供了一种准确、可扩展且可解释的方法，整合了计算和实验数据的优势。

Abstract: Accurate prediction of small molecule solubility using material-sparing approaches is critical for accelerating synthesis and process optimization, yet experimental measurement is costly and many learning approaches either depend on quantumderived descriptors or offer limited interpretability. We introduce Solvaformer, a geometry-aware graph transformer that models solutions as multiple molecules with independent SE(3) symmetries. The architecture combines intramolecular SE(3)-equivariant attention with intermolecular scalar attention, enabling cross-molecular communication without imposing spurious relative geometry. We train Solvaformer in a multi-task setting to predict both solubility (log S) and solvation free energy, using an alternating-batch regimen that trains on quantum-mechanical data (CombiSolv-QM) and on experimental measurements (BigSolDB 2.0). Solvaformer attains the strongest overall performance among the learned models and approaches a DFT-assisted gradient-boosting baseline, while outperforming an EquiformerV2 ablation and sequence-based alternatives. In addition, token-level attention produces chemically coherent attributions: case studies recover known intra- vs. inter-molecular hydrogen-bonding patterns that govern solubility differences in positional isomers. Taken together, Solvaformer provides an accurate, scalable, and interpretable approach to solution-phase property prediction by uniting geometric inductive bias with a mixed dataset training strategy on complementary computational and experimental data.

</details>
