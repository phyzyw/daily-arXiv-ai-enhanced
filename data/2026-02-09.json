{"id": "2602.06791", "title": "Rare Event Analysis of Large Language Models", "authors": ["Jake McAllister Dorman", "Edward Gillman", "Dominic C. Rose", "Jamie F. Mair", "Juan P. Garrahan"], "summary": "Being probabilistic models, during inference large language models (LLMs) display rare events: behaviour that is far from typical but highly significant. By definition all rare events are hard to see, but the enormous scale of LLM usage means that events completely unobserved during development are likely to become prominent in deployment. Here we present an end-to-end framework for the systematic analysis of rare events in LLMs. We provide a practical implementation spanning theory, efficient generation strategies, probability estimation and error analysis, which we illustrate with concrete examples. We outline extensions and applications to other models and contexts, highlighting the generality of the concepts and techniques presented here.", "published": "2026-02-06", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.stat-mech"], "pdf_url": "https://arxiv.org/pdf/2602.06791v1", "primary_category": "cs.LG"}
{"id": "2602.06418", "title": "Adaptive Protein Tokenization", "authors": ["Rohit Dilip", "Ayush Varshney", "David Van Valen"], "summary": "Tokenization is a promising path to multi-modal models capable of jointly understanding protein sequences, structure, and function. Existing protein structure tokenizers create tokens by pooling information from local neighborhoods, an approach that limits their performance on generative and representation tasks. In this work, we present a method for global tokenization of protein structures in which successive tokens contribute increasing levels of detail to a global representation. This change resolves several issues with generative models based on local protein tokenization: it mitigates error accumulation, provides embeddings without sequence-reduction operations, and allows task-specific adaptation of a tokenized sequence's information content. We validate our method on reconstruction, generative, and representation tasks and demonstrate that it matches or outperforms existing models based on local protein structure tokenizers. We show how adaptive tokens enable inference criteria based on information content, which boosts designability. We validate representations generated from our tokenizer on CATH classification tasks and demonstrate that non-linear probing on our tokenized sequences outperforms equivalent probing on representations from other tokenizers. Finally, we demonstrate how our method supports zero-shot protein shrinking and affinity maturation.", "published": "2026-02-06", "categories": ["cs.LG", "q-bio.BM"], "pdf_url": "https://arxiv.org/pdf/2602.06418v1", "primary_category": "cs.LG"}
{"id": "2602.06320", "title": "High-Dimensional Limit of Stochastic Gradient Flow via Dynamical Mean-Field Theory", "authors": ["Sota Nishiyama", "Masaaki Imaizumi"], "summary": "Modern machine learning models are typically trained via multi-pass stochastic gradient descent (SGD) with small batch sizes, and understanding their dynamics in high dimensions is of great interest. However, an analytical framework for describing the high-dimensional asymptotic behavior of multi-pass SGD with small batch sizes for nonlinear models is currently missing. In this study, we address this gap by analyzing the high-dimensional dynamics of a stochastic differential equation called a \\emph{stochastic gradient flow} (SGF), which approximates multi-pass SGD in this regime. In the limit where the number of data samples $n$ and the dimension $d$ grow proportionally, we derive a closed system of low-dimensional and continuous-time equations and prove that it characterizes the asymptotic distribution of the SGF parameters. Our theory is based on the dynamical mean-field theory (DMFT) and is applicable to a wide range of models encompassing generalized linear models and two-layer neural networks. We further show that the resulting DMFT equations recover several existing high-dimensional descriptions of SGD dynamics as special cases, thereby providing a unifying perspective on prior frameworks such as online SGD and high-dimensional linear regression. Our proof builds on the existing DMFT technique for gradient flow and extends it to handle the stochasticity in SGF using tools from stochastic calculus.", "published": "2026-02-06", "categories": ["stat.ML", "cond-mat.dis-nn", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2602.06320v1", "primary_category": "stat.ML"}
