{"id": "2511.19347", "title": "Artificial Intelligence Driven Workflow for Accelerating Design of Novel Photosensitizers", "authors": ["Hongyi Wang", "Xiuli Zheng", "Weimin Liu", "Zitian Tang", "Sheng Gong"], "summary": "The discovery of high-performance photosensitizers has long been hindered by the time-consuming and resource-intensive nature of traditional trial-and-error approaches. Here, we present \\textbf{A}I-\\textbf{A}ccelerated \\textbf{P}hoto\\textbf{S}ensitizer \\textbf{I}nnovation (AAPSI), a closed-loop workflow that integrates expert knowledge, scaffold-based molecule generation, and Bayesian optimization to accelerate the design of novel photosensitizers. The scaffold-driven generation in AAPSI ensures structural novelty and synthetic feasibility, while the iterative AI-experiment loop accelerates the discovery of novel photosensitizers. AAPSI leverages a curated database of 102,534 photosensitizer-solvent pairs and generate 6,148 synthetically accessible candidates. These candidates are screened via graph transformers trained to predict singlet oxygen quantum yield ($φ_Δ$) and absorption maxima ($λ_{max}$), following experimental validation. This work generates several novel candidates for photodynamic therapy (PDT), among which the hypocrellin-based candidate HB4Ph exhibits exceptional performance at the Pareto frontier of high quantum yield of singlet oxygen and long absorption maxima among current photosensitizers ($φ_Δ$=0.85, $λ_{max}$=650nm).", "published": "2025-11-24", "categories": ["cond-mat.mtrl-sci", "cs.LG", "physics.chem-ph"], "pdf_url": "https://arxiv.org/pdf/2511.19347v1", "primary_category": "cond-mat.mtrl-sci"}
{"id": "2511.17753", "title": "$Δ$-ML Ensembles for Selecting Quantum Chemistry Methods to Compute Intermolecular Interactions", "authors": ["Austin M. Wallace", "C. David Sherrill", "Giri P. Krishnan"], "summary": "Ab initio quantum chemical methods for accurately computing interactions between molecules have a wide range of applications but are often computationally expensive. Hence, selecting an appropriate method based on accuracy and computational cost remains a significant challenge due to varying performance of methods. In this work, we propose a framework based on an ensemble of $Δ$-ML models trained on features extracted from a pre-trained atom-pairwise neural network to predict the error of each method relative to all other methods including the ``gold standard'' coupled cluster with single, double, and perturbative triple excitations at the estimated complete basis set limit [CCSD(T)/CBS]. Our proposed approach provides error estimates across various levels of theories and identifies the computationally efficient approach for a given error range utilizing only a subset of the dataset. Further, this approach allows comparison between various theories. We demonstrate the effectiveness of our approach using an extended BioFragment dataset, which includes the interaction energies for common biomolecular fragments and small organic dimers. Our results show that the proposed framework achieves very small mean-absolute-errors below 0.1 kcal/mol regardless of the given method. Furthermore, by analyzing all-to-all $Δ$-ML models for present levels of theory, we identify method groupings that align with theoretical hypotheses, providing evidence that $Δ$-ML models can easily learn corrections from any level of theory to any other level of theory.", "published": "2025-11-21", "categories": ["physics.chem-ph", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2511.17753v1", "primary_category": "physics.chem-ph"}
{"id": "2511.19264", "title": "Interpreting GFlowNets for Drug Discovery: Extracting Actionable Insights for Medicinal Chemistry", "authors": ["Amirtha Varshini A S", "Duminda S. Ranasinghe", "Hok Hei Tam"], "summary": "Generative Flow Networks, or GFlowNets, offer a promising framework for molecular design, but their internal decision policies remain opaque. This limits adoption in drug discovery, where chemists require clear and interpretable rationales for proposed structures. We present an interpretability framework for SynFlowNet, a GFlowNet trained on documented chemical reactions and purchasable starting materials that generates both molecules and the synthetic routes that produce them. Our approach integrates three complementary components. Gradient based saliency combined with counterfactual perturbations identifies which atomic environments influence reward and how structural edits change molecular outcomes. Sparse autoencoders reveal axis aligned latent factors that correspond to physicochemical properties such as polarity, lipophilicity, and molecular size. Motif probes show that functional groups including aromatic rings and halogens are explicitly encoded and linearly decodable from the internal embeddings. Together, these results expose the chemical logic inside SynFlowNet and provide actionable and mechanistic insight that supports transparent and controllable molecular design.", "published": "2025-11-24", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "pdf_url": "https://arxiv.org/pdf/2511.19264v1", "primary_category": "cs.LG"}
{"id": "2511.19184", "title": "Torsion-Space Diffusion for Protein Backbone Generation with Geometric Refinement", "authors": ["Lakshaditya Singh", "Adwait Shelke", "Divyansh Agrawal"], "summary": "Designing new protein structures is fundamental to computational biology, enabling advances in therapeutic molecule discovery and enzyme engineering. Existing diffusion-based generative models typically operate in Cartesian coordinate space, where adding noise disrupts strict geometric constraints such as fixed bond lengths and angles, often producing physically invalid structures. To address this limitation, we propose a Torsion-Space Diffusion Model that generates protein backbones by denoising torsion angles, ensuring perfect local geometry by construction. A differentiable forward-kinematics module reconstructs 3D coordinates with fixed 3.8 Angstrom backbone bond lengths while a constrained post-processing refinement optimizes global compactness via Radius of Gyration (Rg) correction, without violating bond constraints. Experiments on standard PDB proteins demonstrate 100% bond-length accuracy and significantly improved structural compactness, reducing Rg error from 70% to 18.6% compared to Cartesian diffusion baselines. Overall, this hybrid torsion-diffusion plus geometric-refinement framework generates physically valid and compact protein backbones, providing a promising path toward full-atom protein generation.", "published": "2025-11-24", "categories": ["q-bio.BM", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2511.19184v1", "primary_category": "q-bio.BM"}
{"id": "2511.17825", "title": "Analog Physical Systems Can Exhibit Double Descent", "authors": ["Sam Dillavou", "Jason W Rocks", "Jacob F Wycoff", "Andrea J Liu", "Douglas J Durian"], "summary": "An important component of the success of large AI models is double descent, in which networks avoid overfitting as they grow relative to the amount of training data, instead improving their performance on unseen data. Here we demonstrate double descent in a decentralized analog network of self-adjusting resistive elements. This system trains itself and performs tasks without a digital processor, offering potential gains in energy efficiency and speed -- but must endure component non-idealities. We find that standard training fails to yield double descent, but a modified protocol that accommodates this inherent imperfection succeeds. Our findings show that analog physical systems, if appropriately trained, can exhibit behaviors underlying the success of digital AI. Further, they suggest that biological systems might similarly benefit from over-parameterization.", "published": "2025-11-21", "categories": ["cond-mat.dis-nn", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2511.17825v1", "primary_category": "cond-mat.dis-nn"}
{"id": "2511.17789", "title": "Physical Reinforcement Learning", "authors": ["Sam Dillavou", "Shruti Mishra"], "summary": "Digital computers are power-hungry and largely intolerant of damaged components, making them potentially difficult tools for energy-limited autonomous agents in uncertain environments. Recently developed Contrastive Local Learning Networks (CLLNs) - analog networks of self-adjusting nonlinear resistors - are inherently low-power and robust to physical damage, but were constructed to perform supervised learning. In this work we demonstrate success on two simple RL problems using Q-learning adapted for simulated CLLNs. Doing so makes explicit the components (beyond the network being trained) required to enact various tools in the RL toolbox, some of which (policy function and value function) are more natural in this system than others (replay buffer). We discuss assumptions such as the physical safety that digital hardware requires, CLLNs can forgo, and biological systems cannot rely on, and highlight secondary goals that are important in biology and trainable in CLLNs, but make little sense in digital computers.", "published": "2025-11-21", "categories": ["cs.LG", "cond-mat.dis-nn"], "pdf_url": "https://arxiv.org/pdf/2511.17789v1", "primary_category": "cs.LG"}
