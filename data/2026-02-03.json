{"id": "2602.02281", "title": "Backpropagation as Physical Relaxation: Exact Gradients in Finite Time", "authors": ["Antonino Emanuele Scurria"], "summary": "Backpropagation, the foundational algorithm for training neural networks, is typically understood as a symbolic computation that recursively applies the chain rule. We show it emerges exactly as the finite-time relaxation of a physical dynamical system. By formulating feedforward inference as a continuous-time process and applying Lagrangian theory of non-conservative systems to handle asymmetric interactions, we derive a global energy functional on a doubled state space encoding both activations and sensitivities. The saddle-point dynamics of this energy perform inference and credit assignment simultaneously through local interactions. We term this framework ''Dyadic Backpropagation''. Crucially, we prove that unit-step Euler discretization, the natural timescale of layer transitions, recovers standard backpropagation exactly in precisely 2L steps for an L-layer network, with no approximations. Unlike prior energy-based methods requiring symmetric weights, asymptotic convergence, or vanishing perturbations, our framework guarantees exact gradients in finite time. This establishes backpropagation as the digitally optimized shadow of a continuous physical relaxation, providing a rigorous foundation for exact gradient computation in analog and neuromorphic substrates where continuous dynamics are native.", "published": "2026-02-02", "categories": ["cs.LG", "cs.AI", "cs.NE", "physics.class-ph", "physics.comp-ph"], "pdf_url": "https://arxiv.org/pdf/2602.02281v1", "primary_category": "cs.LG"}
{"id": "2602.01941", "title": "FluxNet: Learning Capacity-Constrained Local Transport Operators for Conservative and Bounded PDE Surrogates", "authors": ["Zishuo Lan", "Junjie Li", "Lei Wang", "Jincheng Wang"], "summary": "Autoregressive learning of time-stepping operators offers an effective approach to data-driven PDE simulation on grids. For conservation laws, however, long-horizon rollouts are often destabilized when learned updates violate global conservation and, in many applications, additional state bounds such as nonnegative mass and densities or concentrations constrained to [0,1]. Enforcing these coupled constraints via direct next-state regression remains difficult. We introduce a framework for learning conservative transport operators on regular grids, inspired by lattice Boltzmann-style discrete-velocity transport representations. Instead of predicting the next state, the model outputs local transport operators that update cells through neighborhood exchanges, guaranteeing discrete conservation by construction. For bounded quantities, we parameterize transport within a capacity-constrained feasible set, enforcing bounds structurally rather than by post-hoc clipping. We validate FluxNet on 1D convection-diffusion, 2D shallow water equations, 1D traffic flow, and 2D spinodal decomposition. Experiments on shallow-water equations and traffic flow show improved rollout stability and physical consistency over strong baselines. On phase-field spinodal decomposition, the method enables large time-steps with long-range transport, accelerating simulation while preserving microstructure evolution in both pointwise and statistical measures.", "published": "2026-02-02", "categories": ["cond-mat.mtrl-sci", "cs.CE", "cs.LG", "physics.comp-ph"], "pdf_url": "https://arxiv.org/pdf/2602.01941v1", "primary_category": "cond-mat.mtrl-sci"}
{"id": "2602.01176", "title": "Multi-Fidelity Physics-Informed Neural Networks with Bayesian Uncertainty Quantification and Adaptive Residual Learning for Efficient Solution of Parametric Partial Differential Equations", "authors": ["Olaf Yunus Laitinen Imanov"], "summary": "Physics-informed neural networks (PINNs) have emerged as a powerful paradigm for solving partial differential equations (PDEs) by embedding physical laws directly into neural network training. However, solving high-fidelity PDEs remains computationally prohibitive, particularly for parametric systems requiring multiple evaluations across varying parameter configurations. This paper presents MF-BPINN, a novel multi-fidelity framework that synergistically combines physics-informed neural networks with Bayesian uncertainty quantification and adaptive residual learning. Our approach leverages abundant low-fidelity simulations alongside sparse high-fidelity data through a hierarchical neural architecture that learns nonlinear correlations across fidelity levels. We introduce an adaptive residual network with learnable gating mechanisms that dynamically balances linear and nonlinear fidelity discrepancies. Furthermore, we develop a rigorous Bayesian framework employing Hamiltonian Monte Carlo.", "published": "2026-02-01", "categories": ["cs.LG", "math.NA", "physics.comp-ph"], "pdf_url": "https://arxiv.org/pdf/2602.01176v1", "primary_category": "cs.LG"}
{"id": "2602.00378", "title": "Parametrization of subgrid scales in long-term simulations of the shallow-water equations using machine learning and convex limiting", "authors": ["Md Amran Hossan Mojamder", "Zhihang Xu", "Min Wang", "Ilya Timofeyev"], "summary": "We present a method for parametrizing sub-grid processes in the Shallow Water equations. We define coarse variables and local spatial averages and use a feed-forward neural network to learn sub-grid fluxes. Our method results in a local parametrization that uses a four-point computational stencil, which has several advantages over globally coupled parametrizations. We demonstrate numerically that our method improves energy balance in long-term turbulent simulations and also accurately reproduces individual solutions. The neural network parametrization can be easily combined with flux limiting to reduce oscillations near shocks. More importantly, our method provides reliable parametrizations, even in dynamical regimes that are not included in the training data.", "published": "2026-01-30", "categories": ["physics.flu-dyn", "cs.LG", "physics.ao-ph", "physics.comp-ph"], "pdf_url": "https://arxiv.org/pdf/2602.00378v1", "primary_category": "physics.flu-dyn"}
{"id": "2602.02310", "title": "FragmentFlow: Scalable Transition State Generation for Large Molecules", "authors": ["Ron Shprints", "Peter Holderrieth", "Juno Nam", "Rafael Gómez-Bombarelli", "Tommi Jaakkola"], "summary": "Transition states (TSs) are central to understanding and quantitatively predicting chemical reactivity and reaction mechanisms. Although traditional TS generation methods are computationally expensive, recent generative modeling approaches have enabled chemically meaningful TS prediction for relatively small molecules. However, these methods fail to generalize to practically relevant reaction substrates because of distribution shifts induced by increasing molecular sizes. Furthermore, TS geometries for larger molecules are not available at scale, making it infeasible to train generative models from scratch on such molecules. To address these challenges, we introduce FragmentFlow: a divide-and-conquer approach that trains a generative model to predict TS geometries for the reactive core atoms, which define the reaction mechanism. The full TS structure is then reconstructed by re-attaching substituent fragments to the predicted core. By operating on reactive cores, whose size and composition remain relatively invariant across molecular contexts, FragmentFlow mitigates distribution shifts in generative modeling. Evaluated on a new curated dataset of reactions involving reactants with up to 33 heavy atoms, FragmentFlow correctly identifies 90% of TSs while requiring 30% fewer saddle-point optimization steps than classical initialization schemes. These results point toward scalable TS generation for high-throughput reactivity studies.", "published": "2026-02-02", "categories": ["physics.chem-ph", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2602.02310v1", "primary_category": "physics.chem-ph"}
{"id": "2602.02128", "title": "Scalable Spatio-Temporal SE(3) Diffusion for Long-Horizon Protein Dynamics", "authors": ["Nima Shoghi", "Yuxuan Liu", "Yuning Shen", "Rob Brekelmans", "Pan Li", "Quanquan Gu"], "summary": "Molecular dynamics (MD) simulations remain the gold standard for studying protein dynamics, but their computational cost limits access to biologically relevant timescales. Recent generative models have shown promise in accelerating simulations, yet they struggle with long-horizon generation due to architectural constraints, error accumulation, and inadequate modeling of spatio-temporal dynamics. We present STAR-MD (Spatio-Temporal Autoregressive Rollout for Molecular Dynamics), a scalable SE(3)-equivariant diffusion model that generates physically plausible protein trajectories over microsecond timescales. Our key innovation is a causal diffusion transformer with joint spatio-temporal attention that efficiently captures complex space-time dependencies while avoiding the memory bottlenecks of existing methods. On the standard ATLAS benchmark, STAR-MD achieves state-of-the-art performance across all metrics--substantially improving conformational coverage, structural validity, and dynamic fidelity compared to previous methods. STAR-MD successfully extrapolates to generate stable microsecond-scale trajectories where baseline methods fail catastrophically, maintaining high structural quality throughout the extended rollout. Our comprehensive evaluation reveals severe limitations in current models for long-horizon generation, while demonstrating that STAR-MD's joint spatio-temporal modeling enables robust dynamics simulation at biologically relevant timescales, paving the way for accelerated exploration of protein function.", "published": "2026-02-02", "categories": ["cs.LG", "cs.AI", "physics.bio-ph", "q-bio.BM", "q-bio.QM"], "pdf_url": "https://arxiv.org/pdf/2602.02128v1", "primary_category": "cs.LG"}
{"id": "2602.00663", "title": "SEISMO: Increasing Sample Efficiency in Molecular Optimization with a Trajectory-Aware LLM Agent", "authors": ["Fabian P. Krüger", "Andrea Hunklinger", "Adrian Wolny", "Tim J. Adler", "Igor Tetko", "Santiago David Villalba"], "summary": "Optimizing the structure of molecules to achieve desired properties is a central bottleneck across the chemical sciences, particularly in the pharmaceutical industry where it underlies the discovery of new drugs. Since molecular property evaluation often relies on costly and rate-limited oracles, such as experimental assays, molecular optimization must be highly sample-efficient. To address this, we introduce SEISMO, an LLM agent that performs strictly online, inference-time molecular optimization, updating after every oracle call without the need for population-based or batched learning. SEISMO conditions each proposal on the full optimization trajectory, combining natural-language task descriptions with scalar scores and, when available, structured explanatory feedback. Across the Practical Molecular Optimization benchmark of 23 tasks, SEISMO achieves a 2-3 times higher area under the optimisation curve than prior methods, often reaching near-maximal task scores within 50 oracle calls. Our additional medicinal-chemistry tasks show that providing explanatory feedback further improves efficiency, demonstrating that leveraging domain knowledge and structured information is key to sample-efficient molecular optimization.", "published": "2026-01-31", "categories": ["cs.AI", "cs.LG", "q-bio.BM"], "pdf_url": "https://arxiv.org/pdf/2602.00663v1", "primary_category": "cs.AI"}
{"id": "2602.02320", "title": "A Large-Scale Dataset for Molecular Structure-Language Description via a Rule-Regularized Method", "authors": ["Feiyang Cai", "Guijuan He", "Yi Hu", "Jingjing Wang", "Joshua Luo", "Tianyu Zhu", "Srikanth Pilla", "Gang Li", "Ling Liu", "Feng Luo"], "summary": "Molecular function is largely determined by structure. Accurately aligning molecular structure with natural language is therefore essential for enabling large language models (LLMs) to reason about downstream chemical tasks. However, the substantial cost of human annotation makes it infeasible to construct large-scale, high-quality datasets of structure-grounded descriptions. In this work, we propose a fully automated annotation framework for generating precise molecular structure descriptions at scale. Our approach builds upon and extends a rule-based chemical nomenclature parser to interpret IUPAC names and construct enriched, structured XML metadata that explicitly encodes molecular structure. This metadata is then used to guide LLMs in producing accurate natural-language descriptions. Using this framework, we curate a large-scale dataset of approximately $163$k molecule-description pairs. A rigorous validation protocol combining LLM-based and expert human evaluation on a subset of $2,000$ molecules demonstrates a high description precision of $98.6\\%$. The resulting dataset provides a reliable foundation for future molecule-language alignment, and the proposed annotation method is readily extensible to larger datasets and broader chemical tasks that rely on structural descriptions.", "published": "2026-02-02", "categories": ["cs.CL", "cs.AI", "q-bio.BM"], "pdf_url": "https://arxiv.org/pdf/2602.02320v1", "primary_category": "cs.CL"}
{"id": "2602.00782", "title": "Controlling Repetition in Protein Language Models", "authors": ["Jiahao Zhang", "Zeqing Zhang", "Di Wang", "Lijie Hu"], "summary": "Protein language models (PLMs) have enabled advances in structure prediction and de novo protein design, yet they frequently collapse into pathological repetition during generation. Unlike in text, where repetition merely reduces readability, in proteins it undermines structural confidence and functional viability. To unify this problem, we present the first systematic study of repetition in PLMs. We first propose quantitative metrics to characterize motif-level and homopolymer repetition and then demonstrate their negative impact on folding reliability. To address this challenge, we propose UCCS (Utility-Controlled Contrastive Steering), which steers protein generation with a constrained dataset. Instead of naively contrasting high- vs. low-repetition sequences, we construct contrastive sets that maximize differences in repetition while tightly controlling for structural utility. This disentanglement yields steering vectors that specifically target repetition without degrading foldability. Injected at inference, these vectors consistently reduce repetition without retraining or heuristic decoding. Experiments with ESM-3 and ProtGPT2 in CATH, UniRef50, and SCOP show that our method outperforms decoding penalties and other baselines, substantially lowering repetition while preserving AlphaFold confidence scores. Our results establish repetition control as a central challenge for PLMs and highlight dataset-guided steering as a principled approach for reliable protein generation.", "published": "2026-01-31", "categories": ["q-bio.BM", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2602.00782v1", "primary_category": "q-bio.BM"}
{"id": "2602.00716", "title": "Emergence of Distortions in High-Dimensional Guided Diffusion Models", "authors": ["Enrico Ventura", "Beatrice Achilli", "Luca Ambrogioni", "Carlo Lucibello"], "summary": "Classifier-free guidance (CFG) is the de facto standard for conditional sampling in diffusion models, yet it often leads to a loss of diversity in generated samples. We formalize this phenomenon as generative distortion, defined as the mismatch between the CFG-induced sampling distribution and the true conditional distribution. Considering Gaussian mixtures and their exact scores, and leveraging tools from statistical physics, we characterize the onset of distortion in a high-dimensional regime as a function of the number of classes. Our analysis reveals that distortions emerge through a phase transition in the effective potential governing the guided dynamics. In particular, our dynamical mean-field analysis shows that distortion persists when the number of modes grows exponentially with dimension, but vanishes in the sub-exponential regime. Consistent with prior finite-dimensional results, we further demonstrate that vanilla CFG shifts the mean and shrinks the variance of the conditional distribution. We show that standard CFG schedules are fundamentally incapable of preventing variance shrinkage. Finally, we propose a theoretically motivated guidance schedule featuring a negative-guidance window, which mitigates loss of diversity while preserving class separability.", "published": "2026-01-31", "categories": ["stat.ML", "cond-mat.dis-nn", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2602.00716v1", "primary_category": "stat.ML"}
{"id": "2602.00302", "title": "Neural Ising Machines via Unrolling and Zeroth-Order Training", "authors": ["Sam Reifenstein", "Timothee Leleu"], "summary": "We propose a data-driven heuristic for NP-hard Ising and Max-Cut optimization that learns the update rule of an iterative dynamical system. The method learns a shared, node-wise update rule that maps local interaction fields to spin updates, parameterized by a compact multilayer perceptron with a small number of parameters. Training is performed using a zeroth-order optimizer, since backpropagation through long, recurrent Ising-machine dynamics leads to unstable and poorly informative gradients. We call this approach a neural network parameterized Ising machine (NPIM). Despite its low parameter count, the learned dynamics recover effective algorithmic structure, including momentum-like behavior and time-varying schedules, enabling efficient search in highly non-convex energy landscapes. Across standard Ising and neural combinatorial optimization benchmarks, NPIM achieves competitive solution quality and time-to-solution relative to recent learning-based methods and strong classical Ising-machine heuristics.", "published": "2026-01-30", "categories": ["cs.LG", "cond-mat.dis-nn", "nlin.CD"], "pdf_url": "https://arxiv.org/pdf/2602.00302v1", "primary_category": "cs.LG"}
