<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Learning Pseudorandom Numbers with Transformers: Permuted Congruential Generators, Curricula, and Interpretability](https://arxiv.org/abs/2510.26792)
*Tao Tao, Maissam Barkeshli*

Main category: cs.LG

TL;DR: 本文研究了Transformer模型学习置换线性同余生成器(PCG)序列的能力，发现Transformer能够成功预测PCG序列，并揭示了其学习过程中的规模效应和表示学习机制。


<details>
  <summary>Details</summary>
Motivation: 为了探究Transformer模型能够学习哪些模式，如何高效训练以及其泛化能力背后的机制，本文选择伪随机数生成器(PRNG)作为受控的基准，并特别关注广泛使用的PCG，以评估其在密码学安全方面的潜在风险。

Method: 研究人员使用Transformer模型对不同变体的PCG序列进行上下文预测，并扩展了模数范围到2^22，同时探索了单比特截断情况下的预测能力。他们还研究了在训练时同时呈现多个PRNG的情况，并分析了嵌入层以揭示表示学习的规律。

Result: Transformer模型能够成功预测PCG序列，即使在单比特截断的情况下也能保持较高的准确率。预测所需的上下文序列数量与模数m的平方根成比例。当模数较大时，优化过程会进入停滞阶段，需要使用较小模数的训练数据进行课程学习。研究还发现模型能够自发地将整数输入聚类成旋转不变的簇。

Conclusion: 本文证明了Transformer模型在学习复杂序列模式方面的能力，揭示了其规模效应和表示学习机制，为理解AI系统对密码学原语的潜在攻击能力提供了新的视角，并强调了课程学习在训练大型模型中的重要性。

Abstract: We study the ability of Transformer models to learn sequences generated by Permuted Congruential Generators (PCGs), a widely used family of pseudo-random number generators (PRNGs). PCGs introduce substantial additional difficulty over linear congruential generators (LCGs) by applying a series of bit-wise shifts, XORs, rotations and truncations to the hidden state. We show that Transformers can nevertheless successfully perform in-context prediction on unseen sequences from diverse PCG variants, in tasks that are beyond published classical attacks. In our experiments we scale moduli up to $2^{22}$ using up to $50$ million model parameters and datasets with up to $5$ billion tokens. Surprisingly, we find even when the output is truncated to a single bit, it can be reliably predicted by the model. When multiple distinct PRNGs are presented together during training, the model can jointly learn them, identifying structures from different permutations. We demonstrate a scaling law with modulus $m$: the number of in-context sequence elements required for near-perfect prediction grows as $\sqrt{m}$. For larger moduli, optimization enters extended stagnation phases; in our experiments, learning moduli $m \geq 2^{20}$ requires incorporating training data from smaller moduli, demonstrating a critical necessity for curriculum learning. Finally, we analyze embedding layers and uncover a novel clustering phenomenon: the model spontaneously groups the integer inputs into bitwise rotationally-invariant clusters, revealing how representations can transfer from smaller to larger moduli.

</details>
