{"id": "2512.23566", "title": "From geometry to dynamics: Learning overdamped Langevin dynamics from sparse observations with geometric constraints", "authors": ["Dimitra Maoutsa"], "summary": "How can we learn the laws underlying the dynamics of stochastic systems when their trajectories are sampled sparsely in time? Existing methods either require temporally resolved high-frequency observations, or rely on geometric arguments that apply only to conservative systems, limiting the range of dynamics they can recover. Here, we present a new framework that reconciles these two perspectives by reformulating inference as a stochastic control problem. Our method uses geometry-driven path augmentation, guided by the geometry in the system's invariant density to reconstruct likely trajectories and infer the underlying dynamics without assuming specific parametric models. Applied to overdamped Langevin systems, our approach accurately recovers stochastic dynamics even from extremely undersampled data, outperforming existing methods in synthetic benchmarks. This work demonstrates the effectiveness of incorporating geometric inductive biases into stochastic system identification methods.", "published": "2025-12-29", "categories": ["math.DS", "cond-mat.stat-mech", "cs.LG", "math.OC", "stat.ML"], "pdf_url": "https://arxiv.org/pdf/2512.23566v1", "primary_category": "math.DS"}
{"id": "2512.23295", "title": "Spectral Analysis of Hard-Constraint PINNs: The Spatial Modulation Mechanism of Boundary Functions", "authors": ["Yuchen Xie", "Honghang Chi", "Haopeng Quan", "Yahui Wang", "Wei Wang", "Yu Ma"], "summary": "Physics-Informed Neural Networks with hard constraints (HC-PINNs) are increasingly favored for their ability to strictly enforce boundary conditions via a trial function ansatz $\\tilde{u} = A + B \\cdot N$, yet the theoretical mechanisms governing their training dynamics have remained unexplored.   Unlike soft-constrained formulations where boundary terms act as additive penalties, this work reveals that the boundary function $B$ introduces a multiplicative spatial modulation that fundamentally alters the learning landscape.   A rigorous Neural Tangent Kernel (NTK) framework for HC-PINNs is established, deriving the explicit kernel composition law.   This relationship demonstrates that the boundary function $B(\\vec{x})$ functions as a spectral filter, reshaping the eigenspectrum of the neural network's native kernel.   Through spectral analysis, the effective rank of the residual kernel is identified as a deterministic predictor of training convergence, superior to classical condition numbers.   It is shown that widely used boundary functions can inadvertently induce spectral collapse, leading to optimization stagnation despite exact boundary satisfaction.   Validated across multi-dimensional benchmarks, this framework transforms the design of boundary functions from a heuristic choice into a principled spectral optimization problem, providing a solid theoretical foundation for geometric hard constraints in scientific machine learning.", "published": "2025-12-29", "categories": ["cs.LG", "physics.comp-ph"], "pdf_url": "https://arxiv.org/pdf/2512.23295v1", "primary_category": "cs.LG"}
{"id": "2512.23056", "title": "PI-MFM: Physics-informed multimodal foundation model for solving partial differential equations", "authors": ["Min Zhu", "Jingmin Sun", "Zecheng Zhang", "Hayden Schaeffer", "Lu Lu"], "summary": "Partial differential equations (PDEs) govern a wide range of physical systems, and recent multimodal foundation models have shown promise for learning PDE solution operators across diverse equation families. However, existing multi-operator learning approaches are data-hungry and neglect physics during training. Here, we propose a physics-informed multimodal foundation model (PI-MFM) framework that directly enforces governing equations during pretraining and adaptation. PI-MFM takes symbolic representations of PDEs as the input, and automatically assembles PDE residual losses from the input expression via a vectorized derivative computation. These designs enable any PDE-encoding multimodal foundation model to be trained or adapted with unified physics-informed objectives across equation families. On a benchmark of 13 parametric one-dimensional time-dependent PDE families, PI-MFM consistently outperforms purely data-driven counterparts, especially with sparse labeled spatiotemporal points, partially observed time domains, or few labeled function pairs. Physics losses further improve robustness against noise, and simple strategies such as resampling collocation points substantially improve accuracy. We also analyze the accuracy, precision, and computational cost of automatic differentiation and finite differences for derivative computation within PI-MFM. Finally, we demonstrate zero-shot physics-informed fine-tuning to unseen PDE families: starting from a physics-informed pretrained model, adapting using only PDE residuals and initial/boundary conditions, without any labeled solution data, rapidly reduces test errors to around 1% and clearly outperforms physics-only training from scratch. These results show that PI-MFM provides a practical and scalable path toward data-efficient, transferable PDE solvers.", "published": "2025-12-28", "categories": ["cs.LG", "physics.comp-ph"], "pdf_url": "https://arxiv.org/pdf/2512.23056v1", "primary_category": "cs.LG"}
{"id": "2512.22397", "title": "Integrating Wide and Deep Neural Networks with Squeeze-and-Excitation Blocks for Multi-Target Property Prediction in Additively Manufactured Fiber Reinforced Composites", "authors": ["Behzad Parvaresh", "Rahmat K. Adesunkanmi", "Adel Alaeddini"], "summary": "Continuous fiber-reinforced composite manufactured by additive manufacturing (CFRC-AM) offers opportunities for printing lightweight materials with high specific strength. However, their performance is sensitive to the interaction of process and material parameters, making exhaustive experimental testing impractical. In this study, we introduce a data-efficient, multi-input, multi-target learning approach that integrates Latin Hypercube Sampling (LHS)-guided experimentation with a squeeze-and-excitation wide and deep neural network (SE-WDNN) to jointly predict multiple mechanical and manufacturing properties of CFRC-AMs based on different manufacturing parameters. We printed and tested 155 specimens selected from a design space of 4,320 combinations using a Markforged Mark Two 3D printer. The processed data formed the input-output set for our proposed model. We compared the results with those from commonly used machine learning models, including feedforward neural networks, Kolmogorov-Arnold networks, XGBoost, CatBoost, and random forests. Our model achieved the lowest overall test error (MAPE = 12.33%) and showed statistically significant improvements over the baseline wide and deep neural network for several target variables (paired t-tests, p <= 0.05). SHapley Additive exPlanations (SHAP) analysis revealed that reinforcement strategy was the major influence on mechanical performance. Overall, this study demonstrates that the integration of LHS and SE-WDNN enables interpretable and sample-efficient multi-target predictions, guiding parameter selection in CFRC-AM with a balance between mechanical behavior and manufacturing metrics.", "published": "2025-12-26", "categories": ["physics.comp-ph", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2512.22397v1", "primary_category": "physics.comp-ph"}
{"id": "2512.23396", "title": "PINNs for Electromagnetic Wave Propagation", "authors": ["Nilufer K. Bulut"], "summary": "Physics-Informed Neural Networks (PINNs) are a methodology that aims to solve physical systems by directly embedding PDE constraints into the neural network training process. In electromagnetism, where well-established methodologies such as FDTD and FEM already exist, new methodologies are expected to provide clear advantages to be accepted. Despite their mesh-free nature and applicability to inverse problems, PINNs can exhibit deficiencies in terms of accuracy and energy metrics when compared to FDTD solutions. This study demonstrates hybrid training strategies can bring PINNs closer to FDTD-level accuracy and energy consistency.   This study presents a hybrid methodology addressing common challenges in wave propagation scenarios. The causality collapse problem in time-dependent PINN training is addressed via time marching and causality-aware weighting. In order to mitigate the discontinuities that are introduced by time marching, a two-stage interface continuity loss is applied. In order to suppress loss accumulation, which is manifested as cumulative energy drift in electromagnetic waves, a local Poynting-based regularizer has been developed.   In the developed PINN model, high field accuracy is achieved with an average 0.09\\% $NRMSE$ and 1.01\\% $L^2$ error over time. Energy conservation is achieved on the PINN side with only a 0.024\\% relative energy mismatch in the 2D PEC cavity scenario. Training is performed without labeled field data, using only physics-based residual losses; FDTD is used solely for post-training evaluation. The results demonstrate that PINNs can achieve competitive results with FDTD in canonical electromagnetic examples and are a viable alternative.", "published": "2025-12-29", "categories": ["physics.comp-ph", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2512.23396v1", "primary_category": "physics.comp-ph"}
{"id": "2512.23080", "title": "QSAR-Guided Generative Framework for the Discovery of Synthetically Viable Odorants", "authors": ["Tim C. Pearce", "Ahmed Ibrahim"], "summary": "The discovery of novel odorant molecules is key for the fragrance and flavor industries, yet efficiently navigating the vast chemical space to identify structures with desirable olfactory properties remains a significant challenge. Generative artificial intelligence offers a promising approach for \\textit{de novo} molecular design but typically requires large sets of molecules to learn from. To address this problem, we present a framework combining a variational autoencoder (VAE) with a quantitative structure-activity relationship (QSAR) model to generate novel odorants from limited training sets of odor molecules. The self-supervised learning capabilities of the VAE allow it to learn SMILES grammar from ChemBL database, while its training objective is augmented with a loss term derived from an external QSAR model to structure the latent representation according to odor probability. While the VAE demonstrated high internal consistency in learning the QSAR supervision signal, validation against an external, unseen ground truth dataset (Unique Good Scents) confirms the model generates syntactically valid structures (100\\% validity achieved via rejection sampling) and 94.8\\% unique structures. The latent space is effectively structured by odor likelihood, evidenced by a FrÃ©chet ChemNet Distance (FCD) of $\\approx$ 6.96 between generated molecules and known odorants, compared to $\\approx$ 21.6 for the ChemBL baseline. Structural analysis via Bemis-Murcko scaffolds reveals that 74.4\\% of candidates possess novel core frameworks distinct from the training data, indicating the model performs extensive chemical space exploration beyond simple derivatization of known odorants. Generated candidates display physicochemical properties ....", "published": "2025-12-28", "categories": ["physics.chem-ph", "cs.LG", "q-bio.BM", "q-bio.QM"], "pdf_url": "https://arxiv.org/pdf/2512.23080v1", "primary_category": "physics.chem-ph"}
{"id": "2512.22597", "title": "Energy-Guided Flow Matching Enables Few-Step Conformer Generation and Ground-State Identification", "authors": ["Guikun Xu", "Xiaohan Yi", "Peilin Zhao", "Yatao Bian"], "summary": "Generating low-energy conformer ensembles and identifying ground-state conformations from molecular graphs remain computationally demanding with physics-based pipelines. Current learning-based approaches often suffer from a fragmented paradigm: generative models capture diversity but lack reliable energy calibration, whereas deterministic predictors target a single structure and fail to represent ensemble variability. Here we present EnFlow, a unified framework that couples flow matching (FM) with an explicitly learned energy model through an energy-guided sampling scheme defined along a non-Gaussian FM path. By incorporating energy-gradient guidance during sampling, our method steers trajectories toward lower-energy regions, substantially improving conformational fidelity, particularly in the few-step regime. The learned energy function further enables efficient energy-based ranking of generated ensembles for accurate ground-state identification. Extensive experiments on GEOM-QM9 and GEOM-Drugs demonstrate that EnFlow simultaneously improves generation metrics with 1--2 ODE-steps and reduces ground-state prediction errors compared with state-of-the-art methods.", "published": "2025-12-27", "categories": ["cs.LG", "physics.chem-ph"], "pdf_url": "https://arxiv.org/pdf/2512.22597v1", "primary_category": "cs.LG"}
{"id": "2512.22568", "title": "Lessons from Neuroscience for AI: How integrating Actions, Compositional Structure and Episodic Memory could enable Safe, Interpretable and Human-Like AI", "authors": ["Rajesh P. N. Rao", "Vishwas Sathish", "Linxing Preston Jiang", "Matthew Bryan", "Prashant Rangarajan"], "summary": "The phenomenal advances in large language models (LLMs) and other foundation models over the past few years have been based on optimizing large-scale transformer models on the surprisingly simple objective of minimizing next-token prediction loss, a form of predictive coding that is also the backbone of an increasingly popular model of brain function in neuroscience and cognitive science. However, current foundation models ignore three other important components of state-of-the-art predictive coding models: tight integration of actions with generative models, hierarchical compositional structure, and episodic memory. We propose that to achieve safe, interpretable, energy-efficient, and human-like AI, foundation models should integrate actions, at multiple scales of abstraction, with a compositional generative architecture and episodic memory. We present recent evidence from neuroscience and cognitive science on the importance of each of these components. We describe how the addition of these missing components to foundation models could help address some of their current deficiencies: hallucinations and superficial understanding of concepts due to lack of grounding, a missing sense of agency/responsibility due to lack of control, threats to safety and trustworthiness due to lack of interpretability, and energy inefficiency. We compare our proposal to current trends, such as adding chain-of-thought (CoT) reasoning and retrieval-augmented generation (RAG) to foundation models, and discuss new ways of augmenting these models with brain-inspired components. We conclude by arguing that a rekindling of the historically fruitful exchange of ideas between brain science and AI will help pave the way towards safe and interpretable human-centered AI.", "published": "2025-12-27", "categories": ["cs.AI", "physics.bio-ph", "q-bio.NC"], "pdf_url": "https://arxiv.org/pdf/2512.22568v1", "primary_category": "cs.AI"}
{"id": "2512.23175", "title": "HELM-BERT: A Transformer for Medium-sized Peptide Property Prediction", "authors": ["Seungeon Lee", "Takuto Koyama", "Itsuki Maeda", "Shigeyuki Matsumoto", "Yasushi Okuno"], "summary": "Therapeutic peptides have emerged as a pivotal modality in modern drug discovery, occupying a chemically and topologically rich space. While accurate prediction of their physicochemical properties is essential for accelerating peptide development, existing molecular language models rely on representations that fail to capture this complexity. Atom-level SMILES notation generates long token sequences and obscures cyclic topology, whereas amino-acid-level representations cannot encode the diverse chemical modifications central to modern peptide design. To bridge this representational gap, the Hierarchical Editing Language for Macromolecules (HELM) offers a unified framework enabling precise description of both monomer composition and connectivity, making it a promising foundation for peptide language modeling. Here, we propose HELM-BERT, the first encoder-based peptide language model trained on HELM notation. Based on DeBERTa, HELM-BERT is specifically designed to capture hierarchical dependencies within HELM sequences. The model is pre-trained on a curated corpus of 39,079 chemically diverse peptides spanning linear and cyclic structures. HELM-BERT significantly outperforms state-of-the-art SMILES-based language models in downstream tasks, including cyclic peptide membrane permeability prediction and peptide-protein interaction prediction. These results demonstrate that HELM's explicit monomer- and topology-aware representations offer substantial data-efficiency advantages for modeling therapeutic peptides, bridging a long-standing gap between small-molecule and protein language models.", "published": "2025-12-29", "categories": ["cs.LG", "q-bio.BM"], "pdf_url": "https://arxiv.org/pdf/2512.23175v1", "primary_category": "cs.LG"}
