<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 11]
- [cs.CL](#cs.CL) [Total: 8]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.RO](#cs.RO) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Out-of-Sight Trajectories: Tracking, Fusion, and Prediction](https://arxiv.org/abs/2509.15219)
*Haichao Zhang, Yi Xu, Yun Fu*

Main category: cs.CV

TL;DR: This paper introduces Out-of-Sight Trajectory (OST) prediction, a novel task addressing the challenge of predicting trajectories of objects hidden from view using noisy sensor data. The approach enhances trajectory denoising and prediction, particularly in scenarios with limited camera coverage and obstructions.


<details>
  <summary>Details</summary>
Motivation: Existing trajectory prediction methods struggle with out-of-sight objects and noisy sensor data, posing safety risks in applications like autonomous driving and robotics. There's a need for a method that can effectively predict trajectories even when visual references are limited.

Method: The proposed approach, OOSTraj, utilizes a Vision-Positioning Denoising Module that leverages camera calibration to establish a vision-positioning mapping. This allows for unsupervised denoising of noisy sensor data and prediction of noise-free visual trajectories for both pedestrians and vehicles.

Result: Extensive evaluations on the Vi-Fi and JRDB datasets demonstrate state-of-the-art performance in both trajectory denoising and prediction, significantly outperforming previous baselines and traditional denoising methods like Kalman filtering.

Conclusion: This work represents the first integration of vision-positioning projection for denoising noisy sensor trajectories of out-of-sight agents, opening new avenues for advancements in trajectory prediction and enabling more robust and reliable autonomous systems.

Abstract: Trajectory prediction is a critical task in computer vision and autonomous systems, playing a key role in autonomous driving, robotics, surveillance, and virtual reality. Existing methods often rely on complete and noise-free observational data, overlooking the challenges associated with out-of-sight objects and the inherent noise in sensor data caused by limited camera coverage, obstructions, and the absence of ground truth for denoised trajectories. These limitations pose safety risks and hinder reliable prediction in real-world scenarios. In this extended work, we present advancements in Out-of-Sight Trajectory (OST), a novel task that predicts the noise-free visual trajectories of out-of-sight objects using noisy sensor data. Building on our previous research, we broaden the scope of Out-of-Sight Trajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending its applicability to autonomous driving, robotics, surveillance, and virtual reality. Our enhanced Vision-Positioning Denoising Module leverages camera calibration to establish a vision-positioning mapping, addressing the lack of visual references, while effectively denoising noisy sensor data in an unsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB datasets, our approach achieves state-of-the-art performance in both trajectory denoising and prediction, significantly surpassing previous baselines. Additionally, we introduce comparisons with traditional denoising methods, such as Kalman filtering, and adapt recent trajectory prediction models to our task, providing a comprehensive benchmark. This work represents the first initiative to integrate vision-positioning projection for denoising noisy sensor trajectories of out-of-sight agents, paving the way for future advances. The code and preprocessed datasets are available at github.com/Hai-chao-Zhang/OST

</details>


### [2] [Semi-Supervised 3D Medical Segmentation from 2D Natural Images Pretrained Model](https://arxiv.org/abs/2509.15167)
*Pak-Hei Yeung, Jayroop Ramesh, Pengfei Lyu, Ana Namburete, Jagath Rajapakse*

Main category: cs.CV

TL;DR: This paper introduces M&N, a model-agnostic framework that transfers knowledge from 2D natural image models to improve 3D medical image segmentation in a semi-supervised setting, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Training 3D medical image segmentation models from scratch requires large labeled datasets, which are often scarce. Leveraging knowledge from 2D natural image models, where abundant labeled data exists, offers a promising solution.

Method: M&N iteratively co-trains a 2D pretrained model and a 3D segmentation model using pseudo-masks generated by each other. A learning rate guided sampling strategy adaptively balances labeled and unlabeled data to mitigate the impact of inaccurate pseudo-masks.

Result: M&N achieves state-of-the-art performance on multiple publicly available datasets, outperforming thirteen existing semi-supervised segmentation approaches. The framework is model-agnostic and adaptable to different architectures.

Conclusion: M&N provides a flexible and generalizable approach for semi-supervised 3D medical image segmentation by effectively transferring knowledge from 2D natural image models, demonstrating its potential for improving medical image analysis with limited labeled data.

Abstract: This paper explores the transfer of knowledge from general vision models pretrained on 2D natural images to improve 3D medical image segmentation. We focus on the semi-supervised setting, where only a few labeled 3D medical images are available, along with a large set of unlabeled images. To tackle this, we propose a model-agnostic framework that progressively distills knowledge from a 2D pretrained model to a 3D segmentation model trained from scratch. Our approach, M&N, involves iterative co-training of the two models using pseudo-masks generated by each other, along with our proposed learning rate guided sampling that adaptively adjusts the proportion of labeled and unlabeled data in each training batch to align with the models' prediction accuracy and stability, minimizing the adverse effect caused by inaccurate pseudo-masks. Extensive experiments on multiple publicly available datasets demonstrate that M&N achieves state-of-the-art performance, outperforming thirteen existing semi-supervised segmentation approaches under all different settings. Importantly, ablation studies show that M&N remains model-agnostic, allowing seamless integration with different architectures. This ensures its adaptability as more advanced models emerge. The code is available at https://github.com/pakheiyeung/M-N.

</details>


### [3] [ProtoMedX: Towards Explainable Multi-Modal Prototype Learning for Bone Health Classification](https://arxiv.org/abs/2509.14830)
*Alvaro Lopez Pellicer, Andre Mariucci, Plamen Angelov, Marwan Bukhari, Jemma G. Kerns*

Main category: cs.CV

TL;DR: ProtoMedX is a novel multi-modal prototype learning model for bone health classification that combines DEXA scans and patient records, offering both state-of-the-art accuracy and explainability for clinical decision-making.


<details>
  <summary>Details</summary>
Motivation: Existing bone health classification methods often lack explainability, rely solely on vision, and oversimplify diagnostic categories (e.g., only classifying as 'normal' or 'osteoporosis'), hindering clinical utility and compliance with regulations like the EU AI Act.

Method: ProtoMedX utilizes a prototype-based architecture with dual prototype spaces (visual and clinical) linked by cross-modal attention. It classifies new patients based on similarity to learned prototypes, mimicking clinical reasoning and providing inherent explainability.

Result: ProtoMedX achieves 87.58% accuracy using vision alone and 89.8% with multi-modal data (DEXA scans and patient records), surpassing existing published methods. The prototype-based approach provides visually understandable explanations of model decisions.

Conclusion: ProtoMedX represents a significant advancement in bone health classification by offering a transparent, clinically relevant, and highly accurate model that addresses the limitations of existing deep learning approaches and aligns with the growing need for explainable AI in healthcare.

Abstract: Bone health studies are crucial in medical practice for the early detection and treatment of Osteopenia and Osteoporosis. Clinicians usually make a diagnosis based on densitometry (DEXA scans) and patient history. The applications of AI in this field are ongoing research. Most successful methods rely on deep learning models that use vision alone (DEXA/X-ray imagery) and focus on prediction accuracy, while explainability is often disregarded and left to post hoc assessments of input contributions. We propose ProtoMedX, a multi-modal model that uses both DEXA scans of the lumbar spine and patient records. ProtoMedX's prototype-based architecture is explainable by design, which is crucial for medical applications, especially in the context of the upcoming EU AI Act, as it allows explicit analysis of model decisions, including incorrect ones. ProtoMedX demonstrates state-of-the-art performance in bone health classification while also providing explanations that can be visually understood by clinicians. Using a dataset of 4,160 real NHS patients, the proposed ProtoMedX achieves 87.58% accuracy in vision-only tasks and 89.8% in its multi-modal variant, both surpassing existing published methods.

</details>


### [4] [Synthetic-to-Real Object Detection using YOLOv11 and Domain Randomization Strategies](https://arxiv.org/abs/2509.15045)
*Luisa Torquato Niño, Hamza A. A. Gardi*

Main category: cs.CV

TL;DR: This paper explores training a YOLOv11 object detection model solely on synthetic data to detect soup cans, addressing the synthetic-to-real domain gap through domain randomization and data augmentation strategies. The approach achieved a high mAP@50 score in a Kaggle competition, demonstrating the potential of synthetic-only training.


<details>
  <summary>Details</summary>
Motivation: The high cost and effort of collecting and labeling real-world datasets for object detection motivates the use of synthetic data as a cost-effective and scalable alternative. However, the domain gap between synthetic and real data poses a significant challenge to generalization.

Method: The researchers trained a YOLOv11 model on a diverse synthetic dataset generated by Falcon’s Duality AI Simulator, employing extensive data augmentation and experimenting with dataset composition and model scaling. They evaluated performance both qualitatively and quantitatively, using a manually labeled real-world test set to guide development.

Result: The best performing configuration, a YOLOv11l model trained on an expanded and diverse synthetic dataset, achieved a final mAP@50 of 0.910 on the competition’s hidden test set.  Benchmarking showed YOLOv11 outperformed previous YOLO versions.

Conclusion: The results demonstrate the feasibility of training high-performing object detection models using only synthetic data and domain randomization, while also highlighting the ongoing challenges in fully capturing real-world variability and the importance of dataset diversity and careful data augmentation.

Abstract: This paper addresses the synthetic-to-real domain gap in object detection, focusing on training a YOLOv11 model to detect a specific object (a soup can) using only synthetic data and domain randomization strategies. The methodology involves extensive experimentation with data augmentation, dataset composition, and model scaling. While synthetic validation metrics were consistently high, they proved to be poor predictors of real-world performance. Consequently, models were also evaluated qualitatively, through visual inspection of predictions, and quantitatively, on a manually labeled real-world test set, to guide development. Final mAP@50 scores were provided by the official Kaggle competition. Key findings indicate that increasing synthetic dataset diversity, specifically by including varied perspectives and complex backgrounds, combined with carefully tuned data augmentation, were crucial in bridging the domain gap. The best performing configuration, a YOLOv11l model trained on an expanded and diverse dataset, achieved a final mAP@50 of 0.910 on the competition's hidden test set. This result demonstrates the potential of a synthetic-only training approach while also highlighting the remaining challenges in fully capturing real-world variability.

</details>


### [5] [Template-Based Cortical Surface Reconstruction with Minimal Energy Deformation](https://arxiv.org/abs/2509.14827)
*Patrick Madlindl, Fabian Bongratz, Christian Wachinger*

Main category: cs.CV

TL;DR: This paper introduces a Minimal Energy Deformation (MED) loss to improve the consistency and reproducibility of template-based cortical surface reconstruction (CSR) models, without sacrificing reconstruction accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning-based CSR methods, while fast, suffer from inconsistent training outcomes and potentially suboptimal deformation trajectories, hindering their trustworthiness and reliability for downstream analyses.

Method: The authors incorporated a MED loss, acting as a regularizer on deformation trajectories, into the V2C-Flow model. This loss complements the widely used Chamfer distance and encourages smoother, more efficient deformations.

Result: The incorporation of the MED loss resulted in considerable improvements in training consistency and reproducibility, without negatively impacting reconstruction accuracy or topological correctness.

Conclusion: The proposed MED loss provides a valuable tool for enhancing the reliability and trustworthiness of template-based CSR models, facilitating more robust neuroimage analysis and statistical inference.

Abstract: Cortical surface reconstruction (CSR) from magnetic resonance imaging (MRI) is fundamental to neuroimage analysis, enabling morphological studies of the cerebral cortex and functional brain mapping. Recent advances in learning-based CSR have dramatically accelerated processing, allowing for reconstructions through the deformation of anatomical templates within seconds. However, ensuring the learned deformations are optimal in terms of deformation energy and consistent across training runs remains a particular challenge. In this work, we design a Minimal Energy Deformation (MED) loss, acting as a regularizer on the deformation trajectories and complementing the widely used Chamfer distance in CSR. We incorporate it into the recent V2C-Flow model and demonstrate considerable improvements in previously neglected training consistency and reproducibility without harming reconstruction accuracy and topological correctness.

</details>


### [6] [Leveraging Geometric Visual Illusions as Perceptual Inductive Biases for Vision Models](https://arxiv.org/abs/2509.15156)
*Haobo Yang, Minghao Guo, Dequan Yang, Wenyu Wang*

Main category: cs.CV

TL;DR: This paper explores incorporating geometric visual illusions into image classification training to improve generalization, particularly for transformer-based models. The results demonstrate that these illusions, despite being synthetic, can enhance a model's sensitivity to structural details.


<details>
  <summary>Details</summary>
Motivation: Modern deep learning models often rely on statistical regularities and miss crucial structural information that humans readily perceive. This work aims to bridge the gap between machine vision and human perception by leveraging insights from perceptual psychology.

Method: The researchers created a synthetic dataset of five classic geometric illusions with parametric control over distortion. They then evaluated three multi-source learning strategies (joint, parallel, and hybrid) that combined illusion recognition tasks with ImageNet classification objectives.

Result: Incorporating geometric illusions as auxiliary supervision systematically improved generalization, especially in visually challenging cases.  The synthetic perceptual cues measurably improved classification accuracy, particularly for ViT architectures, by enhancing sensitivity to contour and texture structure.

Conclusion: This study demonstrates a novel integration of perceptual science and machine learning, suggesting that geometric illusions can be repurposed to teach vision models to see structure more robustly and opening new avenues for embedding perceptual priors into vision model design.

Abstract: Contemporary deep learning models have achieved impressive performance in image classification by primarily leveraging statistical regularities within large datasets, but they rarely incorporate structured insights drawn directly from perceptual psychology. To explore the potential of perceptually motivated inductive biases, we propose integrating classic geometric visual illusions well-studied phenomena from human perception into standard image-classification training pipelines. Specifically, we introduce a synthetic, parametric geometric-illusion dataset and evaluate three multi-source learning strategies that combine illusion recognition tasks with ImageNet classification objectives. Our experiments reveal two key conceptual insights: (i) incorporating geometric illusions as auxiliary supervision systematically improves generalization, especially in visually challenging cases involving intricate contours and fine textures; and (ii) perceptually driven inductive biases, even when derived from synthetic stimuli traditionally considered unrelated to natural image recognition, can enhance the structural sensitivity of both CNN and transformer-based architectures. These results demonstrate a novel integration of perceptual science and machine learning and suggest new directions for embedding perceptual priors into vision model design.

</details>


### [7] [Sea-ing Through Scattered Rays: Revisiting the Image Formation Model for Realistic Underwater Image Generation](https://arxiv.org/abs/2509.15011)
*Vasiliki Ismiroglou, Malte Pedersen, Stefan H. Bengtson, Andreas Aakerberg, Thomas B. Moeslund*

Main category: cs.CV

TL;DR: This paper addresses the limitations of existing underwater image formation models (IFMs) in simulating highly turbid environments by incorporating a forward scattering term and considering a non-uniform medium. A new dataset (BUCKET) was collected to validate the improvements.


<details>
  <summary>Details</summary>
Motivation: Existing synthetic underwater image generation methods often fail to accurately represent the complex visibility loss in highly turbid waters, hindering the development of robust underwater computer vision systems.

Method: The authors propose an improved IFM pipeline that includes a forward scattering term and accounts for a non-uniform medium. They also created the BUCKET dataset, a collection of real underwater footage with corresponding reference images captured under controlled turbidity conditions.

Result: The proposed model demonstrates qualitative improvements over existing IFMs, particularly in high-turbidity scenarios, with a selection rate of 82.5% based on survey participant feedback.

Conclusion: The improved IFM provides a more realistic simulation of underwater image formation in turbid environments, facilitating the generation of more effective synthetic training data for underwater computer vision applications and contributing to a better understanding of IFM implementations.

Abstract: In recent years, the underwater image formation model has found extensive use in the generation of synthetic underwater data. Although many approaches focus on scenes primarily affected by discoloration, they often overlook the model's ability to capture the complex, distance-dependent visibility loss present in highly turbid environments. In this work, we propose an improved synthetic data generation pipeline that includes the commonly omitted forward scattering term, while also considering a nonuniform medium. Additionally, we collected the BUCKET dataset under controlled turbidity conditions to acquire real turbid footage with the corresponding reference images. Our results demonstrate qualitative improvements over the reference model, particularly under increasing turbidity, with a selection rate of 82. 5\% by survey participants. Data and code can be accessed on the project page: vap.aau.dk/sea-ing-through-scattered-rays.

</details>


### [8] [RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching](https://arxiv.org/abs/2509.14966)
*Xingwu Zhang, Guanxuan Li, Zhuocheng Zhang, Zijun Long*

Main category: cs.CV

TL;DR: RoboEye is a novel two-stage framework that enhances 2D robotic object identification by dynamically incorporating 3D geometric reasoning, improving recall and avoiding reliance on expensive 3D sensors.


<details>
  <summary>Details</summary>
Motivation: Existing object identification methods relying solely on 2D appearance features struggle with the increasing complexity of large-scale e-commerce warehouses, including viewpoint variations, occlusions, packaging differences, and long-tail distributions, leading to performance degradation.

Method: RoboEye uses a two-stage approach: first, a 2D semantic feature extractor generates candidate rankings. Then, a 3D-feature-awareness module determines if 3D re-ranking is needed, followed by a robot 3D retrieval transformer utilizing geometry-aware dense features and keypoint-based matching for robust verification.

Result: RoboEye achieves a 7.1% improvement in Recall@1 compared to the state-of-the-art (RoboLLM) while operating solely on RGB images.

Conclusion: RoboEye offers a cost-effective and robust solution for robotic object identification in challenging warehouse environments by leveraging 3D geometric information without requiring explicit 3D inputs, significantly improving performance and reducing deployment costs.

Abstract: The rapidly growing number of product categories in large-scale e-commerce makes accurate object identification for automated packing in warehouses substantially more difficult. As the catalog grows, intra-class variability and a long tail of rare or visually similar items increase, and when combined with diverse packaging, cluttered containers, frequent occlusion, and large viewpoint changes-these factors amplify discrepancies between query and reference images, causing sharp performance drops for methods that rely solely on 2D appearance features. Thus, we propose RoboEye, a two-stage identification framework that dynamically augments 2D semantic features with domain-adapted 3D reasoning and lightweight adapters to bridge training deployment gaps. In the first stage, we train a large vision model to extract 2D features for generating candidate rankings. A lightweight 3D-feature-awareness module then estimates 3D feature quality and predicts whether 3D re-ranking is necessary, preventing performance degradation and avoiding unnecessary computation. When invoked, the second stage uses our robot 3D retrieval transformer, comprising a 3D feature extractor that produces geometry-aware dense features and a keypoint-based matcher that computes keypoint-correspondence confidences between query and reference images instead of conventional cosine-similarity scoring. Experiments show that RoboEye improves Recall@1 by 7.1% over the prior state of the art (RoboLLM). Moreover, RoboEye operates using only RGB images, avoiding reliance on explicit 3D inputs and reducing deployment costs. The code used in this paper is publicly available at: https://github.com/longkukuhi/RoboEye.

</details>


### [9] [MARIC: Multi-Agent Reasoning for Image Classification](https://arxiv.org/abs/2509.14860)
*Wonduk Seo, Minhyeong Yu, Hyunjin An, Seunghyun Lee*

Main category: cs.CV

TL;DR: MARIC is a novel multi-agent framework for image classification that decomposes the task into collaborative reasoning, overcoming limitations of traditional training and single-pass VLMs.


<details>
  <summary>Details</summary>
Motivation: Traditional image classification methods require extensive training data and fine-tuning, while existing vision-language models (VLMs) often lack robust reasoning capabilities and multi-perspective analysis.

Method: MARIC utilizes an Outliner Agent to generate prompts, three Aspect Agents to extract fine-grained descriptions, and a Reasoning Agent to synthesize these outputs through reflection, creating a unified representation for classification.

Result: Experiments on four diverse image classification datasets demonstrate that MARIC significantly outperforms baselines, achieving improved accuracy and interpretability.

Conclusion: MARIC highlights the potential of multi-agent visual reasoning as a scalable and interpretable paradigm for advancing image classification beyond traditional approaches, offering a more robust and explainable solution.

Abstract: Image classification has traditionally relied on parameter-intensive model training, requiring large-scale annotated datasets and extensive fine tuning to achieve competitive performance. While recent vision language models (VLMs) alleviate some of these constraints, they remain limited by their reliance on single pass representations, often failing to capture complementary aspects of visual content. In this paper, we introduce Multi Agent based Reasoning for Image Classification (MARIC), a multi agent framework that reformulates image classification as a collaborative reasoning process. MARIC first utilizes an Outliner Agent to analyze the global theme of the image and generate targeted prompts. Based on these prompts, three Aspect Agents extract fine grained descriptions along distinct visual dimensions. Finally, a Reasoning Agent synthesizes these complementary outputs through integrated reflection step, producing a unified representation for classification. By explicitly decomposing the task into multiple perspectives and encouraging reflective synthesis, MARIC mitigates the shortcomings of both parameter-heavy training and monolithic VLM reasoning. Experiments on 4 diverse image classification benchmark datasets demonstrate that MARIC significantly outperforms baselines, highlighting the effectiveness of multi-agent visual reasoning for robust and interpretable image classification.

</details>


### [10] [Not All Degradations Are Equal: A Targeted Feature Denoising Framework for Generalizable Image Super-Resolution](https://arxiv.org/abs/2509.14841)
*Hongjun Wang, Jiyuan Chen, Zhengwei Yin, Xuan Song, Yinqiang Zheng*

Main category: cs.CV

TL;DR: This paper addresses the challenge of generalizable image super-resolution by identifying that models primarily overfit to noise degradation. It proposes a targeted feature denoising framework to mitigate this overfitting without architectural changes.


<details>
  <summary>Details</summary>
Motivation: Existing super-resolution models struggle to generalize to real-world images due to a domain gap between synthetic training data and complex, diverse real-world degradations. While regularization techniques exist, they treat all degradation types equally, failing to account for the unique overfitting behavior caused by noise.

Method: The proposed framework consists of a noise detection module and a denoising module, seamlessly integrated with existing super-resolution models. It identifies and removes noise-related features, allowing the model to focus on content-related features.

Result: The targeted feature denoising framework outperforms previous regularization-based methods across five benchmark datasets, demonstrating improved generalization performance in both synthetic and real-world scenarios.

Conclusion: This work highlights the importance of targeted degradation mitigation and provides a practical solution for improving the generalizability of image super-resolution models by specifically addressing noise overfitting. The framework offers a general and easily integrable approach to enhance SR performance.

Abstract: Generalizable Image Super-Resolution aims to enhance model generalization capabilities under unknown degradations. To achieve this goal, the models are expected to focus only on image content-related features instead of overfitting degradations. Recently, numerous approaches such as Dropout and Feature Alignment have been proposed to suppress models' natural tendency to overfit degradations and yield promising results. Nevertheless, these works have assumed that models overfit to all degradation types (e.g., blur, noise, JPEG), while through careful investigations in this paper, we discover that models predominantly overfit to noise, largely attributable to its distinct degradation pattern compared to other degradation types. In this paper, we propose a targeted feature denoising framework, comprising noise detection and denoising modules. Our approach presents a general solution that can be seamlessly integrated with existing super-resolution models without requiring architectural modifications. Our framework demonstrates superior performance compared to previous regularization-based methods across five traditional benchmarks and datasets, encompassing both synthetic and real-world scenarios.

</details>


### [11] [[Re] Improving Interpretation Faithfulness for Vision Transformers](https://arxiv.org/abs/2509.14846)
*Izabela Kurek, Wojciech Trejter, Stipe Frkovic, Andro Erdelez*

Main category: cs.CV

TL;DR: This paper reproduces and extends the findings of Hu et al. (2024) on Faithful Vision Transformers (FViTs), which use Diffusion Denoised Smoothing (DDS) to improve interpretability robustness. The study investigates the effectiveness of DDS on various interpretability methods and assesses its computational cost.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers (ViTs) are vulnerable to adversarial attacks that can drastically alter their explanations, undermining the reliability of interpretability methods. The research aims to validate and expand upon the proposed solution of FViTs and DDS to address this vulnerability.

Method: The authors replicated the experiments from Hu et al. (2024) using DDS to create FViTs. They tested the robustness of FViTs and other interpretability methods (Raw Attention, GradCAM, Rollout, LRP, TA) against attacks in segmentation and classification tasks, and measured the computational overhead of DDS.

Result: The results generally align with Hu et al.'s findings, demonstrating improved interpretability robustness with DDS. However, minor discrepancies were observed and discussed. The study also quantified the computational costs and environmental impact associated with using DDS.

Conclusion: This work provides a validation and extension of FViTs, highlighting the potential of DDS to enhance the faithfulness and robustness of interpretability methods for Vision Transformers. The findings contribute to the ongoing debate between post-hoc and model-based interpretability, suggesting that integrating interpretability directly into the model can improve both performance and faithfulness.

Abstract: This work aims to reproduce the results of Faithful Vision Transformers (FViTs) proposed by arXiv:2311.17983 alongside interpretability methods for Vision Transformers from arXiv:2012.09838 and Xu (2022) et al. We investigate claims made by arXiv:2311.17983, namely that the usage of Diffusion Denoised Smoothing (DDS) improves interpretability robustness to (1) attacks in a segmentation task and (2) perturbation and attacks in a classification task. We also extend the original study by investigating the authors' claims that adding DDS to any interpretability method can improve its robustness under attack. This is tested on baseline methods and the recently proposed Attribution Rollout method. In addition, we measure the computational costs and environmental impact of obtaining an FViT through DDS. Our results broadly agree with the original study's findings, although minor discrepancies were found and discussed.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [12] [Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning](https://arxiv.org/abs/2509.15188)
*Yeongbin Seo, Dongha Lee, Jaehyung Kim, Jinyoung Yeo*

Main category: cs.CL

TL;DR: This paper introduces Convolutional Decoding (Conv) and Rejecting Rule-based Fine-Tuning (R2FT) to address the 'long decoding-window' problem in diffusion language models, enabling faster and more fluent text generation while maintaining bidirectionality.


<details>
  <summary>Details</summary>
Motivation: Diffusion language models offer speed and bidirectionality advantages over autoregressive models, but suffer from the 'long decoding-window' problem where distant tokens become irrelevant or repetitive. Existing solutions like semi-autoregressive methods sacrifice speed and bidirectionality to mitigate this issue.

Method: The authors propose two methods: (1) Convolutional Decoding (Conv) which narrows the decoding window using normalization instead of hard segmentation; and (2) Rejecting Rule-based Fine-Tuning (R2FT), a post-hoc training scheme to better align distant tokens.

Result: The proposed methods achieve state-of-the-art results on open-ended generation benchmarks (AlpacaEval) with significantly lower step sizes compared to previous diffusion LM baselines, demonstrating both speed and quality improvements.

Conclusion: The work demonstrates a significant advancement in diffusion language models, overcoming a key bottleneck to achieve faster, more fluent, and bidirectional text generation, making them a more competitive alternative to autoregressive models.

Abstract: Autoregressive (AR) language models generate text one token at a time, which limits their inference speed. Diffusion-based language models offer a promising alternative, as they can decode multiple tokens in parallel. However, we identify a key bottleneck in current diffusion LMs: the long decoding-window problem, where tokens generated far from the input context often become irrelevant or repetitive. Previous solutions like semi-autoregressive address this issue by splitting windows into blocks, but this sacrifices speed and bidirectionality, eliminating the main advantage of diffusion models. To overcome this, we propose Convolutional decoding (Conv), a normalization-based method that narrows the decoding window without hard segmentation, leading to better fluency and flexibility. Additionally, we introduce Rejecting Rule-based Fine-Tuning (R2FT), a post-hoc training scheme that better aligns tokens at positions far from context. Our methods achieve state-of-the-art results on open-ended generation benchmarks (e.g., AlpacaEval) among diffusion LM baselines, with significantly lower step size than previous works, demonstrating both speed and quality improvements.

</details>


### [13] [Patent Language Model Pretraining with ModernBERT](https://arxiv.org/abs/2509.14926)
*Amirhossein Yousefiramandi, Ciaran Cooney*

Main category: cs.CL

TL;DR: This paper introduces ModernBERT-base-PT, a domain-specific language model pretrained on a large corpus of patent records, demonstrating improved performance and significantly faster inference compared to existing patent NLP models.


<details>
  <summary>Details</summary>
Motivation: Existing language models, including BERT, struggle with specialized domains like patents due to their training on general-domain text. There's a need for domain-specific models that leverage recent architectural advancements for improved performance and efficiency in patent NLP tasks.

Method: The authors pretrained three domain-specific masked language models (ModernBERT-base-PT, ModernBERT-base-VX, and Mosaic-BERT-large) using the ModernBERT architecture and over 60 million patent records. They incorporated architectural optimizations like FlashAttention, rotary embeddings, and GLU feed-forward layers, and evaluated the models on four patent classification tasks.

Result: ModernBERT-base-PT consistently outperformed the general-purpose ModernBERT baseline on three out of four datasets and achieved competitive performance with PatentBERT. Scaling the model size and customizing the tokenizer further enhanced performance. Notably, all ModernBERT variants exhibited substantially faster inference (over 3×) than PatentBERT.

Conclusion: Domain-specific pretraining with architectural improvements like ModernBERT significantly benefits patent-focused NLP tasks, offering improved accuracy and faster inference, making it suitable for time-sensitive applications.

Abstract: Transformer-based language models such as BERT have become foundational in NLP, yet their performance degrades in specialized domains like patents, which contain long, technical, and legally structured text. Prior approaches to patent NLP have primarily relied on fine-tuning general-purpose models or domain-adapted variants pretrained with limited data. In this work, we pretrain 3 domain-specific masked language models for patents, using the ModernBERT architecture and a curated corpus of over 60 million patent records. Our approach incorporates architectural optimizations, including FlashAttention, rotary embeddings, and GLU feed-forward layers. We evaluate our models on four downstream patent classification tasks. Our model, ModernBERT-base-PT, consistently outperforms the general-purpose ModernBERT baseline on three out of four datasets and achieves competitive performance with a baseline PatentBERT. Additional experiments with ModernBERT-base-VX and Mosaic-BERT-large demonstrate that scaling the model size and customizing the tokenizer further enhance performance on selected tasks. Notably, all ModernBERT variants retain substantially faster inference over - 3x that of PatentBERT - underscoring their suitability for time-sensitive applications. These results underscore the benefits of domain-specific pretraining and architectural improvements for patent-focused NLP tasks.

</details>


### [14] [SMARTER: A Data-efficient Framework to Improve Toxicity Detection with Explanation via Self-augmenting Large Language Models](https://arxiv.org/abs/2509.15174)
*Huy Nghiem, Advik Sachdeva, Hal Daumé III*

Main category: cs.CL

TL;DR: SMARTER is a novel two-stage framework that leverages Large Language Models (LLMs) to improve toxicity detection and provide explanations, even with limited training data. It uses self-augmentation and cross-model refinement to achieve high performance and explainability.


<details>
  <summary>Details</summary>
Motivation: Content moderation is crucial for social media platforms, but traditional methods are inefficient and lack transparency. Existing machine learning models require substantial training data and often fail to provide human-understandable explanations.

Method: SMARTER consists of two stages: (1) LLMs self-augment on few-shot data using preference optimization with synthetic explanations, and (2) cross-model refinement where one LLM is trained on responses generated by another. This approach utilizes LLMs' self-improving capabilities for both classification and explanation.

Result: Experiments on three benchmark datasets (HateXplain, Latent Hate, and Implicit Hate) demonstrate that SMARTER achieves up to 13.5% macro-F1 improvement over standard few-shot baselines, using a fraction of the full training data.

Conclusion: SMARTER provides a scalable and practical approach for low-resource NLP practitioners to achieve both strong performance and explanability in content moderation, leveraging the self-improving capabilities of LLMs.

Abstract: WARNING: This paper contains examples of offensive materials. Toxic content has become pervasive on social media platforms. We introduce SMARTER, a data-efficient two-stage framework for explainable content moderation using Large Language Models (LLMs). In Stage 1, we leverage LLMs' own outputs to generate synthetic explanations for both correct and incorrect labels, enabling alignment via preference optimization with minimal human supervision. In Stage 2, we refine explanation quality through cross-model training, allowing weaker models to align stylistically and semantically with stronger ones. Experiments on three benchmark tasks -- HateXplain, Latent Hate, and Implicit Hate -- demonstrate that SMARTER enables LLMs to achieve up to a 13.5% macro-F1 improvement over standard few-shot baselines while using only a fraction of the full training data. Our framework offers a scalable strategy for low-resource settings by harnessing LLMs' self-improving capabilities for both classification and explanation.

</details>


### [15] [TextMine: LLM-Powered Knowledge Extraction for Humanitarian Mine Action](https://arxiv.org/abs/2509.15098)
*Chenyue Zhou, Gürkan Solmaz, Flavio Cirillo, Kiril Gashteovski, Jonathan Fürst*

Main category: cs.CL

TL;DR: TextMine is a novel LLM-powered pipeline for extracting structured knowledge from unstructured Humanitarian Mine Action (HMA) reports, creating a valuable resource for improving demining efforts. It introduces a dedicated HMA ontology and a curated dataset for evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing HMA knowledge is largely locked in unstructured reports, hindering sharing and learning. This work aims to transform this data into a structured knowledge base to improve decision-making and operational efficiency in demining.

Method: TextMine utilizes a prompt-based pipeline integrating document chunking, domain-aware prompting guided by a newly created HMA ontology, triple extraction, and both reference-based and LLM-as-a-Judge evaluation. It processes entire paragraphs to enable coreference resolution and multi-step inference.

Result: Experiments demonstrate that ontology-aligned prompts significantly improve extraction accuracy (44.2%), reduce hallucinations (22.5%), and enhance format conformance (20.9%) compared to baseline methods. The system was validated on Cambodian reports.

Conclusion: TextMine provides a practical and adaptable solution for automated knowledge extraction in the HMA domain, with potential for broader application in other domains. It addresses a critical resource gap and paves the way for more effective and data-driven humanitarian demining operations.

Abstract: Humanitarian Mine Action has generated extensive best-practice knowledge, but much remains locked in unstructured reports. We introduce TextMine, an ontology-guided pipeline that uses Large Language Models to extract knowledge triples from HMA texts. TextMine integrates document chunking, domain-aware prompting, triple extraction, and both reference-based and LLM-as-a-Judge evaluation. We also create the first HMA ontology and a curated dataset of real-world demining reports. Experiments show ontology-aligned prompts boost extraction accuracy by 44.2%, cut hallucinations by 22.5%, and improve format conformance by 20.9% over baselines. While validated on Cambodian reports, TextMine can adapt to global demining efforts or other domains, transforming unstructured data into structured knowledge.

</details>


### [16] [CLEAR: A Comprehensive Linguistic Evaluation of Argument Rewriting by Large Language Models](https://arxiv.org/abs/2509.15027)
*Thomas Huber, Christina Niklaus*

Main category: cs.CL

TL;DR: This paper introduces CLEAR, a comprehensive evaluation pipeline, to analyze how Large Language Models (LLMs) rewrite argumentative texts (ArgImp) across four linguistic levels. The study finds that LLMs primarily shorten texts while increasing average word length and merging sentences, leading to improvements in persuasion and coherence.


<details>
  <summary>Details</summary>
Motivation: While LLMs excel in general text generation, their behavior in text rewriting, particularly in improving argumentative texts (ArgImp), remains underexplored. This research aims to understand the linguistic transformations LLMs perform during ArgImp and identify potential biases.

Method: The researchers developed CLEAR, an evaluation pipeline comprising 57 metrics mapped to lexical, syntactic, semantic, and pragmatic linguistic levels. This pipeline was applied to various argumentation corpora to examine the qualities of LLM-rewritten arguments and compare the behavior of different LLMs.

Result: The analysis revealed that LLMs tend to shorten argumentative texts during rewriting, simultaneously increasing average word length and merging sentences.  Furthermore, the models demonstrated an overall increase in the persuasion and coherence dimensions of the arguments.

Conclusion: CLEAR provides a valuable tool for evaluating LLMs in ArgImp, offering insights into their linguistic rewriting strategies and highlighting the need for further investigation into potential biases. The findings contribute to a better understanding of LLMs' role in enhancing argumentative writing.

Abstract: While LLMs have been extensively studied on general text generation tasks, there is less research on text rewriting, a task related to general text generation, and particularly on the behavior of models on this task. In this paper we analyze what changes LLMs make in a text rewriting setting. We focus specifically on argumentative texts and their improvement, a task named Argument Improvement (ArgImp). We present CLEAR: an evaluation pipeline consisting of 57 metrics mapped to four linguistic levels: lexical, syntactic, semantic and pragmatic. This pipeline is used to examine the qualities of LLM-rewritten arguments on a broad set of argumentation corpora and compare the behavior of different LLMs on this task and analyze the behavior of different LLMs on this task in terms of linguistic levels. By taking all four linguistic levels into consideration, we find that the models perform ArgImp by shortening the texts while simultaneously increasing average word length and merging sentences. Overall we note an increase in the persuasion and coherence dimensions.

</details>


### [17] [Cross-Modal Knowledge Distillation for Speech Large Language Models](https://arxiv.org/abs/2509.14930)
*Enzhi Wang, Qicheng Li, Zhiyuan Tang, Yuhang Jia*

Main category: cs.CL

TL;DR: This paper addresses the performance degradation and knowledge loss in speech large language models (LLMs) when incorporating speech capabilities. It proposes a cross-modal knowledge distillation framework to transfer knowledge from a text-based teacher model to a speech LLM, preserving textual knowledge and improving cross-modal alignment.


<details>
  <summary>Details</summary>
Motivation: Introducing speech capabilities to LLMs often leads to catastrophic forgetting of previously learned knowledge and modality inequivalence, resulting in degraded performance, especially in speech-based interactions. Existing approaches like freezing the backbone often fail to fully restore reasoning capabilities.

Method: The authors propose a cross-modal knowledge distillation framework that leverages both text-to-text and speech-to-text channels. This framework transfers knowledge from a text-based teacher model to a speech LLM, aiming to mitigate catastrophic forgetting and improve cross-modal alignment.

Result: Extensive experiments on dialogue and audio understanding tasks demonstrate that the proposed approach effectively preserves textual knowledge, improves cross-modal alignment, and enhances reasoning in speech-based interactions, leading to improved performance compared to existing methods.

Conclusion: This work provides a systematic evaluation of challenges in speech LLMs and introduces a novel knowledge distillation framework to address them. The findings highlight the importance of cross-modal knowledge transfer for building robust and effective speech-enabled LLMs.

Abstract: In this work, we present the first systematic evaluation of catastrophic forgetting and modality inequivalence in speech large language models, showing that introducing speech capabilities can degrade knowledge and reasoning even when inputs remain textual, and performance further decreases with spoken queries. To address these challenges, we propose a cross-modal knowledge distillation framework that leverages both text-to-text and speech-to-text channels to transfer knowledge from a text-based teacher model to a speech LLM. Extensive experiments on dialogue and audio understanding tasks validate the effectiveness of our approach in preserving textual knowledge, improving cross-modal alignment, and enhancing reasoning in speech-based interactions.

</details>


### [18] [A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation](https://arxiv.org/abs/2509.14886)
*Ye Shen, Junying Wang, Farong Wen, Yijin Guo, Qi Jia, Zicheng Zhang, Guangtao Zhai*

Main category: cs.CL

TL;DR: This paper introduces a 'multi-to-one interview' paradigm for evaluating Multi-Modal Large Language Models (MLLMs) that is more efficient than traditional full-coverage question-answering benchmarks while maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: Conventional MLLM evaluation methods using full-coverage Question-Answering (Q&A) are inefficient due to redundancy. The research aims to develop a more efficient and reliable evaluation paradigm inspired by human interview processes.

Method: The proposed paradigm consists of a two-stage interview strategy (pre-interview and formal interview), dynamic adjustment of interviewer weights for fairness, and an adaptive mechanism for question difficulty selection. It aims to mimic the efficiency of human interviews where a few well-chosen questions can effectively assess a candidate's abilities.

Result: Experiments on MMT-Bench, ScienceQA, and SEED-Bench demonstrate that the multi-to-one interview paradigm significantly improves correlation with full-coverage results (up to 17.6% in PLCC and 16.7% in SRCC) while using fewer questions.

Conclusion: The proposed paradigm offers a reliable and efficient alternative for large-scale MLLM benchmarking, providing a comprehensive, accurate, fair, and efficient evaluation of MLLMs.

Abstract: The rapid progress of Multi-Modal Large Language Models (MLLMs) has spurred the creation of numerous benchmarks. However, conventional full-coverage Question-Answering evaluations suffer from high redundancy and low efficiency. Inspired by human interview processes, we propose a multi-to-one interview paradigm for efficient MLLM evaluation. Our framework consists of (i) a two-stage interview strategy with pre-interview and formal interview phases, (ii) dynamic adjustment of interviewer weights to ensure fairness, and (iii) an adaptive mechanism for question difficulty-level chosen. Experiments on different benchmarks show that the proposed paradigm achieves significantly higher correlation with full-coverage results than random sampling, with improvements of up to 17.6% in PLCC and 16.7% in SRCC, while reducing the number of required questions. These findings demonstrate that the proposed paradigm provides a reliable and efficient alternative for large-scale MLLM benchmarking.

</details>


### [19] [Empathy-R1: A Chain-of-Empathy and Reinforcement Learning Framework for Long-Form Mental Health Support](https://arxiv.org/abs/2509.14851)
*Xianrong Yao, Dong She, Chenxu Zhang, Yimeng Zhang, Yueru Sun, Noman Ahmed, Yang Gao, Zhanpeng Jin*

Main category: cs.CL

TL;DR: Empathy-R1 is a novel framework that combines Chain-of-Empathy reasoning with Reinforcement Learning to improve the quality of AI-powered mental health support for long-form counseling texts (LCTs), particularly in Chinese.


<details>
  <summary>Details</summary>
Motivation: Existing Large Language Models (LLMs) often lack the structured reasoning and therapeutic depth needed to effectively respond to complex mental health concerns expressed in lengthy, detailed posts (LCTs), especially within a Chinese cultural context. There's a need for more interpretable and contextually nuanced AI support.

Method: Empathy-R1 integrates a Chain-of-Empathy (CoE) reasoning process, inspired by cognitive-behavioral therapy, with Reinforcement Learning (RL). It utilizes a new large-scale Chinese dataset (Empathy-QA) and a two-stage training process: Supervised Fine-Tuning followed by RL with a dedicated reward model.

Result: Empathy-R1 achieves strong performance on automatic metrics and, more importantly, demonstrates a clear preference over strong baselines in human evaluations, achieving a Win@1 rate of 44.30% on a new benchmark.

Conclusion: Empathy-R1 represents a significant advancement in developing responsible and genuinely beneficial AI for mental health support by enabling interpretable, contextually nuanced responses and addressing the limitations of existing LLMs in handling complex LCTs.

Abstract: Empathy is critical for effective mental health support, especially when addressing Long Counseling Texts (LCTs). However, existing Large Language Models (LLMs) often generate replies that are semantically fluent but lack the structured reasoning necessary for genuine psychological support, particularly in a Chinese context. To bridge this gap, we introduce Empathy-R1, a novel framework that integrates a Chain-of-Empathy (CoE) reasoning process with Reinforcement Learning (RL) to enhance response quality for LCTs. Inspired by cognitive-behavioral therapy, our CoE paradigm guides the model to sequentially reason about a help-seeker's emotions, causes, and intentions, making its thinking process both transparent and interpretable. Our framework is empowered by a new large-scale Chinese dataset, Empathy-QA, and a two-stage training process. First, Supervised Fine-Tuning instills the CoE's reasoning structure. Subsequently, RL, guided by a dedicated reward model, refines the therapeutic relevance and contextual appropriateness of the final responses. Experiments show that Empathy-R1 achieves strong performance on key automatic metrics. More importantly, human evaluations confirm its superiority, showing a clear preference over strong baselines and achieving a Win@1 rate of 44.30% on our new benchmark. By enabling interpretable and contextually nuanced responses, Empathy-R1 represents a significant advancement in developing responsible and genuinely beneficial AI for mental health support.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [20] [Generalizable Geometric Image Caption Synthesis](https://arxiv.org/abs/2509.15217)
*Yue Xin, Wenyuan Wang, Rui Pan, Ruida Wang, Howard Meng, Renjie Pi, Shizhe Diao, Tong Zhang*

Main category: cs.AI

TL;DR: This paper introduces a novel data synthesis pipeline using Reinforcement Learning with Verifiable Rewards (RLVR) to generate high-quality image-text pairs for geometric reasoning. The generated dataset significantly improves the general reasoning capabilities of multimodal large language models (MLLMs) across various tasks.


<details>
  <summary>Details</summary>
Motivation: Existing datasets for geometric reasoning lack high-quality image-text alignment, hindering MLLMs' ability to develop strong cross-modal reasoning skills. Template-based data synthesis pipelines also struggle to generalize beyond predefined templates.

Method: The pipeline synthesizes geometric images from 50 basic geometric relations and refines captions using RLVR. A reward function incorporates both reasoning and caption quality, derived from mathematical problem-solving tasks, to guide the RL process.

Result: The generated dataset improves MLLMs' accuracy by 2.8%-4.8% in statistics, arithmetic, algebraic, and numerical tasks on MathVista and MathVerse (using non-geometric images), and by 2.4%-3.9% in Art & Design and Tech & Engineering tasks on MMMU.

Conclusion: This work demonstrates the effectiveness of RLVR for generating datasets that enhance the general reasoning capabilities of MLLMs, enabling better generalization to out-of-distribution scenarios and improving performance on a wider range of tasks.

Abstract: Multimodal large language models have various practical applications that demand strong reasoning abilities. Despite recent advancements, these models still struggle to solve complex geometric problems. A key challenge stems from the lack of high-quality image-text pair datasets for understanding geometric images. Furthermore, most template-based data synthesis pipelines typically fail to generalize to questions beyond their predefined templates. In this paper, we bridge this gap by introducing a complementary process of Reinforcement Learning with Verifiable Rewards (RLVR) into the data generation pipeline. By adopting RLVR to refine captions for geometric images synthesized from 50 basic geometric relations and using reward signals derived from mathematical problem-solving tasks, our pipeline successfully captures the key features of geometry problem-solving. This enables better task generalization and yields non-trivial improvements. Furthermore, even in out-of-distribution scenarios, the generated dataset enhances the general reasoning capabilities of multimodal large language models, yielding accuracy improvements of $2.8\%\text{-}4.8\%$ in statistics, arithmetic, algebraic, and numerical tasks with non-geometric input images of MathVista and MathVerse, along with $2.4\%\text{-}3.9\%$ improvements in Art, Design, Tech, and Engineering tasks in MMMU.

</details>


### [21] [A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical Decision-making](https://arxiv.org/abs/2509.14998)
*Xiao Wu, Ting-Zhu Huang, Liang-Jian Deng, Yanyuan Qiao, Imran Razzak, Yutong Xie*

Main category: cs.AI

TL;DR: This paper introduces KAMAC, a novel knowledge-driven adaptive multi-agent collaboration framework for LLMs, enabling dynamic team formation and improved medical decision-making by addressing the limitations of static roles in existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent LLM frameworks for medical decision-making utilize static, pre-assigned roles, hindering adaptability and dynamic knowledge integration. The need for flexible and scalable collaboration in complex clinical scenarios, mirroring multidisciplinary teams, motivates the development of a more adaptive approach.

Method: KAMAC dynamically forms and expands expert teams based on the evolving diagnostic context. It begins with initial expert agents and uses knowledge-driven discussions to identify and recruit additional specialists as needed, finalizing decisions through reviewing updated agent comments.

Result: Experiments on two real-world medical benchmarks demonstrate that KAMAC significantly outperforms both single-agent and advanced multi-agent methods, particularly in complex scenarios like cancer prognosis requiring cross-specialty expertise.

Conclusion: KAMAC offers a significant advancement in LLM-based medical decision-making by enabling adaptive collaboration and dynamic knowledge integration, leading to improved performance in complex clinical scenarios and paving the way for more effective AI-assisted healthcare.

Abstract: Medical decision-making often involves integrating knowledge from multiple clinical specialties, typically achieved through multidisciplinary teams. Inspired by this collaborative process, recent work has leveraged large language models (LLMs) in multi-agent collaboration frameworks to emulate expert teamwork. While these approaches improve reasoning through agent interaction, they are limited by static, pre-assigned roles, which hinder adaptability and dynamic knowledge integration. To address these limitations, we propose KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration framework that enables LLM agents to dynamically form and expand expert teams based on the evolving diagnostic context. KAMAC begins with one or more expert agents and then conducts a knowledge-driven discussion to identify and fill knowledge gaps by recruiting additional specialists as needed. This supports flexible, scalable collaboration in complex clinical scenarios, with decisions finalized through reviewing updated agent comments. Experiments on two real-world medical benchmarks demonstrate that KAMAC significantly outperforms both single-agent and advanced multi-agent methods, particularly in complex clinical scenarios (i.e., cancer prognosis) requiring dynamic, cross-specialty expertise. Our code is publicly available at: https://github.com/XiaoXiao-Woo/KAMAC.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [22] [Forecasting and Visualizing Air Quality from Sky Images with Vision-Language Models](https://arxiv.org/abs/2509.15076)
*Mohammad Saleh Vahdatpour, Maryam Eyvazi, Yanqing Zhang*

Main category: cs.LG

TL;DR: This paper proposes an AI system that predicts air pollution levels and generates realistic visualizations from sky images, addressing limitations of traditional monitoring systems.


<details>
  <summary>Details</summary>
Motivation: Conventional air quality monitoring systems often have limited spatial coverage and accessibility. There's a need for more scalable, cost-effective, and interpretable methods for air quality forecasting, especially in low-resource settings.

Method: The approach combines statistical texture analysis with supervised learning for pollution classification and leverages vision-language models (VLMs) to generate interpretable visualizations of air quality conditions. It integrates sky images with generative modeling to simulate varying pollution levels.

Result: The system effectively estimates pollution levels and synthesizes semantically consistent visuals using a dataset of urban sky images. The generated visuals can be integrated into applications to enhance situational awareness and inform decision-making.

Conclusion: This work demonstrates the potential of using sky images and VLMs for air quality forecasting and visualization, offering a foundation for more transparent and engaging environmental monitoring and decision support systems. Future work will focus on energy-efficient deployment using green CNN architectures and FPGA-based incremental learning.

Abstract: Air pollution remains a critical threat to public health and environmental sustainability, yet conventional monitoring systems are often constrained by limited spatial coverage and accessibility. This paper proposes an AI-driven agent that predicts ambient air pollution levels from sky images and synthesizes realistic visualizations of pollution scenarios using generative modeling. Our approach combines statistical texture analysis with supervised learning for pollution classification, and leverages vision-language model (VLM)-guided image generation to produce interpretable representations of air quality conditions. The generated visuals simulate varying degrees of pollution, offering a foundation for user-facing interfaces that improve transparency and support informed environmental decision-making. These outputs can be seamlessly integrated into intelligent applications aimed at enhancing situational awareness and encouraging behavioral responses based on real-time forecasts. We validate our method using a dataset of urban sky images and demonstrate its effectiveness in both pollution level estimation and semantically consistent visual synthesis. The system design further incorporates human-centered user experience principles to ensure accessibility, clarity, and public engagement in air quality forecasting. To support scalable and energy-efficient deployment, future iterations will incorporate a green CNN architecture enhanced with FPGA-based incremental learning, enabling real-time inference on edge platforms.

</details>


### [23] [Communication Efficient Split Learning of ViTs with Attention-based Double Compression](https://arxiv.org/abs/2509.15058)
*Federico Alvetreti, Jary Pomponi, Paolo Di Lorenzo, Simone Scardapane*

Main category: cs.LG

TL;DR: This paper introduces Attention-based Double Compression (ADC), a novel Split Learning (SL) framework that significantly reduces communication overhead in Vision Transformer (ViT) training by merging similar activations and discarding less important tokens.


<details>
  <summary>Details</summary>
Motivation: Traditional deep learning training requires significant computational resources and data transmission, raising privacy concerns. Split Learning addresses this by partitioning the model between edge devices and a server, but communication bottlenecks remain a challenge.

Method: ADC employs two parallel compression strategies: (1) merging activations of similar samples based on attention scores, and (2) discarding the least meaningful tokens. This approach compresses both activations and gradients without requiring additional tuning or approximations.

Result: Experimental results on CIFAR100 demonstrate that ADC outperforms state-of-the-art SL frameworks across various compression ratios, maintaining high accuracy even under extreme communication constraints.

Conclusion: ADC provides a communication-efficient solution for Split Learning with ViTs, enabling reliable training even with limited bandwidth and paving the way for wider adoption of federated learning in resource-constrained environments.

Abstract: This paper proposes a novel communication-efficient Split Learning (SL) framework, named Attention-based Double Compression (ADC), which reduces the communication overhead required for transmitting intermediate Vision Transformers activations during the SL training process. ADC incorporates two parallel compression strategies. The first one merges samples' activations that are similar, based on the average attention score calculated in the last client layer; this strategy is class-agnostic, meaning that it can also merge samples having different classes, without losing generalization ability nor decreasing final results. The second strategy follows the first and discards the least meaningful tokens, further reducing the communication cost. Combining these strategies not only allows for sending less during the forward pass, but also the gradients are naturally compressed, allowing the whole model to be trained without additional tuning or approximations of the gradients. Simulation results demonstrate that Attention-based Double Compression outperforms state-of-the-art SL frameworks by significantly reducing communication overheads while maintaining high accuracy.

</details>


### [24] [FlowRL: Matching Reward Distributions for LLM Reasoning](https://arxiv.org/abs/2509.15207)
*Xuekai Zhu, Daixuan Cheng, Dinghuai Zhang, Hengli Li, Kaiyan Zhang, Che Jiang, Youbang Sun, Ermo Hua, Yuxin Zuo, Xingtai Lv, Qizheng Zhang, Lin Chen, Fanghao Shao, Bo Xue, Yunchong Song, Zhenjie Yang, Ganqu Cui, Ning Ding, Jianfeng Gao, Xiaodong Liu, Bowen Zhou, Hongyuan Mei, Zhouhan Lin*

Main category: cs.LG

TL;DR: FlowRL is a new reinforcement learning algorithm for LLMs that matches the full reward distribution instead of maximizing rewards, leading to more diverse and generalizable reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing reward-maximizing RL methods for LLMs (like PPO and GRPO) tend to over-optimize dominant reward signals, neglecting less frequent but valid reasoning paths and reducing diversity.

Method: FlowRL transforms scalar rewards into a normalized target distribution using a learnable partition function and minimizes the reverse KL divergence between the policy and this target distribution, effectively balancing the reward distribution.

Result: FlowRL achieves a significant average improvement of 10.0% over GRPO and 5.1% over PPO on math benchmarks, and consistently performs better on code reasoning tasks.

Conclusion: Reward distribution-matching with FlowRL is a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning, addressing the mode-collapse limitations of traditional reward-maximizing approaches.

Abstract: We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning (RL). Recent advanced reasoning models adopt reward-maximizing methods (\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, thus reducing diversity. In contrast, we transform scalar rewards into a normalized target distribution using a learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution. We implement this idea as a flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories. We conduct experiments on math and code reasoning tasks: FlowRL achieves a significant average improvement of $10.0\%$ over GRPO and $5.1\%$ over PPO on math benchmarks, and performs consistently better on code reasoning tasks. These results highlight reward distribution-matching as a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning.

</details>


### [25] [Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation](https://arxiv.org/abs/2509.15194)
*Yujun Zhou, Zhenwen Liang, Haolin Liu, Wenhao Yu, Kishan Panaganti, Linfeng Song, Dian Yu, Xiangliang Zhang, Haitao Mi, Dong Yu*

Main category: cs.LG

TL;DR: This paper introduces EVOL-RL, a novel label-free reinforcement learning method that enables language models to continuously improve without collapsing into repetitive and brittle responses. It balances stability (selection) with variation (novelty) to maintain exploration and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing label-free methods for language model improvement often lead to 'entropy collapse,' where models produce shorter, less diverse, and less robust outputs. The goal is to develop models that can autonomously evolve – continuously learning and improving without labels or external judges, while preserving exploration and generalization ability.

Method: EVOL-RL combines majority voting for stable selection with a novelty-aware reward that encourages responses differing from previously generated ones (measured in semantic space). It utilizes GRPO, asymmetric clipping, and an entropy regularizer to prevent collapse and sustain search. The core principle is 'majority-for-selection + novelty-for-variation'.

Result: EVOL-RL significantly outperforms the majority-only TTRL baseline on various benchmarks (AIME24, AIME25, GPQA), achieving substantial improvements in pass@1 and pass@n metrics. It prevents diversity collapse, unlocks stronger generalization across domains, and even boosts performance in the RLVR setting.

Conclusion: EVOL-RL demonstrates a practical and effective approach to label-free language model evolution, enabling continuous self-improvement and broader applicability across different tasks and settings. This work paves the way for autonomous intelligence by allowing LLMs to learn and adapt from unlabeled data streams.

Abstract: Large language models (LLMs) are increasingly trained with reinforcement learning from verifiable rewards (RLVR), yet real-world deployment demands models that can self-improve without labels or external judges. Existing label-free methods, confidence minimization, self-consistency, or majority-vote objectives, stabilize learning but steadily shrink exploration, causing an entropy collapse: generations become shorter, less diverse, and brittle. Unlike prior approaches such as Test-Time Reinforcement Learning (TTRL), which primarily adapt models to the immediate unlabeled dataset at hand, our goal is broader: to enable general improvements without sacrificing the model's inherent exploration capacity and generalization ability, i.e., evolving. We formalize this issue and propose EVolution-Oriented and Label-free Reinforcement Learning (EVOL-RL), a simple rule that couples stability with variation under a label-free setting. EVOL-RL keeps the majority-voted answer as a stable anchor (selection) while adding a novelty-aware reward that favors responses whose reasoning differs from what has already been produced (variation), measured in semantic space. Implemented with GRPO, EVOL-RL also uses asymmetric clipping to preserve strong signals and an entropy regularizer to sustain search. This majority-for-selection + novelty-for-variation design prevents collapse, maintains longer and more informative chains of thought, and improves both pass@1 and pass@n. EVOL-RL consistently outperforms the majority-only TTRL baseline; e.g., training on label-free AIME24 lifts Qwen3-4B-Base AIME25 pass@1 from TTRL's 4.6% to 16.4%, and pass@16 from 18.5% to 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks stronger generalization across domains (e.g., GPQA). Furthermore, we demonstrate that EVOL-RL also boosts performance in the RLVR setting, highlighting its broad applicability.

</details>


### [26] [Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning](https://arxiv.org/abs/2509.15157)
*Shiwan Zhao, Xuyang Zhao, Jiaming Zhou, Aobo Kong, Qicheng Li, Yong Qin*

Main category: cs.LG

TL;DR: This paper introduces a data rewriting framework for stable off-policy supervised fine-tuning (SFT) of large language models, proactively shrinking the policy gap to improve training stability and performance.


<details>
  <summary>Details</summary>
Motivation: Standard SFT suffers from training instability and high variance due to the policy gap between expert demonstrations and the evolving model policy. Existing methods passively constrain updates, but don't actively address the data distribution mismatch.

Method: The proposed framework rewrites incorrect solutions by prompting the model with the ground-truth solution to re-solve the problem (digest-and-retell), keeping correct solutions as on-policy data.  It falls back to expert demonstrations only when self-solve and re-solve fail.

Result: Experiments on five mathematical reasoning benchmarks demonstrate consistent and significant gains over vanilla SFT and state-of-the-art Dynamic Fine-Tuning (DFT) approaches.

Conclusion: The data rewriting technique effectively aligns the training distribution with the target policy, reducing importance sampling variance and stabilizing off-policy fine-tuning, providing a more robust foundation for SFT and future RL-based approaches.

Abstract: Supervised fine-tuning (SFT) of large language models can be viewed as an off-policy learning problem, where expert demonstrations come from a fixed behavior policy while training aims to optimize a target policy. Importance sampling is the standard tool for correcting this distribution mismatch, but large policy gaps lead to high variance and training instability. Existing approaches mitigate this issue using KL penalties or clipping, which passively constrain updates rather than actively reducing the gap. We propose a simple yet effective data rewriting framework that proactively shrinks the policy gap by keeping correct solutions as on-policy data and rewriting incorrect ones with guided re-solving, falling back to expert demonstrations only when needed. This aligns the training distribution with the target policy before optimization, reducing importance sampling variance and stabilizing off-policy fine-tuning. Experiments on five mathematical reasoning benchmarks demonstrate consistent and significant gains over both vanilla SFT and the state-of-the-art Dynamic Fine-Tuning (DFT) approach. The data and code will be released at https://github.com/NKU-HLT/Off-Policy-SFT.

</details>


### [27] [TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference](https://arxiv.org/abs/2509.15110)
*Dan Zhang, Min Cai, Jonathan Li, Ziniu Hu, Yisong Yue, Yuxiao Dong, Jie Tang*

Main category: cs.LG

TL;DR: TDRM introduces a temporal difference regularization method to train smoother and more reliable reward models for language model reinforcement learning and inference. This improves policy updates and alignment with long-term objectives, leading to significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Existing reward models often lack temporal consistency, leading to unstable training and ineffective policy updates in language model reinforcement learning. This inconsistency makes it difficult to distinguish beneficial reasoning steps.

Method: TDRM minimizes temporal differences during reward model training using n-step temporal difference learning. It can be used as a supplement to verifiable reward methods and incorporated into actor-critic style online RL loops.

Result: TDRM improves performance in both Best-of-N and tree-search settings. When combined with RLVR, it achieves comparable performance with significantly less data (2.5k vs. 50.1k) and yields higher-quality language model policies across multiple model variants.

Conclusion: TDRM provides a simple yet effective way to improve the quality and efficiency of reward models for language models, leading to more stable and data-efficient reinforcement learning and better language model policies.

Abstract: Reward models are central to both reinforcement learning (RL) with language models and inference-time verification. However, existing reward models often lack temporal consistency, leading to ineffective policy updates and unstable RL training. We introduce TDRM, a method for learning smoother and more reliable reward models by minimizing temporal differences during training. This temporal-difference (TD) regularization produces smooth rewards and improves alignment with long-term objectives. Incorporating TDRM into the actor-critic style online RL loop yields consistent empirical gains. It is worth noting that TDRM is a supplement to verifiable reward methods, and both can be used in series. Experiments show that TD-trained process reward models (PRMs) improve performance across Best-of-N (up to 6.6%) and tree-search (up to 23.7%) settings. When combined with Reinforcement Learning with Verifiable Rewards (RLVR), TD-trained PRMs lead to more data-efficient RL -- achieving comparable performance with just 2.5k data to what baseline methods require 50.1k data to attain -- and yield higher-quality language model policies on 8 model variants (5 series), e.g., Qwen2.5-(0.5B, 1,5B), GLM4-9B-0414, GLM-Z1-9B-0414, Qwen2.5-Math-(1.5B, 7B), and DeepSeek-R1-Distill-Qwen-(1.5B, 7B). We release all code at https://github.com/THUDM/TDRM.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [28] [M4Diffuser: Multi-View Diffusion Policy with Manipulability-Aware Control for Robust Mobile Manipulation](https://arxiv.org/abs/2509.14980)
*Ju Dong, Lei Zhang, Liding Zhang, Yao Ling, Yu Fu, Kaixin Bai, Zoltán-Csaba Márton, Zhenshan Bing, Zhaopeng Chen, Alois Christian Knoll, Jianwei Zhang*

Main category: cs.RO

TL;DR: M4Diffuser is a novel hybrid framework for robust mobile manipulation that combines a multi-view diffusion policy for high-level goal generation with a manipulability-aware QP controller for efficient and stable execution.


<details>
  <summary>Details</summary>
Motivation: Existing mobile manipulation approaches struggle with limited fields of view, exploration, generalization, computational inefficiency, and robustness near singularities. Classical controllers lack efficiency and manipulability, while learning-based methods often lack stability and are susceptible to occlusion.

Method: M4Diffuser integrates a Multi-View Diffusion Transformer Policy (leveraging proprioception and multiple camera views) to generate end-effector goals, which are then executed by a Reduced and Manipulability-aware QP (ReM-QP) controller. ReM-QP eliminates slack variables for efficiency and incorporates manipulability preferences for robustness.

Result: Experiments in simulation and real-world environments demonstrate that M4Diffuser achieves 7%–56% higher success rates and reduces collisions by 3%–31% compared to baselines. It exhibits robust whole-body coordination and strong generalization to unseen tasks.

Conclusion: M4Diffuser provides a promising approach for reliable mobile manipulation in unstructured environments by effectively combining the strengths of diffusion policies and optimized control, paving the way for more adaptable and robust robotic systems.

Abstract: Mobile manipulation requires the coordinated control of a mobile base and a robotic arm while simultaneously perceiving both global scene context and fine-grained object details. Existing single-view approaches often fail in unstructured environments due to limited fields of view, exploration, and generalization abilities. Moreover, classical controllers, although stable, struggle with efficiency and manipulability near singularities. To address these challenges, we propose M4Diffuser, a hybrid framework that integrates a Multi-View Diffusion Policy with a novel Reduced and Manipulability-aware QP (ReM-QP) controller for mobile manipulation. The diffusion policy leverages proprioceptive states and complementary camera perspectives with both close-range object details and global scene context to generate task-relevant end-effector goals in the world frame. These high-level goals are then executed by the ReM-QP controller, which eliminates slack variables for computational efficiency and incorporates manipulability-aware preferences for robustness near singularities. Comprehensive experiments in simulation and real-world environments show that M4Diffuser achieves 7 to 56 percent higher success rates and reduces collisions by 3 to 31 percent over baselines. Our approach demonstrates robust performance for smooth whole-body coordination, and strong generalization to unseen tasks, paving the way for reliable mobile manipulation in unstructured environments. Details of the demo and supplemental material are available on our project website https://sites.google.com/view/m4diffuser.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [29] [Learning Mechanistic Subtypes of Neurodegeneration with a Physics-Informed Variational Autoencoder Mixture Model](https://arxiv.org/abs/2509.15124)
*Sanduni Pinnawala, Annabelle Hartanto, Ivor J. A. Simpson, Peter A. Wijeratne*

Main category: eess.IV

TL;DR: This paper introduces BrainPhys, a novel physics-informed variational autoencoder (VAE) mixture model, to learn mixtures of latent dynamic models governed by physics-based partial differential equations (PDEs) for neurodegenerative diseases, addressing limitations of single-PDE approaches.


<details>
  <summary>Details</summary>
Motivation: Current physics-integrated machine learning methods are limited to single PDEs, hindering their application to heterogeneous diseases with multiple underlying mechanisms and exacerbating issues like model misspecification and degeneracy. Neurodegenerative diseases, like Alzheimer's, exhibit subtype variations in progression, necessitating models that can account for this variability.

Method: The authors propose BrainPhys, a deep generative model integrating reaction-diffusion PDEs within a VAE mixture model framework. This allows for inference of subtypes of interpretable latent variables (e.g., diffusivity and reaction rates) from neuroimaging data, effectively learning multiple PDEs simultaneously.

Result: The model successfully recovers clusters of mechanistic models and their parameters using synthetic data.  Application to Alzheimer's Disease Neuroimaging Initiative (ADNI) PET data revealed evidence supporting a two-component mixture model.

Conclusion: BrainPhys offers a powerful approach for uncovering mechanistic subtypes of neurodegenerative diseases, addressing model misspecification and degeneracy challenges, and providing a more nuanced understanding of disease progression from neuroimaging data.

Abstract: Modelling the underlying mechanisms of neurodegenerative diseases demands methods that capture heterogeneous and spatially varying dynamics from sparse, high-dimensional neuroimaging data. Integrating partial differential equation (PDE) based physics knowledge with machine learning provides enhanced interpretability and utility over classic numerical methods. However, current physics-integrated machine learning methods are limited to considering a single PDE, severely limiting their application to diseases where multiple mechanisms are responsible for different groups (i.e., subtypes) and aggravating problems with model misspecification and degeneracy. Here, we present a deep generative model for learning mixtures of latent dynamic models governed by physics-based PDEs, going beyond traditional approaches that assume a single PDE structure. Our method integrates reaction-diffusion PDEs within a variational autoencoder (VAE) mixture model framework, supporting inference of subtypes of interpretable latent variables (e.g. diffusivity and reaction rates) from neuroimaging data. We evaluate our method on synthetic benchmarks and demonstrate its potential for uncovering mechanistic subtypes of Alzheimer's disease progression from positron emission tomography (PET) data.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [30] [WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance](https://arxiv.org/abs/2509.15130)
*Chenxi Song, Yanming Yang, Tong Zhao, Ruibo Li, Chi Zhang*

Main category: cs.GR

TL;DR: WorldForge is a training-free framework that leverages pre-trained video diffusion models for 3D/4D tasks like scene generation and re-rendering, enabling precise camera trajectory control without retraining.


<details>
  <summary>Details</summary>
Motivation: Existing video diffusion models have strong generative priors but lack controllability and geometric consistency for 3D/4D tasks, often requiring retraining which degrades performance and is computationally expensive.

Method: WorldForge introduces three modules: Intra-Step Recursive Refinement for trajectory injection, Flow-Gated Latent Fusion to decouple motion and appearance, and Dual-Path Self-Corrective Guidance to correct trajectory drift. These modules operate during inference without any training.

Result: Extensive experiments demonstrate WorldForge's superiority in realism, trajectory consistency, and visual fidelity across diverse benchmarks, achieving accurate motion control and photorealistic content generation.

Conclusion: WorldForge establishes a novel plug-and-play paradigm for controllable video synthesis, offering a new approach to leverage generative priors for spatial intelligence and enabling applications in novel view synthesis, 3D scene generation, and dynamic scene reconstruction.

Abstract: Recent video diffusion models demonstrate strong potential in spatial intelligence tasks due to their rich latent world priors. However, this potential is hindered by their limited controllability and geometric inconsistency, creating a gap between their strong priors and their practical use in 3D/4D tasks. As a result, current approaches often rely on retraining or fine-tuning, which risks degrading pretrained knowledge and incurs high computational costs. To address this, we propose WorldForge, a training-free, inference-time framework composed of three tightly coupled modules. Intra-Step Recursive Refinement introduces a recursive refinement mechanism during inference, which repeatedly optimizes network predictions within each denoising step to enable precise trajectory injection. Flow-Gated Latent Fusion leverages optical flow similarity to decouple motion from appearance in the latent space and selectively inject trajectory guidance into motion-related channels. Dual-Path Self-Corrective Guidance compares guided and unguided denoising paths to adaptively correct trajectory drift caused by noisy or misaligned structural signals. Together, these components inject fine-grained, trajectory-aligned guidance without training, achieving both accurate motion control and photorealistic content generation. Extensive experiments across diverse benchmarks validate our method's superiority in realism, trajectory consistency, and visual fidelity. This work introduces a novel plug-and-play paradigm for controllable video synthesis, offering a new perspective on leveraging generative priors for spatial intelligence.

</details>
