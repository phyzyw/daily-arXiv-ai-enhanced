{"id": "2510.24616", "title": "Statistical physics of deep learning: Optimal learning of a multi-layer perceptron near interpolation", "authors": ["Jean Barbier", "Francesco Camilli", "Minh-Toan Nguyen", "Mauro Pastore", "Rudy Skerk"], "summary": "For three decades statistical physics has been providing a framework to analyse neural networks. A long-standing question remained on its capacity to tackle deep learning models capturing rich feature learning effects, thus going beyond the narrow networks or kernel methods analysed until now. We positively answer through the study of the supervised learning of a multi-layer perceptron. Importantly, (i) its width scales as the input dimension, making it more prone to feature learning than ultra wide networks, and more expressive than narrow ones or with fixed embedding layers; and (ii) we focus on the challenging interpolation regime where the number of trainable parameters and data are comparable, which forces the model to adapt to the task. We consider the matched teacher-student setting. It provides the fundamental limits of learning random deep neural network targets and helps in identifying the sufficient statistics describing what is learnt by an optimally trained network as the data budget increases. A rich phenomenology emerges with various learning transitions. With enough data optimal performance is attained through model's \"specialisation\" towards the target, but it can be hard to reach for training algorithms which get attracted by sub-optimal solutions predicted by the theory. Specialisation occurs inhomogeneously across layers, propagating from shallow towards deep ones, but also across neurons in each layer. Furthermore, deeper targets are harder to learn. Despite its simplicity, the Bayesian-optimal setting provides insights on how the depth, non-linearity and finite (proportional) width influence neural networks in the feature learning regime that are potentially relevant way beyond it.", "published": "2025-10-28", "categories": ["stat.ML", "cond-mat.dis-nn", "cond-mat.stat-mech", "cs.IT", "cs.LG", "math.IT"], "pdf_url": "http://arxiv.org/pdf/2510.24616v1", "primary_category": "stat.ML"}
{"id": "2510.23810", "title": "A Physics-informed Multi-resolution Neural Operator", "authors": ["Sumanta Roy", "Bahador Bahmani", "Ioannis G. Kevrekidis", "Michael D. Shields"], "summary": "The predictive accuracy of operator learning frameworks depends on the quality and quantity of available training data (input-output function pairs), often requiring substantial amounts of high-fidelity data, which can be challenging to obtain in some real-world engineering applications. These datasets may be unevenly discretized from one realization to another, with the grid resolution varying across samples. In this study, we introduce a physics-informed operator learning approach by extending the Resolution Independent Neural Operator (RINO) framework to a fully data-free setup, addressing both challenges simultaneously. Here, the arbitrarily (but sufficiently finely) discretized input functions are projected onto a latent embedding space (i.e., a vector space of finite dimensions), using pre-trained basis functions. The operator associated with the underlying partial differential equations (PDEs) is then approximated by a simple multi-layer perceptron (MLP), which takes as input a latent code along with spatiotemporal coordinates to produce the solution in the physical space. The PDEs are enforced via a finite difference solver in the physical space. The validation and performance of the proposed method are benchmarked on several numerical examples with multi-resolution data, where input functions are sampled at varying resolutions, including both coarse and fine discretizations.", "published": "2025-10-27", "categories": ["cs.LG", "math.AP", "physics.comp-ph", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2510.23810v1", "primary_category": "cs.LG"}
{"id": "2510.23221", "title": "Accelerating IC Thermal Simulation Data Generation via Block Krylov and Operator Action", "authors": ["Hong Wang", "Wenkai Yang", "Jie Wang", "Huanshuo Dong", "Zijie Geng", "Zhen Huang", "Depeng Xie", "Zhezheng Hao", "Hande Dong"], "summary": "Recent advances in data-driven approaches, such as neural operators (NOs), have shown substantial efficacy in reducing the solution time for integrated circuit (IC) thermal simulations. However, a limitation of these approaches is requiring a large amount of high-fidelity training data, such as chip parameters and temperature distributions, thereby incurring significant computational costs. To address this challenge, we propose a novel algorithm for the generation of IC thermal simulation data, named block Krylov and operator action (BlocKOA), which simultaneously accelerates the data generation process and enhances the precision of generated data. BlocKOA is specifically designed for IC applications. Initially, we use the block Krylov algorithm based on the structure of the heat equation to quickly obtain a few basic solutions. Then we combine them to get numerous temperature distributions that satisfy the physical constraints. Finally, we apply heat operators on these functions to determine the heat source distributions, efficiently generating precise data points. Theoretical analysis shows that the time complexity of BlocKOA is one order lower than the existing method. Experimental results further validate its efficiency, showing that BlocKOA achieves a 420-fold speedup in generating thermal simulation data for 5000 chips with varying physical parameters and IC structures. Even with just 4% of the generation time, data-driven approaches trained on the data generated by BlocKOA exhibits comparable performance to that using the existing method.", "published": "2025-10-27", "categories": ["cs.AI", "physics.comp-ph"], "pdf_url": "http://arxiv.org/pdf/2510.23221v1", "primary_category": "cs.AI"}
