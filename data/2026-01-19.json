{"id": "2601.10885", "title": "Learning collision operators from plasma phase space data using differentiable simulators", "authors": ["Diogo D. Carvalho", "Pablo J. Bilbao", "Warren B. Mori", "Luis O. Silva", "E. Paulo Alves"], "summary": "We propose a methodology to infer collision operators from phase space data of plasma dynamics. Our approach combines a differentiable kinetic simulator, whose core component in this work is a differentiable Fokker-Planck solver, with a gradient-based optimisation method to learn the collisional operators that best describe the phase space dynamics. We test our method using data from two-dimensional Particle-in-Cell simulations of spatially uniform thermal plasmas, and learn the collision operator that captures the self-consistent electromagnetic interaction between finite-size charged particles over a wide variety of simulation parameters. We demonstrate that the learned operators are more accurate than alternative estimates based on particle tracks, while making no prior assumptions about the relevant time-scales of the processes and significantly reducing memory requirements. We find that the retrieved operators, obtained in the non-relativistic regime, are in excellent agreement with theoretical predictions derived for electrostatic scenarios. Our results show that differentiable simulators offer a powerful and computational efficient approach to infer novel operators for a wide rage of problems, such as electromagnetically dominated collisional dynamics and stochastic wave-particle interactions.", "published": "2026-01-15", "categories": ["physics.plasm-ph", "cs.LG", "physics.comp-ph"], "pdf_url": "https://arxiv.org/pdf/2601.10885v1", "primary_category": "physics.plasm-ph"}
{"id": "2601.10962", "title": "Transient learning dynamics drive escape from sharp valleys in Stochastic Gradient Descent", "authors": ["Ning Yang", "Yikuan Zhang", "Qi Ouyang", "Chao Tang", "Yuhai Tu"], "summary": "Stochastic gradient descent (SGD) is central to deep learning, yet the dynamical origin of its preference for flatter, more generalizable solutions remains unclear. Here, by analyzing SGD learning dynamics, we identify a nonequilibrium mechanism governing solution selection. Numerical experiments reveal a transient exploratory phase in which SGD trajectories repeatedly escape sharp valleys and transition toward flatter regions of the loss landscape. By using a tractable physical model, we show that the SGD noise reshapes the landscape into an effective potential that favors flat solutions. Crucially, we uncover a transient freezing mechanism: as training proceeds, growing energy barriers suppress inter-valley transitions and ultimately trap the dynamics within a single basin. Increasing the SGD noise strength delays this freezing, which enhances convergence to flatter minima. Together, these results provide a unified physical framework linking learning dynamics, loss-landscape geometry, and generalization, and suggest principles for the design of more effective optimization algorithms.", "published": "2026-01-16", "categories": ["cs.LG", "cond-mat.dis-nn"], "pdf_url": "https://arxiv.org/pdf/2601.10962v1", "primary_category": "cs.LG"}
