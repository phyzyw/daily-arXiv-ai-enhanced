{"id": "2602.15632", "title": "Neural-POD: A Plug-and-Play Neural Operator Framework for Infinite-Dimensional Functional Nonlinear Proper Orthogonal Decomposition", "authors": ["Changhong Mou", "Binghang Lu", "Guang Lin"], "summary": "The rapid development of AI for Science is often hindered by the \"discretization\", where learned representations remain restricted to the specific grids or resolutions used during training. We propose the Neural Proper Orthogonal Decomposition (Neural-POD), a plug-and-play neural operator framework that constructs nonlinear, orthogonal basis functions in infinite-dimensional space using neural networks. Unlike the classical Proper Orthogonal Decomposition (POD), which is limited to linear subspace approximations obtained through singular value decomposition (SVD), Neural-POD formulates basis construction as a sequence of residual minimization problems solved through neural network training. Each basis function is obtained by learning to represent the remaining structure in the data, following a process analogous to Gram--Schmidt orthogonalization. This neural formulation introduces several key advantages over classical POD: it enables optimization in arbitrary norms (e.g., $L^2$, $L^1$), learns mappings between infinite-dimensional function spaces that is resolution-invariant, generalizes effectively to unseen parameter regimes, and inherently captures nonlinear structures in complex spatiotemporal systems. The resulting basis functions are interpretable, reusable, and enabling integration into both reduced order modeling (ROM) and operator learning frameworks such as deep operator learning (DeepONet). We demonstrate the robustness of Neural-POD with different complex spatiotemporal systems, including the Burgers' and Navier-Stokes equations. We further show that Neural-POD serves as a high performance, plug-and-play bridge between classical Galerkin projection and operator learning that enables consistent integration with both projection-based reduced order models and DeepONet frameworks.", "abs": "", "categories": ["physics.comp-ph", "cs.LG", "math.NA"], "AI": {"tldr": "本文提出了Neural-POD，一种基于神经网络的框架，用于构建无限维空间中的非线性正交基函数，克服了传统POD在分辨率依赖性、非线性捕捉和泛化能力方面的局限性。", "motivation": "传统POD方法在处理不同分辨率、复杂物理场景和非线性问题时存在局限性，例如分辨率依赖性、线性假设限制以及泛化能力不足。", "method": "Neural-POD将基函数构建视为一系列残差最小化问题，通过神经网络训练解决，类似于Gram–Schmidt正交化过程。它利用神经网络学习映射无限维函数空间，并支持在任意范数（如L2, L1）下优化。", "result": "Neural-POD在Burgers方程和Navier-Stokes方程等复杂时空系统中表现出鲁棒性，能够有效捕捉非线性结构，并可无缝集成到降阶模型（ROM）和深度算子学习（DeepONet）框架中。", "conclusion": "Neural-POD提供了一种高性能、可插拔的桥梁，连接了传统的Galerkin投影和算子学习，为构建更强大、更通用的科学计算模型提供了新的途径。"}}
{"id": "2602.15592", "title": "Uni-Flow: a unified autoregressive-diffusion model for complex multiscale flows", "authors": ["Xiao Xue", "Tianyue Yang", "Mingyang Gao", "Leyu Pan", "Maida Wang", "Kewei Zhu", "Shuo Wang", "Jiuling Li", "Marco F. P. ten Eikelder", "Peter V. Coveney"], "summary": "Spatiotemporal flows govern diverse phenomena across physics, biology, and engineering, yet modelling their multiscale dynamics remains a central challenge. Despite major advances in physics-informed machine learning, existing approaches struggle to simultaneously maintain long-term temporal evolution and resolve fine-scale structure across chaotic, turbulent, and physiological regimes. Here, we introduce Uni-Flow, a unified autoregressive-diffusion framework that explicitly separates temporal evolution from spatial refinement for modelling complex dynamical systems. The autoregressive component learns low-resolution latent dynamics that preserve large-scale structure and ensure stable long-horizon rollouts, while the diffusion component reconstructs high-resolution physical fields, recovering fine-scale features in a small number of denoising steps. We validate Uni-Flow across canonical benchmarks, including two-dimensional Kolmogorov flow, three-dimensional turbulent channel inflow generation with a quantum-informed autoregressive prior, and patient-specific simulations of aortic coarctation derived from high-fidelity lattice Boltzmann hemodynamic solvers. In the cardiovascular setting, Uni-Flow enables task-level faster than real-time inference of pulsatile hemodynamics, reconstructing high-resolution pressure fields over physiologically relevant time horizons in seconds rather than hours. By transforming high-fidelity hemodynamic simulation from an offline, HPC-bound process into a deployable surrogate, Uni-Flow establishes a pathway to faster-than-real-time modelling of complex multiscale flows, with broad implications for scientific machine learning in flow physics.", "abs": "", "categories": ["physics.flu-dyn", "cs.LG", "physics.comp-ph"], "AI": {"tldr": "Uni-Flow是一种统一的自回归-扩散模型，通过分离时间演化和空间精化，实现了对复杂多尺度流的快速建模，尤其在心血管模拟中实现了实时甚至超实时推理。", "motivation": "传统数值方法在模拟多尺度流，特别是湍流和生理流时，面临计算成本高昂的挑战。现有的基于物理信息的机器学习方法难以同时保持长时间演化和解析精细结构。", "method": "Uni-Flow框架将自回归部分用于学习低分辨率的潜在动态，保证大尺度结构的稳定性和长时间演化；扩散部分则用于重建高分辨率的物理场，恢复精细特征。该模型结合了量子信息自回归先验，并应用于Kolmogorov流、湍流通道流动和患者特异性主动脉狭窄模拟。", "result": "Uni-Flow在多个基准测试中表现出色，包括实现了在心血管模拟中，在生理相关的时间尺度内以秒为单位重建高分辨率压力场，速度比传统方法快得多。", "conclusion": "Uni-Flow将高保真心血管模拟从离线、HPC依赖的过程转变为可部署的代理，为复杂多尺度流的超实时建模开辟了道路，具有广泛的科学机器学习应用前景。"}}
{"id": "2602.15593", "title": "A unified theory of feature learning in RNNs and DNNs", "authors": ["Jan P. Bauer", "Kirsten Fischer", "Moritz Helias", "Agostina Palmigiano"], "summary": "Recurrent and deep neural networks (RNNs/DNNs) are cornerstone architectures in machine learning. Remarkably, RNNs differ from DNNs only by weight sharing, as can be shown through unrolling in time. How does this structural similarity fit with the distinct functional properties these networks exhibit? To address this question, we here develop a unified mean-field theory for RNNs and DNNs in terms of representational kernels, describing fully trained networks in the feature learning ($μ$P) regime. This theory casts training as Bayesian inference over sequences and patterns, directly revealing the functional implications induced by the RNNs' weight sharing. In DNN-typical tasks, we identify a phase transition when the learning signal overcomes the noise due to randomness in the weights: below this threshold, RNNs and DNNs behave identically; above it, only RNNs develop correlated representations across timesteps. For sequential tasks, the RNNs' weight sharing furthermore induces an inductive bias that aids generalization by interpolating unsupervised time steps. Overall, our theory offers a way to connect architectural structure to functional biases.", "abs": "", "categories": ["cs.LG", "cond-mat.dis-nn"], "AI": {"tldr": "本文提出了一种统一的均场理论，用于描述循环神经网络（RNN）和深度神经网络（DNN）的特征学习，揭示了权重共享如何影响RNN的功能偏好，尤其是在序列任务中。", "motivation": "尽管RNN和DNN在结构上非常相似（RNN可以通过时间展开转化为DNN），但它们在功能上却表现出不同的特性。本文旨在理解这种结构相似性与功能差异之间的关系。", "method": "研究人员开发了一种统一的均场理论，基于表示核来描述训练后的RNN和DNN，并将训练过程视为时间步和模式上的贝叶斯推断。该理论分析了不同信号强度下的特征学习阶段。", "result": "研究发现，对于DNN典型的端点监督任务，当学习信号超过噪声阈值时，RNN和DNN表现出不同的特征学习行为，RNN会发展出时间相关的表示；对于RNN典型的序列任务，RNN的权重共享会产生一种时间一致的诱导偏置，有助于泛化。", "conclusion": "该理论将架构结构与功能偏好联系起来，揭示了权重共享如何影响RNN的学习能力，并强调了任务-模型对齐的重要性，不仅取决于表达能力，还取决于对任务结构的适应性。"}}
