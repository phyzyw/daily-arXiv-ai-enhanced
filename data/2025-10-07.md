<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Probing Geometry of Next Token Prediction Using Cumulant Expansion of the Softmax Entropy](https://arxiv.org/abs/2510.04285)
*Karthik Viswanathan, Sang Eon Park*

Main category: cs.CL

TL;DR: 本文提出了一种基于 softmax 熵的 cumulant 展开框架，用于量化大型语言模型 (LLMs) 在预测下一个 token 时如何内化高阶统计结构。通过分析 cumulant，研究揭示了 LLMs 的学习动态和不同类型内容的处理机制。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明 LLMs 以不同的阶段处理信息，但缺乏一种轻量级且数学严谨的方法来探究其特征学习动态。本文旨在填补这一空白，提供一种新的视角来理解 LLMs 如何学习高阶统计结构。

Method: 研究人员将每个层 logit 分布的 softmax 熵视为其“中心”分布的扰动，推导出了闭合形式的 cumulant 可观测量，以隔离不同阶次的相关性。他们使用 GPT-2 和 Pythia 模型在 Pile-10K prompts 上跟踪这些 cumulants，并分析了结构化和打乱的 prompts 以及训练过程中的变化。

Result: 研究发现：(i) 结构化 prompts 表现出层数随深度变化的特征曲线，而打乱的 prompts 保持平坦，表明 cumulant 曲线依赖于有意义的上下文；(ii) 训练过程中，所有 cumulants 均单调递增并饱和，直接可视化了模型从捕捉方差到学习偏度、峰度和高阶统计结构的演变过程；(iii) 数学 prompts 表现出与通用文本不同的 cumulant 签名，量化了模型对数学和语言内容的不同处理机制。

Conclusion: 本文提出的 cumulant 分析方法是一种轻量级且数学严谨的工具，可用于探究高维神经网络中的特征学习动态，为理解 LLMs 的内部工作机制提供了新的视角。

Abstract: We introduce a cumulant-expansion framework for quantifying how large language models (LLMs) internalize higher-order statistical structure during next-token prediction. By treating the softmax entropy of each layer's logit distribution as a perturbation around its "center" distribution, we derive closed-form cumulant observables that isolate successively higher-order correlations. Empirically, we track these cumulants in GPT-2 and Pythia models on Pile-10K prompts. (i) Structured prompts exhibit a characteristic rise-and-plateau profile across layers, whereas token-shuffled prompts remain flat, revealing the dependence of the cumulant profile on meaningful context. (ii) During training, all cumulants increase monotonically before saturating, directly visualizing the model's progression from capturing variance to learning skew, kurtosis, and higher-order statistical structures. (iii) Mathematical prompts show distinct cumulant signatures compared to general text, quantifying how models employ fundamentally different processing mechanisms for mathematical versus linguistic content. Together, these results establish cumulant analysis as a lightweight, mathematically grounded probe of feature-learning dynamics in high-dimensional neural networks.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [2] [Learning Linear Regression with Low-Rank Tasks in-Context](https://arxiv.org/abs/2510.04548)
*Kaito Takanami, Takashi Takahashi, Yoshiyuki Kabashima*

Main category: cond-mat.dis-nn

TL;DR: 本文分析了在低秩任务下训练的线性注意力模型，揭示了上下文学习（ICL）的运作机制，并发现了泛化误差与任务结构之间的关系。


<details>
  <summary>Details</summary>
Motivation: 现有ICL理论主要关注独立任务，而实际应用中任务通常存在共享结构。本文旨在研究任务共享结构时ICL的行为，并提供对ICL理论的更精确和可解释的分析。

Method: 通过分析线性注意力模型在低秩线性回归任务上的训练，并在高维极限条件下精确求解模型，将ICL预测分解为算法成分和噪声成分。

Result: 研究发现，模型学习了一种利用任务低秩结构的有效算法，有限的预训练数据会产生隐式正则化，并且任务结构导致泛化误差出现尖锐的相变。

Conclusion: 本文提供了一个理解Transformer如何学习任务结构并进行上下文学习的框架，为ICL理论研究提供了新的视角和方法。

Abstract: In-context learning (ICL) is a key building block of modern large language models, yet its theoretical mechanisms remain poorly understood. It is particularly mysterious how ICL operates in real-world applications where tasks have a common structure. In this work, we address this problem by analyzing a linear attention model trained on low-rank regression tasks. Within this setting, we precisely characterize the distribution of predictions and the generalization error in the high-dimensional limit. Moreover, we find that statistical fluctuations in finite pre-training data induce an implicit regularization. Finally, we identify a sharp phase transition of the generalization error governed by task structure. These results provide a framework for understanding how transformers learn to learn the task structure.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [3] [A Universal Deep Learning Force Field for Molecular Dynamic Simulation and Vibrational Spectra Prediction](https://arxiv.org/abs/2510.04227)
*Shengjiao Ji, Yujin Zhang, Zihan Zou, Bin Jiang, Jun Jiang, Yi Luo, Wei Hu*

Main category: physics.chem-ph

TL;DR: 本文提出了一种基于深度学习的通用力场和机器学习分子动力学(MLMD)框架，能够以极高的速度和精度预测红外(IR)和拉曼光谱，显著优于传统的量子化学方法和Ab initio分子动力学。


<details>
  <summary>Details</summary>
Motivation: 传统量子化学方法忽略了非谐效应和核量子效应，而Ab initio分子动力学计算成本过高，难以应用于大规模分子系统。因此，需要一种更高效、更准确的模拟振动光谱的方法。

Method: 研究人员将先前开发的深度等变张量注意力网络(DetaNet)与速度-Verlet积分器相结合，构建了MLMD框架。首先，在QMe14S数据集上训练DetaNet，得到一个通用的、可转移的力场。然后，利用MLMD和环聚合物分子动力学(RPMD)轨迹推导时间相关函数，模拟IR和拉曼光谱。

Result: DetaNet-based MD方法能够准确捕捉非谐效应和核量子效应，预测光谱与实验数据高度一致，计算速度比AIMD快三个数量级。该框架成功应用于更复杂的系统，如分子和无机晶体、分子聚集体和生物大分子，且只需少量微调。

Conclusion: 这项工作提供了一个通用的机器学习力场和张量感知MLMD框架，能够实现对各种分子和材料系统进行快速准确的动态模拟和IR/拉曼光谱预测，为分子识别和结构分析提供了一种强大的工具。

Abstract: Accurate and efficient simulation of infrared (IR) and Raman spectra is essential for molecular identification and structural analysis. Traditional quantum chemistry methods based on the harmonic approximation neglect anharmonicity and nuclear quantum effects, while ab initio molecular dynamics (AIMD) remains computationally expensive. Here, we integrate our deep equivariant tensor attention network (DetaNet) with a velocity-Verlet integrator to enable fast and accurate machine learning molecular dynamics (MLMD) simulations for spectral prediction. Trained on the QMe14S dataset containing energies, forces, dipole moments, and polarizabilities for 186,102 small organic molecules, DetaNet yields a universal and transferable force field with high-order tensor prediction capability. Using time-correlation functions derived from MLMD and ring-polymer molecular dynamics (RPMD) trajectories, we computed IR and Raman spectra that accurately reproduce anharmonic and nuclear quantum effects. Benchmark tests on isolated molecules, including polycyclic aromatic hydrocarbons, demonstrate that the DetaNet-based MD approach achieves near-experimental spectral accuracy with speedups up to three orders of magnitude over AIMD. Furthermore, the framework extends seamlessly to molecular and inorganic crystals, molecular aggregates, and biological macromolecules such as polypeptides with minimal fine-tuning. In all systems, DetaNet maintains high accuracy while significantly reducing computational cost. Overall, this work establishes a universal machine learning force field and tensor-aware MLMD framework that enable fast, accurate, and broadly applicable dynamic simulations and IR/Raman spectral predictions across diverse molecular and material systems.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [4] [Optimal Computation from Fluctuation Responses](https://arxiv.org/abs/2510.03900)
*Jinghao Lyu, Kyle J. Ray, James P. Crutchfield*

Main category: cond-mat.stat-mech

TL;DR: 本文提出了一种基于涨落响应关系（FRR）和机器学习的统一框架，用于设计物理信息处理系统中热力学效率最高的控制协议，以最小化能量成本并确保正确的计算结果。


<details>
  <summary>Details</summary>
Motivation: 计算消耗大量能量，寻找在物理系统中高效执行计算并最小化能量成本的方法是当前研究的重要挑战。传统的优化方法要么只关注分布，要么只关注协议，缺乏统一的优化框架。

Method: 该框架利用涨落响应关系（FRR）推导梯度，将分布和协议优化统一起来。通过迭代学习采样的噪声轨迹，构建能量成本与任务误差之间的权衡损失函数，从而优化控制协议。

Result: 该框架在双势阱、谐振子等经典例子中，实现了理论最优协议或与有限时间界限相当的能量成本。该方法也适用于欠阻尼系统，并成功优化了位翻转。

Conclusion: 本文提供了一种设计热力学高效协议的原则性策略，可应用于量子门、化学网络和合成生物学等领域，为在物理信息处理系统中实现节能计算提供了新的思路。

Abstract: The energy cost of computation has emerged as a central challenge at the intersection of physics and computer science. Recent advances in statistical physics -- particularly in stochastic thermodynamics -- enable precise characterizations of work, heat, and entropy production in information-processing systems driven far from equilibrium by time-dependent control protocols. A key open question is then how to design protocols that minimize thermodynamic cost while ensur- ing correct outcomes. To this end, we develop a unified framework to identify optimal protocols using fluctuation response relations (FRR) and machine learning. Unlike previous approaches that optimize either distributions or protocols separately, our method unifies both using FRR-derived gradients. Moreover, our method is based primarily on iteratively learning from sampled noisy trajectories, which is generally much easier than solving for the optimal protocol directly from a set of governing equations. We apply the framework to canonical examples -- bit erasure in a double-well potential and translating harmonic traps -- demonstrating how to construct loss functions that trade-off energy cost against task error. The framework extends trivially to underdamped systems, and we show this by optimizing a bit-flip in an underdamped system. In all computations we test, the framework achieves the theoretically optimal protocol or achieves work costs comparable to relevant finite time bounds. In short, the results provide principled strategies for designing thermodynamically efficient protocols in physical information-processing systems. Applications range from quantum gates robust under noise to energy-efficient control of chemical and synthetic biological networks.

</details>
