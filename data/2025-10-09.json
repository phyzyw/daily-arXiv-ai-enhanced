{"id": "2510.06361", "title": "Diffusion-Guided Renormalization of Neural Systems via Tensor Networks", "authors": ["Nathan X. Kodama"], "summary": "Far from equilibrium, neural systems self-organize across multiple scales. Exploiting multiscale self-organization in neuroscience and artificial intelligence requires a computational framework for modeling the effective non-equilibrium dynamics of stochastic neural trajectories. Non-equilibrium thermodynamics and representational geometry offer theoretical foundations, but we need scalable data-driven techniques for modeling collective properties of high-dimensional neural networks from partial subsampled observations. Renormalization is a coarse-graining technique central to studying emergent scaling properties of many-body and nonlinear dynamical systems. While widely applied in physics and machine learning, coarse-graining complex dynamical networks remains unsolved, affecting many computational sciences. Recent diffusion-based renormalization, inspired by quantum statistical mechanics, coarse-grains networks near entropy transitions marked by maximal changes in specific heat or information transmission. Here I explore diffusion-based renormalization of neural systems by generating symmetry-breaking representations across scales and offering scalable algorithms using tensor networks. Diffusion-guided renormalization bridges microscale and mesoscale dynamics of dissipative neural systems. For microscales, I developed a scalable graph inference algorithm for discovering community structure from subsampled neural activity. Using community-based node orderings, diffusion-guided renormalization generates renormalization group flow through metagraphs and joint probability functions. Towards mesoscales, diffusion-guided renormalization targets learning the effective non-equilibrium dynamics of dissipative neural trajectories occupying lower-dimensional subspaces, enabling coarse-to-fine control in systems neuroscience and artificial intelligence.", "published": "2025-10-07", "categories": ["q-bio.NC", "cond-mat.stat-mech", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/2510.06361v1", "primary_category": "q-bio.NC"}
{"id": "2510.06367", "title": "Lagrangian neural ODEs: Measuring the existence of a Lagrangian with Helmholtz metrics", "authors": ["Luca Wolf", "Tobias Buck", "Bjoern Malte Schaefer"], "summary": "Neural ODEs are a widely used, powerful machine learning technique in particular for physics. However, not every solution is physical in that it is an Euler-Lagrange equation. We present Helmholtz metrics to quantify this resemblance for a given ODE and demonstrate their capabilities on several fundamental systems with noise. We combine them with a second order neural ODE to form a Lagrangian neural ODE, which allows to learn Euler-Lagrange equations in a direct fashion and with zero additional inference cost. We demonstrate that, using only positional data, they can distinguish Lagrangian and non-Lagrangian systems and improve the neural ODE solutions.", "published": "2025-10-07", "categories": ["cs.LG", "math.DS", "physics.comp-ph", "physics.data-an"], "pdf_url": "http://arxiv.org/pdf/2510.06367v1", "primary_category": "cs.LG"}
{"id": "2510.06286", "title": "Mass Conservation on Rails -- Rethinking Physics-Informed Learning of Ice Flow Vector Fields", "authors": ["Kim Bente", "Roman Marchant", "Fabio Ramos"], "summary": "To reliably project future sea level rise, ice sheet models require inputs that respect physics. Embedding physical principles like mass conservation into models that interpolate Antarctic ice flow vector fields from sparse & noisy measurements not only promotes physical adherence but can also improve accuracy and robustness. While physics-informed neural networks (PINNs) impose physics as soft penalties, offering flexibility but no physical guarantees, we instead propose divergence-free neural networks (dfNNs), which enforce local mass conservation exactly via a vector calculus trick. Our comparison of dfNNs, PINNs, and unconstrained NNs on ice flux interpolation over Byrd Glacier suggests that \"mass conservation on rails\" yields more reliable estimates, and that directional guidance, a learning strategy leveraging continent-wide satellite velocity data, boosts performance across models.", "published": "2025-10-07", "categories": ["physics.ao-ph", "cs.LG", "physics.comp-ph", "physics.flu-dyn", "physics.geo-ph", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2510.06286v1", "primary_category": "physics.ao-ph"}
{"id": "2510.07286", "title": "Evolutionary Profiles for Protein Fitness Prediction", "authors": ["Jigang Fan", "Xiaoran Jiao", "Shengdong Lin", "Zhanming Liang", "Weian Mao", "Chenchen Jing", "Hao Chen", "Chunhua Shen"], "summary": "Predicting the fitness impact of mutations is central to protein engineering but constrained by limited assays relative to the size of sequence space. Protein language models (pLMs) trained with masked language modeling (MLM) exhibit strong zero-shot fitness prediction; we provide a unifying view by interpreting natural evolution as implicit reward maximization and MLM as inverse reinforcement learning (IRL), in which extant sequences act as expert demonstrations and pLM log-odds serve as fitness estimates. Building on this perspective, we introduce EvoIF, a lightweight model that integrates two complementary sources of evolutionary signal: (i) within-family profiles from retrieved homologs and (ii) cross-family structural-evolutionary constraints distilled from inverse folding logits. EvoIF fuses sequence-structure representations with these profiles via a compact transition block, yielding calibrated probabilities for log-odds scoring. On ProteinGym (217 mutational assays; >2.5M mutants), EvoIF and its MSA-enabled variant achieve state-of-the-art or competitive performance while using only 0.15% of the training data and fewer parameters than recent large models. Ablations confirm that within-family and cross-family profiles are complementary, improving robustness across function types, MSA depths, taxa, and mutation depths. The codes will be made publicly available at https://github.com/aim-uofa/EvoIF.", "published": "2025-10-08", "categories": ["cs.LG", "cs.AI", "q-bio.BM", "q-bio.QM"], "pdf_url": "http://arxiv.org/pdf/2510.07286v1", "primary_category": "cs.LG"}
{"id": "2510.06945", "title": "Fisher Information, Training and Bias in Fourier Regression Models", "authors": ["Lorenzo Pastori", "Veronika Eyring", "Mierk Schwabe"], "summary": "Motivated by the growing interest in quantum machine learning, in particular quantum neural networks (QNNs), we study how recently introduced evaluation metrics based on the Fisher information matrix (FIM) are effective for predicting their training and prediction performance. We exploit the equivalence between a broad class of QNNs and Fourier models, and study the interplay between the \\emph{effective dimension} and the \\emph{bias} of a model towards a given task, investigating how these affect the model's training and performance. We show that for a model that is completely agnostic, or unbiased, towards the function to be learned, a higher effective dimension likely results in a better trainability and performance. On the other hand, for models that are biased towards the function to be learned a lower effective dimension is likely beneficial during training. To obtain these results, we derive an analytical expression of the FIM for Fourier models and identify the features controlling a model's effective dimension. This allows us to construct models with tunable effective dimension and bias, and to compare their training. We furthermore introduce a tensor network representation of the considered Fourier models, which could be a tool of independent interest for the analysis of QNN models. Overall, these findings provide an explicit example of the interplay between geometrical properties, model-task alignment and training, which are relevant for the broader machine learning community.", "published": "2025-10-08", "categories": ["cs.LG", "cond-mat.dis-nn", "physics.data-an", "quant-ph"], "pdf_url": "http://arxiv.org/pdf/2510.06945v1", "primary_category": "cs.LG"}
