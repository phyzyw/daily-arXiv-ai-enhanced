{"id": "2602.03546", "title": "How to Train Your Resistive Network: Generalized Equilibrium Propagation and Analytical Learning", "authors": ["Jonathan Lin", "Aman Desai", "Frank Barrows", "Francesco Caravelli"], "summary": "Machine learning is a powerful method of extracting meaning from data; unfortunately, current digital hardware is extremely energy-intensive. There is interest in an alternative analog computing implementation that could match the performance of traditional machine learning while being significantly more energy-efficient. However, it remains unclear how to train such analog computing systems while adhering to locality constraints imposed by the physical (as opposed to digital) nature of these systems. Local learning algorithms such as Equilibrium Propagation and Coupled Learning have been proposed to address this issue. In this paper, we develop an algorithm to exactly calculate gradients using a graph theoretic and analytical framework for Kirchhoff's laws. We also introduce Generalized Equilibrium Propagation, a framework encompassing a broad class of Hebbian learning algorithms, including Coupled Learning and Equilibrium Propagation, and show how our algorithm compares. We demonstrate our algorithm using numerical simulations and show that we can train resistor networks without the need for a replica or readout over all resistors, only at the output layer. We also show that under the analytical gradient approach, it is possible to update only a subset of the resistance values without a strong degradation in performance.", "abs": "", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.mes-hall", "cond-mat.soft", "cs.ET"], "AI": {"tldr": "本文提出了一种精确计算电阻网络梯度的算法，并引入了广义平衡传播（GEP）框架，统一了平衡传播和耦合学习，无需复制网络或精确控制所有边电阻。", "motivation": "现代机器学习的能耗问题日益突出，寻找更节能的替代方案，例如基于物理的模拟计算，但物理系统存在局部性约束，难以进行梯度计算。", "method": "利用图论和解析框架，基于基尔霍夫定律精确计算电阻网络中电阻的梯度。引入广义平衡传播（GEP）框架，将平衡传播和耦合学习统一起来，并通过数值模拟验证算法的有效性。", "result": "通过数值模拟，证明了可以在无需复制网络或精确控制所有边电阻的情况下训练电阻网络，并展示了分析电路梯度与传统两阶段估计器的比较。", "conclusion": "该算法为设计和优化基于物理的机器学习系统提供了新的思路，特别适用于新兴的“学习机器”观点，即硬件、动力学和学习规则协同设计。"}}
{"id": "2602.02788", "title": "Structure-Preserving Learning Improves Geometry Generalization in Neural PDEs", "authors": ["Benjamin D. Shaffer", "Shawn Koohy", "Brooks Kinch", "M. Ani Hsieh", "Nathaniel Trask"], "summary": "We aim to develop physics foundation models for science and engineering that provide real-time solutions to Partial Differential Equations (PDEs) which preserve structure and accuracy under adaptation to unseen geometries. To this end, we introduce General-Geometry Neural Whitney Forms (Geo-NeW): a data-driven finite element method. We jointly learn a differential operator and compatible reduced finite element spaces defined on the underlying geometry. The resulting model is solved to generate predictions, while exactly preserving physical conservation laws through Finite Element Exterior Calculus. Geometry enters the model as a discretized mesh both through a transformer-based encoding and as the basis for the learned finite element spaces. This explicitly connects the underlying geometry and imposed boundary conditions to the solution, providing a powerful inductive bias for learning neural PDEs, which we demonstrate improves generalization to unseen domains. We provide a novel parameterization of the constitutive model ensuring the existence and uniqueness of the solution. Our approach demonstrates state-of-the-art performance on several steady-state PDE benchmarks, and provides a significant improvement over conventional baselines on out-of-distribution geometries.", "abs": "", "categories": ["cs.LG", "cs.AI", "physics.comp-ph"], "AI": {"tldr": "本文提出了General-Geometry Neural Whitney Forms (Geo-NeW)，一种数据驱动的有限元方法，通过联合学习微分算子和有限元空间，实现了对复杂几何形状的偏微分方程(PDE)解的结构保持和几何泛化。", "motivation": "为了构建适用于科学和工程领域，能够实时求解偏微分方程并保持结构和准确性的物理基础模型，特别是在面对不同几何形状时，需要解决复杂几何形状对科学基础模型部署的限制。", "method": "Geo-NeW通过以下方式实现：1) 使用Transformer编码结合坐标无关描述符和距离场来条件化几何形状；2) 联合学习微分算子和兼容的降阶有限元空间；3) 利用有限元外微分(FEEC)确保物理守恒定律；4) 在推理时，生成一个降阶的传统有限元系统，可以在新网格上实时求解。", "result": "Geo-NeW在多个稳态PDE基准测试中表现出最先进的性能，并且在分布外几何形状上比传统基线方法有了显著改进。", "conclusion": "Geo-NeW通过显式地连接底层几何形状和施加的边界条件，为学习神经网络PDE提供了强大的归纳偏差，从而显著提高了对未知域的泛化能力，为科学基础模型的发展提供了新的思路。"}}
{"id": "2602.03031", "title": "Physics-inspired transformer quantum states via latent imaginary-time evolution", "authors": ["Kimihiro Yamazaki", "Itsushi Sakata", "Takuya Konishi", "Yoshinobu Kawahara"], "summary": "Neural quantum states (NQS) are powerful ansätze in the variational Monte Carlo framework, yet their architectures are often treated as black boxes. We propose a physically transparent framework in which NQS are treated as neural approximations to latent imaginary-time evolution. This viewpoint suggests that standard Transformer-based NQS (TQS) architectures correspond to physically unmotivated effective Hamiltonians dependent on imaginary time in a latent space. Building on this interpretation, we introduce physics-inspired transformer quantum states (PITQS), which enforce a static effective Hamiltonian by sharing weights across layers and improve propagation accuracy via Trotter-Suzuki decompositions without increasing the number of variational parameters. For the frustrated $J_1$-$J_2$ Heisenberg model, our ansätze achieve accuracies comparable to or exceeding state-of-the-art TQS while using substantially fewer variational parameters. This study demonstrates that reinterpreting the deep network structure as a latent cooling process enables a more physically grounded, systematic, and compact design, thereby bridging the gap between black-box expressivity and physically transparent construction.", "abs": "", "categories": ["cond-mat.dis-nn", "cs.LG", "quant-ph"], "AI": {"tldr": "本文提出了一种基于潜伏时程演化的神经网络量子态(NQS)框架，并在此基础上发展了物理启发式Transformer量子态(PITQS)，通过共享权重和Trotter-Suzuki分解，在减少参数数量的同时，实现了与先进TQS相当或更好的精度。", "motivation": "现有神经网络量子态(NQS)，尤其是Transformer量子态(TQS)通常被视为黑盒模型，缺乏物理透明性，且需要大量的参数才能达到良好的精度。本文旨在弥合黑盒表达能力和物理透明构建之间的差距。", "method": "本文将NQS视为潜伏时程演化的神经网络近似，并在此基础上提出了物理启发式Transformer量子态(PITQS)。PITQS通过共享权重来强制执行静态有效哈密顿量，并通过Trotter-Suzuki分解提高传播精度，同时避免增加参数数量。", "result": "在受挫的J1-J2海森堡模型上，PITQS实现了与最先进的TQS相当或更高的精度，同时使用了显著更少的参数。", "conclusion": "本文的研究表明，将深度网络结构重新解释为潜伏冷却过程，可以实现更具物理基础、系统性和紧凑性的设计，为神经网络量子态的设计提供了一种新的思路。"}}
{"id": "2602.02855", "title": "When pre-training hurts LoRA fine-tuning: a dynamical analysis via single-index models", "authors": ["Gibbs Nwemadji", "Bruno Loureiro", "Jean Barbier"], "summary": "Pre-training on a source task is usually expected to facilitate fine-tuning on similar downstream problems. In this work, we mathematically show that this naive intuition is not always true: excessive pre-training can computationally slow down fine-tuning optimization. We study this phenomenon for low-rank adaptation (LoRA) fine-tuning on single-index models trained under one-pass SGD. Leveraging a summary statistics description of the fine-tuning dynamics, we precisely characterize how the convergence rate depends on the initial fine-tuning alignment and the degree of non-linearity of the target task. The key take away is that even when the pre-training and down- stream tasks are well aligned, strong pre-training can induce a prolonged search phase and hinder convergence. Our theory thus provides a unified picture of how pre-training strength and task difficulty jointly shape the dynamics and limitations of LoRA fine-tuning in a nontrivial tractable model.", "abs": "", "categories": ["cs.LG", "cond-mat.dis-nn", "math.ST"], "AI": {"tldr": "本文数学证明了过度的预训练可能会减慢LoRA微调的速度，即使预训练和下游任务对齐良好。研究揭示了预训练强度和任务难度如何共同影响LoRA微调的动态和局限性。", "motivation": "尽管预训练模型通常被认为可以促进下游任务的微调，但本文旨在研究预训练强度对LoRA微调的影响，并解释为什么过度预训练有时会阻碍优化。", "method": "研究人员使用单索引模型，并利用对微调动态的统计描述，精确地刻画了收敛速率与初始微调对齐以及目标任务非线性之间的依赖关系。研究基于一趟SGD训练。", "result": "研究结果表明，即使预训练和下游任务对齐良好，强大的预训练也可能导致搜索阶段延长并阻碍收敛。", "conclusion": "本文提供了一个统一的视角，阐释了预训练强度和任务难度如何共同塑造LoRA微调的动态和局限性，为理解LoRA的优化行为提供了理论基础。"}}
