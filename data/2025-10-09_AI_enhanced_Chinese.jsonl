{"id": "2510.06361", "title": "Diffusion-Guided Renormalization of Neural Systems via Tensor Networks", "authors": ["Nathan X. Kodama"], "summary": "Far from equilibrium, neural systems self-organize across multiple scales. Exploiting multiscale self-organization in neuroscience and artificial intelligence requires a computational framework for modeling the effective non-equilibrium dynamics of stochastic neural trajectories. Non-equilibrium thermodynamics and representational geometry offer theoretical foundations, but we need scalable data-driven techniques for modeling collective properties of high-dimensional neural networks from partial subsampled observations. Renormalization is a coarse-graining technique central to studying emergent scaling properties of many-body and nonlinear dynamical systems. While widely applied in physics and machine learning, coarse-graining complex dynamical networks remains unsolved, affecting many computational sciences. Recent diffusion-based renormalization, inspired by quantum statistical mechanics, coarse-grains networks near entropy transitions marked by maximal changes in specific heat or information transmission. Here I explore diffusion-based renormalization of neural systems by generating symmetry-breaking representations across scales and offering scalable algorithms using tensor networks. Diffusion-guided renormalization bridges microscale and mesoscale dynamics of dissipative neural systems. For microscales, I developed a scalable graph inference algorithm for discovering community structure from subsampled neural activity. Using community-based node orderings, diffusion-guided renormalization generates renormalization group flow through metagraphs and joint probability functions. Towards mesoscales, diffusion-guided renormalization targets learning the effective non-equilibrium dynamics of dissipative neural trajectories occupying lower-dimensional subspaces, enabling coarse-to-fine control in systems neuroscience and artificial intelligence.", "abs": "", "categories": ["q-bio.NC", "cond-mat.stat-mech", "cs.LG"], "AI": {"tldr": "本文提出了一种基于扩散的重整化方法，用于模拟非平衡神经网络的动态，并利用张量网络实现可扩展的算法。", "motivation": "远非平衡状态下，神经网络通过自组织实现高效的信息处理和对外部环境的鲁棒适应。需要一种计算框架来模拟非平衡神经网络的动态，并解决复杂动态网络粗粒化的难题。", "method": "该方法基于扩散重整化，灵感来源于量子统计力学，在熵转变附近进行粗粒化。通过开发可扩展的图推理算法发现社区结构，并利用基于社区的节点排序，生成重整化群流，最终学习非平衡神经网络的有效动态。", "result": "成功地将微观扩散与中观神经网络动态联系起来，实现了多尺度对称性破坏表示，并生成了可扩展的算法。", "conclusion": "该研究为系统神经科学和人工智能中的粗粒化预测问题提供了新的方法，并为理解和建模复杂神经网络的涌现行为提供了理论基础。"}}
{"id": "2510.06367", "title": "Lagrangian neural ODEs: Measuring the existence of a Lagrangian with Helmholtz metrics", "authors": ["Luca Wolf", "Tobias Buck", "Bjoern Malte Schaefer"], "summary": "Neural ODEs are a widely used, powerful machine learning technique in particular for physics. However, not every solution is physical in that it is an Euler-Lagrange equation. We present Helmholtz metrics to quantify this resemblance for a given ODE and demonstrate their capabilities on several fundamental systems with noise. We combine them with a second order neural ODE to form a Lagrangian neural ODE, which allows to learn Euler-Lagrange equations in a direct fashion and with zero additional inference cost. We demonstrate that, using only positional data, they can distinguish Lagrangian and non-Lagrangian systems and improve the neural ODE solutions.", "abs": "", "categories": ["cs.LG", "math.DS", "physics.comp-ph", "physics.data-an"], "AI": {"tldr": "本文提出了一种新的方法，即Lagrangian neural ODEs，通过Helmholtz指标量化ODE与Euler-Lagrange方程的相似度，从而学习Euler-Lagrange方程，并改进神经网络ODE的解。", "motivation": "传统的神经网络ODE虽然强大，但并非所有解都是物理上的Euler-Lagrange方程。为了使神经网络ODE更接近物理规律，需要一种方法来判断和约束ODE是否满足静止作用原理。", "method": "本文结合Helmholtz指标和二阶神经网络ODE，构建Lagrangian neural ODE。Helmholtz指标用于量化ODE与Euler-Lagrange方程的相似度，并将其作为正则化项，引导神经网络ODE学习Euler-Lagrange方程。", "result": "实验表明，Lagrangian neural ODEs仅使用位置数据即可区分Lagrangian和非Lagrangian系统，并能提高神经网络ODE的解的准确性。该方法在训练过程中无需计算或反向传播Euler-Lagrange方程，具有更高的稳定性和更低的计算成本。", "conclusion": "Lagrangian neural ODEs提供了一种有效且高效的方法来学习Euler-Lagrange方程，并能更好地逼近物理系统。该方法有望在物理建模和科学计算领域得到广泛应用，并能帮助识别不满足静止作用原理的系统，从而揭示其潜在的物理意义。"}}
{"id": "2510.06286", "title": "Mass Conservation on Rails -- Rethinking Physics-Informed Learning of Ice Flow Vector Fields", "authors": ["Kim Bente", "Roman Marchant", "Fabio Ramos"], "summary": "To reliably project future sea level rise, ice sheet models require inputs that respect physics. Embedding physical principles like mass conservation into models that interpolate Antarctic ice flow vector fields from sparse & noisy measurements not only promotes physical adherence but can also improve accuracy and robustness. While physics-informed neural networks (PINNs) impose physics as soft penalties, offering flexibility but no physical guarantees, we instead propose divergence-free neural networks (dfNNs), which enforce local mass conservation exactly via a vector calculus trick. Our comparison of dfNNs, PINNs, and unconstrained NNs on ice flux interpolation over Byrd Glacier suggests that \"mass conservation on rails\" yields more reliable estimates, and that directional guidance, a learning strategy leveraging continent-wide satellite velocity data, boosts performance across models.", "abs": "", "categories": ["physics.ao-ph", "cs.LG", "physics.comp-ph", "physics.flu-dyn", "physics.geo-ph", "stat.ML"], "AI": {"tldr": "本文提出了一种新的神经网络模型（dfNN），通过精确执行质量守恒定律来插值南极冰流矢量场，并证明其在冰流插值中优于传统的物理信息神经网络（PINNs）和无约束神经网络。", "motivation": "现有冰盖模型需要尊重物理规律才能可靠地预测海平面上升。传统的物理信息神经网络（PINNs）虽然灵活，但无法保证物理一致性，尤其是在数据稀疏和噪声的情况下。因此，需要一种能够精确执行质量守恒定律的模型。", "method": "研究人员提出了一种基于向量微积分技巧的无源神经网络（dfNN），该模型精确地执行质量守恒定律。他们将dfNN、PINNs和无约束NN在Byrd冰川上的冰流插值中进行了比较，并引入了“方向引导”学习策略，利用洲际卫星速度数据来提升模型性能。", "result": "实验结果表明，“质量守恒在轨道上”的dfNN模型能够产生更可靠的冰流估计，并且方向引导学习策略能够提升所有模型的性能。", "conclusion": "dfNN模型提供了一种更可靠且物理上一致的方法来插值冰流矢量场，对于准确预测海平面上升至关重要。该研究表明，精确执行物理约束比使用软惩罚方法更有效，尤其是在数据有限的情况下。"}}
{"id": "2510.07286", "title": "Evolutionary Profiles for Protein Fitness Prediction", "authors": ["Jigang Fan", "Xiaoran Jiao", "Shengdong Lin", "Zhanming Liang", "Weian Mao", "Chenchen Jing", "Hao Chen", "Chunhua Shen"], "summary": "Predicting the fitness impact of mutations is central to protein engineering but constrained by limited assays relative to the size of sequence space. Protein language models (pLMs) trained with masked language modeling (MLM) exhibit strong zero-shot fitness prediction; we provide a unifying view by interpreting natural evolution as implicit reward maximization and MLM as inverse reinforcement learning (IRL), in which extant sequences act as expert demonstrations and pLM log-odds serve as fitness estimates. Building on this perspective, we introduce EvoIF, a lightweight model that integrates two complementary sources of evolutionary signal: (i) within-family profiles from retrieved homologs and (ii) cross-family structural-evolutionary constraints distilled from inverse folding logits. EvoIF fuses sequence-structure representations with these profiles via a compact transition block, yielding calibrated probabilities for log-odds scoring. On ProteinGym (217 mutational assays; >2.5M mutants), EvoIF and its MSA-enabled variant achieve state-of-the-art or competitive performance while using only 0.15% of the training data and fewer parameters than recent large models. Ablations confirm that within-family and cross-family profiles are complementary, improving robustness across function types, MSA depths, taxa, and mutation depths. The codes will be made publicly available at https://github.com/aim-uofa/EvoIF.", "abs": "", "categories": ["cs.LG", "cs.AI", "q-bio.BM", "q-bio.QM"], "AI": {"tldr": "本文提出了一种轻量级模型EvoIF，它结合了同系物谱和跨家族结构-进化约束，实现了在蛋白质健身预测任务上的卓越性能，同时显著减少了训练数据和模型参数。", "motivation": "蛋白质健身预测是蛋白质工程的关键，但实验数据稀缺限制了预测的准确性。现有的蛋白质语言模型（pLMs）在零样本健身预测方面表现出强大的能力，但缺乏合理的解释，且大型模型计算成本高昂。", "method": "EvoIF模型将自然进化视为隐式奖励最大化，并将MLM视为逆强化学习。它融合了来自检索到的同源物内的家族谱和来自逆折叠logits提炼的跨家族结构-进化约束，通过紧凑的过渡块将序列-结构表示与这些谱融合，从而生成校准的对数概率评分。", "result": "在ProteinGym数据集上，EvoIF及其MSA-enabled变体实现了最先进或具有竞争力的性能，仅使用了0.15%的训练数据和比最近的大型模型更少的参数。消融实验证实了同系物谱和跨家族谱的互补性。", "conclusion": "EvoIF提供了一种轻量级且有效的蛋白质健身预测方法，它通过整合进化信号，提高了模型的鲁棒性和泛化能力，为蛋白质工程和设计提供了有价值的工具。"}}
{"id": "2510.06945", "title": "Fisher Information, Training and Bias in Fourier Regression Models", "authors": ["Lorenzo Pastori", "Veronika Eyring", "Mierk Schwabe"], "summary": "Motivated by the growing interest in quantum machine learning, in particular quantum neural networks (QNNs), we study how recently introduced evaluation metrics based on the Fisher information matrix (FIM) are effective for predicting their training and prediction performance. We exploit the equivalence between a broad class of QNNs and Fourier models, and study the interplay between the \\emph{effective dimension} and the \\emph{bias} of a model towards a given task, investigating how these affect the model's training and performance. We show that for a model that is completely agnostic, or unbiased, towards the function to be learned, a higher effective dimension likely results in a better trainability and performance. On the other hand, for models that are biased towards the function to be learned a lower effective dimension is likely beneficial during training. To obtain these results, we derive an analytical expression of the FIM for Fourier models and identify the features controlling a model's effective dimension. This allows us to construct models with tunable effective dimension and bias, and to compare their training. We furthermore introduce a tensor network representation of the considered Fourier models, which could be a tool of independent interest for the analysis of QNN models. Overall, these findings provide an explicit example of the interplay between geometrical properties, model-task alignment and training, which are relevant for the broader machine learning community.", "abs": "", "categories": ["cs.LG", "cond-mat.dis-nn", "physics.data-an", "quant-ph"], "AI": {"tldr": "本文研究了基于Fisher信息矩阵的评估指标在预测量子神经网络训练和预测性能方面的有效性，并揭示了模型有效维度和偏置与训练效果之间的关系。", "motivation": "量子机器学习，特别是量子神经网络（QNNs）日益受到关注，需要有效的评估指标来预测其训练和预测性能。本文旨在研究基于Fisher信息矩阵（FIM）的评估指标，并探索有效维度和模型偏置对训练和性能的影响。", "method": "本文将QNNs与傅里叶模型等价，推导了傅里叶模型的FIM解析表达式，并识别了控制模型有效维度的特征。通过构建具有可调有效维度和偏置的模型，并比较它们的训练过程，来验证假设。", "result": "研究发现，对于完全无偏的模型，较高的有效维度可能导致更好的训练能力和性能；而对于有偏模型，较低的有效维度可能更有利于训练。此外，引入了张量网络表示，可用于分析QNN模型。", "conclusion": "本文明确了几何属性、模型-任务对齐和训练之间的相互作用，为更广泛的机器学习社区提供了参考，并对量子机器学习领域中有效维度与模型训练的关系进行了深入研究。"}}
