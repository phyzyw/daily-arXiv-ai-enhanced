{"id": "2509.23937", "title": "Diffusion Models are Kelly Gamblers", "authors": ["Akhil Premkumar"], "summary": "We draw a connection between diffusion models and the Kelly criterion for maximizing returns in betting games. We find that conditional diffusion models store additional information to bind the signal $X$ with the conditioning information $Y$, equal to the mutual information between them. Classifier-free guidance effectively boosts the mutual information between $X$ and $Y$ at sampling time. This is especially helpful in image models, since the mutual information between images and their labels is low, a fact which is intimately connected to the manifold hypothesis. Finally, we point out some nuances in the popular perspective that diffusion models are infinitely deep autoencoders. In doing so, we relate the denoising loss to the Fermi Golden Rule from quantum mechanics.", "published": "2025-09-28", "categories": ["cs.LG", "cond-mat.stat-mech", "cs.AI", "cs.IT", "math.IT"], "pdf_url": "http://arxiv.org/pdf/2509.23937v1", "primary_category": "cs.LG"}
{"id": "2509.23629", "title": "How LLMs Learn to Reason: A Complex Network Perspective", "authors": ["Sihan Hu", "Xiansheng Cai", "Yuan Huang", "Zhiyuan Yao", "Linfeng Zhang", "Pan Zhang", "Youjin Deng", "Kun Chen"], "summary": "Training large language models with Reinforcement Learning from Verifiable Rewards (RLVR) exhibits a set of distinctive and puzzling behaviors that remain poorly understood, including a two-stage learning curve, V-shaped response-length trajectories, and a pronounced vulnerability to catastrophic forgetting. In this work, we propose that these seemingly disparate phenomena can be explained using a single unifying theory: the model's reasoning process maps to the self-organization of a semantic complex network whose topology remains persistently sparse, with the average degree pinned close to two. This topology imposes a fundamental mechanism for forgetting and learning: it first drives the system into a maximally frustrated state where ``skill islands'' form, slow-learning happens, and forgetting is induced; then it enters a sharp growth phase where the new skills are ``bolted on'', driven by phase-transition-like learning at the web's frontier. Equipped with the theory, we propose \\textit{Annealed-RLVR}, a principled algorithm that introduces an SFT-based ``heating'' step at the point of maximal frustration to resolve the competitive bottleneck and enhance the reasoning capability of the model. Experiments on a 1.5B-parameter model demonstrate that the approach outperforms standard RLVR on both in-distribution and out-of-distribution benchmarks. By recasting RLVR from black-box optimization into a predictable process of structural self-organization, our work provides a new physical intuition for engineering the emergent reasoning capabilities of future AI systems.", "published": "2025-09-28", "categories": ["cs.AI", "cond-mat.dis-nn", "cond-mat.stat-mech", "cs.LG", "physics.soc-ph"], "pdf_url": "http://arxiv.org/pdf/2509.23629v1", "primary_category": "cs.AI"}
{"id": "2509.23043", "title": "IsingFormer: Augmenting Parallel Tempering With Learned Proposals", "authors": ["Saleh Bunaiyan", "Corentin Delacour", "Shuvro Chowdhury", "Kyle Lee", "Kerem Y. Camsari"], "summary": "Markov Chain Monte Carlo (MCMC) underlies both statistical physics and combinatorial optimization, but mixes slowly near critical points and in rough landscapes. Parallel Tempering (PT) improves mixing by swapping replicas across temperatures, yet each replica still relies on slow local updates to change its configuration. We introduce IsingFormer, a Transformer trained on equilibrium samples that can generate entire spin configurations resembling those from the target distribution. These uncorrelated samples are used as proposals for global moves within a Metropolis step in PT, complementing the usual single-spin flips. On 2D Ising models (sampling), IsingFormer reproduces magnetization and free-energy curves and generalizes to unseen temperatures, including the critical region. Injecting even a single proposal sharply reduces equilibration time, replacing thousands of local updates. On 3D spin glasses (optimization), PT enhanced with IsingFormer finds substantially lower-energy states, demonstrating how global moves accelerate search in rugged landscapes. Finally, applied to integer factorization encoded as Ising problems, IsingFormer trained on a limited set of semiprimes transfers successfully to unseen semiprimes, boosting success rates beyond the training distribution. Since factorization is a canonical hard benchmark, this ability to generalize across instances highlights the potential of learning proposals that move beyond single problems to entire families of instances. The IsingFormer demonstrates that Monte Carlo methods can be systematically accelerated by neural proposals that capture global structure, yielding faster sampling and stronger performance in combinatorial optimization.", "published": "2025-09-27", "categories": ["cs.LG", "cond-mat.stat-mech", "cs.AI", "physics.comp-ph"], "pdf_url": "http://arxiv.org/pdf/2509.23043v1", "primary_category": "cs.LG"}
{"id": "2509.23144", "title": "Coordination Requires Simplification: Thermodynamic Bounds on Multi-Objective Compromise in Natural and Artificial Intelligence", "authors": ["Atma Anand"], "summary": "Information-processing systems coordinating across multiple agents and objectives face fundamental thermodynamic constraints. We show that solutions with maximum utility to act as coordination focal points have much higher selection pressure for being findable across agents rather than accuracy. We derive that the information-theoretic minimum description length of coordination protocols to precision $\\varepsilon$ scales as $L(P)\\geq NK\\log_2 K+N^2d^2\\log (1/\\varepsilon)$ for $N$ agents with $d$ potentially conflicting objectives and internal model complexity $K$. This scaling forces progressive simplification, with coordination dynamics changing the environment itself and shifting optimization across hierarchical levels. Moving from established focal points requires re-coordination, creating persistent metastable states and hysteresis until significant environmental shifts trigger phase transitions through spontaneous symmetry breaking. We operationally define coordination temperature to predict critical phenomena and estimate coordination work costs, identifying measurable signatures across systems from neural networks to restaurant bills to bureaucracies. Extending the topological version of Arrow's theorem on the impossibility of consistent preference aggregation, we find it recursively binds whenever preferences are combined. This potentially explains the indefinite cycling in multi-objective gradient descent and alignment faking in Large Language Models trained with reinforcement learning with human feedback. We term this framework Thermodynamic Coordination Theory (TCT), which demonstrates that coordination requires radical information loss.", "published": "2025-09-27", "categories": ["cs.AI", "cond-mat.stat-mech", "cs.MA", "nlin.AO", "physics.soc-ph"], "pdf_url": "http://arxiv.org/pdf/2509.23144v1", "primary_category": "cs.AI"}
{"id": "2509.24868", "title": "DRIFT-Net: A Spectral--Coupled Neural Operator for PDEs Learning", "authors": ["Jiayi Li", "Flora D. Salim"], "summary": "Learning PDE dynamics with neural solvers can significantly improve wall-clock efficiency and accuracy compared with classical numerical solvers. In recent years, foundation models for PDEs have largely adopted multi-scale windowed self-attention, with the scOT backbone in \\textsc{Poseidon} serving as a representative example.   However, because of their locality, truly globally consistent spectral coupling can only be propagated gradually through deep stacking and window shifting. This weakens global coupling and leads to error accumulation and drift during closed-loop rollouts. To address this, we propose \\textbf{DRIFT-Net}. It employs a dual-branch design comprising a spectral branch and an image branch. The spectral branch is responsible for capturing global, large-scale low-frequency information, whereas the image branch focuses on local details and nonstationary structures. Specifically, we first perform controlled, lightweight mixing within the low-frequency range. Then we fuse the spectral and image paths at each layer via bandwise weighting, which avoids the width inflation and training instability caused by naive concatenation. The fused result is transformed back into the spatial domain and added to the image branch, thereby preserving both global structure and high-frequency details across scales. Compared with strong attention-based baselines, DRIFT-Net achieves lower error and higher throughput with fewer parameters under identical training settings and budget. On Navier--Stokes benchmarks, the relative $L_{1}$ error is reduced by 7\\%--54\\%, the parameter count decreases by about 15\\%, and the throughput remains higher than scOT. Ablation studies and theoretical analyses further demonstrate the stability and effectiveness of this design. The code is available at https://github.com/cruiseresearchgroup/DRIFT-Net.", "published": "2025-09-29", "categories": ["cs.LG", "physics.comp-ph"], "pdf_url": "http://arxiv.org/pdf/2509.24868v1", "primary_category": "cs.LG"}
{"id": "2509.24264", "title": "Graph-Based Learning of Free Surface Dynamics in Generalized Newtonian Fluids using Smoothed Particle Hydrodynamics", "authors": ["Hyo-Jin Kim", "Jaekwang Kim", "Hyung-Jun Park"], "summary": "In this study, we propose a graph neural network (GNN) model for efficiently predicting the flow behavior of non-Newtonian fluids with free surface dynamics. The numerical analysis of non-Newtonian fluids presents significant challenges, as traditional algorithms designed for Newtonian fluids with constant viscosity often struggle to converge when applied to non-Newtonian cases, where rheological properties vary dynamically with flow conditions. Among these, power-law fluids exhibit viscosity that decreases exponentially as the shear rate increases, making numerical simulations particularly difficult. The complexity further escalates in free surface flow scenarios, where computational challenges intensify. In such cases, particle-based methods like smoothed particle hydrodynamics (SPH) provide advantages over traditional grid-based techniques, such as the finite element method (FEM). Building on this approach, we introduce a novel GNN-based numerical model to enhance the computational efficiency of non-Newtonian power-law fluid flow simulations. Our model is trained on SPH simulation data, learning the effects of particle accelerations in the presence of SPH interactions based on the fluid's power-law parameters. The GNN significantly accelerates computations while maintaining reliable accuracy in benchmark tests, including dam-break and droplet impact simulations. The results underscore the potential of GNN-based simulation frameworks for efficiently modeling non-Newtonian fluid behavior, paving the way for future advancements in data-driven fluid simulations.", "published": "2025-09-29", "categories": ["physics.flu-dyn", "cs.LG", "physics.comp-ph"], "pdf_url": "http://arxiv.org/pdf/2509.24264v1", "primary_category": "physics.flu-dyn"}
{"id": "2509.24117", "title": "GeoFunFlow: Geometric Function Flow Matching for Inverse Operator Learning over Complex Geometries", "authors": ["Sifan Wang", "Zhikai Wu", "David van Dijk", "Lu Lu"], "summary": "Inverse problems governed by partial differential equations (PDEs) are crucial in science and engineering. They are particularly challenging due to ill-posedness, data sparsity, and the added complexity of irregular geometries. Classical PDE-constrained optimization methods are computationally expensive, especially when repeated posterior sampling is required. Learning-based approaches improve efficiency and scalability, yet most are designed for regular domains or focus on forward modeling. Here, we introduce {\\em GeoFunFlow}, a geometric diffusion model framework for inverse problems on complex geometries. GeoFunFlow combines a novel geometric function autoencoder (GeoFAE) and a latent diffusion model trained via rectified flow. GeoFAE employs a Perceiver module to process unstructured meshes of varying sizes and produces continuous reconstructions of physical fields, while the diffusion model enables posterior sampling from sparse and noisy data. Across five benchmarks, GeoFunFlow achieves state-of-the-art reconstruction accuracy over complex geometries, provides calibrated uncertainty quantification, and delivers efficient inference compared to operator-learning and diffusion model baselines.", "published": "2025-09-28", "categories": ["cs.LG", "physics.comp-ph", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2509.24117v1", "primary_category": "cs.LG"}
{"id": "2509.23453", "title": "PHASE: Physics-Integrated, Heterogeneity-Aware Surrogates for Scientific Simulations", "authors": ["Dawei Gao", "Dali Wang", "Zhuowei Gu", "Qinglei Cao", "Xiao Wang", "Peter Thornton", "Dan Ricciuto", "Yunhe Feng"], "summary": "Large-scale numerical simulations underpin modern scientific discovery but remain constrained by prohibitive computational costs. AI surrogates offer acceleration, yet adoption in mission-critical settings is limited by concerns over physical plausibility, trustworthiness, and the fusion of heterogeneous data. We introduce PHASE, a modular deep-learning framework for physics-integrated, heterogeneity-aware surrogates in scientific simulations. PHASE combines data-type-aware encoders for heterogeneous inputs with multi-level physics-based constraints that promote consistency from local dynamics to global system behavior. We validate PHASE on the biogeochemical (BGC) spin-up workflow of the U.S. Department of Energy's Energy Exascale Earth System Model (E3SM) Land Model (ELM), presenting-to our knowledge-the first scientifically validated AI-accelerated solution for this task. Using only the first 20 simulation years, PHASE infers a near-equilibrium state that otherwise requires more than 1,200 years of integration, yielding an effective reduction in required integration length by at least 60x. The framework is enabled by a pipeline for fusing heterogeneous scientific data and demonstrates strong generalization to higher spatial resolutions with minimal fine-tuning. These results indicate that PHASE captures governing physical regularities rather than surface correlations, enabling practical, physically consistent acceleration of land-surface modeling and other complex scientific workflows.", "published": "2025-09-27", "categories": ["cs.LG", "physics.comp-ph"], "pdf_url": "http://arxiv.org/pdf/2509.23453v1", "primary_category": "cs.LG"}
{"id": "2509.24779", "title": "MarS-FM: Generative Modeling of Molecular Dynamics via Markov State Models", "authors": ["Kacper Kapuśniak", "Cristian Gabellini", "Michael Bronstein", "Prudencio Tossou", "Francesco Di Giovanni"], "summary": "Molecular Dynamics (MD) is a powerful computational microscope for probing protein functions. However, the need for fine-grained integration and the long timescales of biomolecular events make MD computationally expensive. To address this, several generative models have been proposed to generate surrogate trajectories at lower cost. Yet, these models typically learn a fixed-lag transition density, causing the training signal to be dominated by frequent but uninformative transitions. We introduce a new class of generative models, MSM Emulators, which instead learn to sample transitions across discrete states defined by an underlying Markov State Model (MSM). We instantiate this class with Markov Space Flow Matching (MarS-FM), whose sampling offers more than two orders of magnitude speedup compared to implicit- or explicit-solvent MD simulations. We benchmark Mars-FM ability to reproduce MD statistics through structural observables such as RMSD, radius of gyration, and secondary structure content. Our evaluation spans protein domains (up to 500 residues) with significant chemical and structural diversity, including unfolding events, and enforces strict sequence dissimilarity between training and test sets to assess generalization. Across all metrics, MarS-FM outperforms existing methods, often by a substantial margin.", "published": "2025-09-29", "categories": ["cs.LG", "q-bio.BM"], "pdf_url": "http://arxiv.org/pdf/2509.24779v1", "primary_category": "cs.LG"}
{"id": "2509.23254", "title": "ABConformer: Physics-inspired Sliding Attention for Antibody-Antigen Interface Prediction", "authors": ["Zhang-Yu You", "Jiahao Ma", "Hongzong Li", "Ye-Fan Hu", "Jian-Dong Huang"], "summary": "Accurate prediction of antibody-antigen (Ab-Ag) interfaces is critical for vaccine design, immunodiagnostics, and therapeutic antibody development. However, achieving reliable predictions from sequences alone remains a challenge. In this paper, we present ABCONFORMER, a model based on the Conformer backbone that captures both local and global features of a biosequence. To accurately capture Ab-Ag interactions, we introduced the physics-inspired sliding attention, enabling residue-level contact recovery without relying on three-dimensional structural data. ABConformer can accurately predict paratopes and epitopes given the antibody and antigen sequence, and predict pan-epitopes on the antigen without antibody information. In comparison experiments, ABCONFORMER achieves state-of-the-art performance on a recent SARS-CoV-2 Ab-Ag dataset, and surpasses widely used sequence-based methods for antibody-agnostic epitope prediction. Ablation studies further quantify the contribution of each component, demonstrating that, compared to conventional cross-attention, sliding attention significantly enhances the precision of epitope prediction. To facilitate reproducibility, we will release the code under an open-source license upon acceptance.", "published": "2025-09-27", "categories": ["cs.LG", "q-bio.BM"], "pdf_url": "http://arxiv.org/pdf/2509.23254v1", "primary_category": "cs.LG"}
{"id": "2509.24914", "title": "Inductive Bias and Spectral Properties of Single-Head Attention in High Dimensions", "authors": ["Fabrizio Boncoraglio", "Vittorio Erba", "Emanuele Troiani", "Florent Krzakala", "Lenka Zdeborová"], "summary": "We study empirical risk minimization in a single-head tied-attention layer trained on synthetic high-dimensional sequence tasks, given by the recently introduced attention-indexed model. Using tools from random matrix theory, spin-glass physics, and approximate message passing, we derive sharp asymptotics for training and test errors, locate interpolation and recovery thresholds, and characterize the limiting spectral distribution of the learned weights. Weight decay induces an implicit nuclear-norm regularization, favoring low-rank query and key matrices. Leveraging this, we compare the standard factorized training of query and key matrices with a direct parameterization in which their product is trained element-wise, revealing the inductive bias introduced by the factorized form. Remarkably, the predicted spectral distribution echoes empirical trends reported in large-scale transformers, offering a theoretical perspective consistent with these phenomena.", "published": "2025-09-29", "categories": ["stat.ML", "cond-mat.dis-nn", "cs.IT", "cs.LG", "math.IT"], "pdf_url": "http://arxiv.org/pdf/2509.24914v1", "primary_category": "stat.ML"}
{"id": "2509.24882", "title": "Scaling Laws and Spectra of Shallow Neural Networks in the Feature Learning Regime", "authors": ["Leonardo Defilippis", "Yizhou Xu", "Julius Girardin", "Emanuele Troiani", "Vittorio Erba", "Lenka Zdeborová", "Bruno Loureiro", "Florent Krzakala"], "summary": "Neural scaling laws underlie many of the recent advances in deep learning, yet their theoretical understanding remains largely confined to linear models. In this work, we present a systematic analysis of scaling laws for quadratic and diagonal neural networks in the feature learning regime. Leveraging connections with matrix compressed sensing and LASSO, we derive a detailed phase diagram for the scaling exponents of the excess risk as a function of sample complexity and weight decay. This analysis uncovers crossovers between distinct scaling regimes and plateau behaviors, mirroring phenomena widely reported in the empirical neural scaling literature. Furthermore, we establish a precise link between these regimes and the spectral properties of the trained network weights, which we characterize in detail. As a consequence, we provide a theoretical validation of recent empirical observations connecting the emergence of power-law tails in the weight spectrum with network generalization performance, yielding an interpretation from first principles.", "published": "2025-09-29", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.AI", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2509.24882v1", "primary_category": "cs.LG"}
