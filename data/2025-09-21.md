<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]
- [cs.CL](#cs.CL) [Total: 8]
- [cs.GR](#cs.GR) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Out-of-Sight Trajectories: Tracking, Fusion, and Prediction](https://arxiv.org/abs/2509.15219)
*Haichao Zhang, Yi Xu, Yun Fu*

Main category: cs.CV

TL;DR: This paper introduces Out-of-Sight Trajectory (OST) prediction, a novel task addressing the challenge of predicting trajectories of objects hidden from view using noisy sensor data. The approach enhances trajectory denoising and prediction, particularly in scenarios with limited camera coverage and obstructions.


<details>
  <summary>Details</summary>
Motivation: Existing trajectory prediction methods struggle with out-of-sight objects and noisy sensor data, posing safety risks in applications like autonomous driving and robotics. The lack of visual references and ground truth for denoised trajectories further exacerbates these issues.

Method: The proposed approach, building on OOSTraj, utilizes an enhanced Vision-Positioning Denoising Module that leverages camera calibration to establish a vision-positioning mapping. This allows for unsupervised denoising of noisy sensor data and prediction of noise-free visual trajectories for both pedestrians and vehicles.

Result: Extensive evaluations on the Vi-Fi and JRDB datasets demonstrate state-of-the-art performance in both trajectory denoising and prediction, significantly outperforming previous baselines and traditional denoising methods like Kalman filtering.

Conclusion: This work represents the first integration of vision-positioning projection for denoising noisy sensor trajectories of out-of-sight agents, paving the way for advancements in trajectory prediction and enabling more robust and reliable autonomous systems in real-world scenarios.

Abstract: Trajectory prediction is a critical task in computer vision and autonomous systems, playing a key role in autonomous driving, robotics, surveillance, and virtual reality. Existing methods often rely on complete and noise-free observational data, overlooking the challenges associated with out-of-sight objects and the inherent noise in sensor data caused by limited camera coverage, obstructions, and the absence of ground truth for denoised trajectories. These limitations pose safety risks and hinder reliable prediction in real-world scenarios. In this extended work, we present advancements in Out-of-Sight Trajectory (OST), a novel task that predicts the noise-free visual trajectories of out-of-sight objects using noisy sensor data. Building on our previous research, we broaden the scope of Out-of-Sight Trajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending its applicability to autonomous driving, robotics, surveillance, and virtual reality. Our enhanced Vision-Positioning Denoising Module leverages camera calibration to establish a vision-positioning mapping, addressing the lack of visual references, while effectively denoising noisy sensor data in an unsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB datasets, our approach achieves state-of-the-art performance in both trajectory denoising and prediction, significantly surpassing previous baselines. Additionally, we introduce comparisons with traditional denoising methods, such as Kalman filtering, and adapt recent trajectory prediction models to our task, providing a comprehensive benchmark. This work represents the first initiative to integrate vision-positioning projection for denoising noisy sensor trajectories of out-of-sight agents, paving the way for future advances. The code and preprocessed datasets are available at github.com/Hai-chao-Zhang/OST

</details>


### [2] [Semi-Supervised 3D Medical Segmentation from 2D Natural Images Pretrained Model](https://arxiv.org/abs/2509.15167)
*Pak-Hei Yeung, Jayroop Ramesh, Pengfei Lyu, Ana Namburete, Jagath Rajapakse*

Main category: cs.CV

TL;DR: This paper introduces M&N, a model-agnostic framework that transfers knowledge from 2D natural image models to improve 3D medical image segmentation in a semi-supervised setting, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Training 3D medical image segmentation models from scratch requires large labeled datasets, which are often scarce. Leveraging knowledge from 2D natural image models, where abundant labeled data exists, offers a promising solution.

Method: M&N iteratively co-trains a 2D pretrained model and a 3D segmentation model using pseudo-masks generated by each other, coupled with a learning rate guided sampling strategy to adaptively balance labeled and unlabeled data.

Result: M&N achieves state-of-the-art performance on multiple publicly available datasets, outperforming thirteen existing semi-supervised segmentation approaches, and demonstrates model-agnostic compatibility.

Conclusion: The proposed M&N framework provides a flexible and generalizable approach for transferring knowledge from 2D vision models to 3D medical image segmentation, particularly beneficial in low-data scenarios, and adaptable to future model advancements.

Abstract: This paper explores the transfer of knowledge from general vision models pretrained on 2D natural images to improve 3D medical image segmentation. We focus on the semi-supervised setting, where only a few labeled 3D medical images are available, along with a large set of unlabeled images. To tackle this, we propose a model-agnostic framework that progressively distills knowledge from a 2D pretrained model to a 3D segmentation model trained from scratch. Our approach, M&N, involves iterative co-training of the two models using pseudo-masks generated by each other, along with our proposed learning rate guided sampling that adaptively adjusts the proportion of labeled and unlabeled data in each training batch to align with the models' prediction accuracy and stability, minimizing the adverse effect caused by inaccurate pseudo-masks. Extensive experiments on multiple publicly available datasets demonstrate that M&N achieves state-of-the-art performance, outperforming thirteen existing semi-supervised segmentation approaches under all different settings. Importantly, ablation studies show that M&N remains model-agnostic, allowing seamless integration with different architectures. This ensures its adaptability as more advanced models emerge. The code is available at https://github.com/pakheiyeung/M-N.

</details>


### [3] [Synthetic-to-Real Object Detection using YOLOv11 and Domain Randomization Strategies](https://arxiv.org/abs/2509.15045)
*Luisa Torquato Niño, Hamza A. A. Gardi*

Main category: cs.CV

TL;DR: This paper explores training a YOLOv11 object detection model solely on synthetic data and domain randomization techniques to detect soup cans, addressing the synthetic-to-real domain gap. The approach achieved a high mAP@50 score in a Kaggle competition, demonstrating the potential of synthetic-only training.


<details>
  <summary>Details</summary>
Motivation: The high cost and effort of collecting and labeling real-world datasets for object detection motivates the use of synthetic data as a cost-effective and scalable alternative. However, the synthetic-to-real domain gap poses a significant challenge to achieving good generalization performance.

Method: The researchers trained a YOLOv11 model on a diverse and expanded synthetic dataset generated by Falcon’s Duality AI Simulator. They experimented with extensive data augmentation, dataset composition, and model scaling, focusing on increasing synthetic dataset diversity (varied perspectives and complex backgrounds) and carefully tuning data augmentation strategies. Qualitative and quantitative evaluation on both synthetic and real-world datasets guided development.

Result: The best performing configuration, a YOLOv11l model, achieved a final mAP@50 of 0.910 on the competition’s hidden test set. Benchmarking showed YOLOv11 outperformed previous YOLO versions in accuracy, speed, and efficiency.

Conclusion: The results demonstrate the potential of a synthetic-only training approach for object detection, particularly when combined with domain randomization and diverse synthetic datasets. However, the study also highlights the remaining challenges in fully capturing real-world variability and bridging the domain gap.

Abstract: This paper addresses the synthetic-to-real domain gap in object detection, focusing on training a YOLOv11 model to detect a specific object (a soup can) using only synthetic data and domain randomization strategies. The methodology involves extensive experimentation with data augmentation, dataset composition, and model scaling. While synthetic validation metrics were consistently high, they proved to be poor predictors of real-world performance. Consequently, models were also evaluated qualitatively, through visual inspection of predictions, and quantitatively, on a manually labeled real-world test set, to guide development. Final mAP@50 scores were provided by the official Kaggle competition. Key findings indicate that increasing synthetic dataset diversity, specifically by including varied perspectives and complex backgrounds, combined with carefully tuned data augmentation, were crucial in bridging the domain gap. The best performing configuration, a YOLOv11l model trained on an expanded and diverse dataset, achieved a final mAP@50 of 0.910 on the competition's hidden test set. This result demonstrates the potential of a synthetic-only training approach while also highlighting the remaining challenges in fully capturing real-world variability.

</details>


### [4] [ProtoMedX: Towards Explainable Multi-Modal Prototype Learning for Bone Health Classification](https://arxiv.org/abs/2509.14830)
*Alvaro Lopez Pellicer, Andre Mariucci, Plamen Angelov, Marwan Bukhari, Jemma G. Kerns*

Main category: cs.CV

TL;DR: ProtoMedX is a novel multi-modal prototype learning model for bone health classification that combines DEXA scans and patient records, offering both state-of-the-art accuracy and explainability for clinical use.


<details>
  <summary>Details</summary>
Motivation: Existing bone health classification methods often lack explainability, rely solely on imaging data, and oversimplify diagnostic categories (e.g., only classifying as 'normal' or 'osteoporosis'), hindering clinical adoption and compliance with regulations like the EU AI Act.

Method: ProtoMedX utilizes a prototype-based architecture with dual prototype spaces (visual and clinical) linked by cross-modal attention. It identifies representative prototypes for each diagnostic category and classifies new patients based on similarity to these prototypes, mimicking clinical reasoning.

Result: ProtoMedX achieves 87.58% accuracy using vision-only data and 89.8% accuracy with multi-modal data (DEXA scans and patient records), surpassing existing published methods.  Crucially, it provides explainable predictions through direct comparison to learned prototypes.

Conclusion: ProtoMedX represents a significant advancement in bone health classification by offering a transparent, clinically relevant, and highly accurate model that addresses the limitations of existing deep learning approaches and aligns with the growing need for explainable AI in healthcare.

Abstract: Bone health studies are crucial in medical practice for the early detection and treatment of Osteopenia and Osteoporosis. Clinicians usually make a diagnosis based on densitometry (DEXA scans) and patient history. The applications of AI in this field are ongoing research. Most successful methods rely on deep learning models that use vision alone (DEXA/X-ray imagery) and focus on prediction accuracy, while explainability is often disregarded and left to post hoc assessments of input contributions. We propose ProtoMedX, a multi-modal model that uses both DEXA scans of the lumbar spine and patient records. ProtoMedX's prototype-based architecture is explainable by design, which is crucial for medical applications, especially in the context of the upcoming EU AI Act, as it allows explicit analysis of model decisions, including incorrect ones. ProtoMedX demonstrates state-of-the-art performance in bone health classification while also providing explanations that can be visually understood by clinicians. Using a dataset of 4,160 real NHS patients, the proposed ProtoMedX achieves 87.58% accuracy in vision-only tasks and 89.8% in its multi-modal variant, both surpassing existing published methods.

</details>


### [5] [Leveraging Geometric Visual Illusions as Perceptual Inductive Biases for Vision Models](https://arxiv.org/abs/2509.15156)
*Haobo Yang, Minghao Guo, Dequan Yang, Wenyu Wang*

Main category: cs.CV

TL;DR: This paper explores incorporating geometric visual illusions into image classification training to improve generalization, particularly for transformer-based models. The results demonstrate that these illusions, despite being synthetic, can enhance a model's sensitivity to structural details.


<details>
  <summary>Details</summary>
Motivation: Modern deep learning models often rely on statistical regularities and miss crucial structural information that humans readily perceive. This work aims to bridge the gap between machine vision and human perception by leveraging insights from perceptual psychology.

Method: The researchers created a synthetic dataset of five classic geometric illusions with parametric control over distortion. They then evaluated three multi-source learning strategies (joint, parallel, and hybrid) that combined illusion recognition tasks with ImageNet classification.

Result: Incorporating geometric illusions as auxiliary supervision systematically improved generalization, especially in challenging cases with intricate contours and fine textures.  The use of illusions enhanced the structural sensitivity of both CNN and transformer-based architectures, with the most significant improvements observed in ViT models.

Conclusion: This study demonstrates a novel integration of perceptual science and machine learning, suggesting that geometric illusions can be repurposed to teach vision models to see structure more robustly. It opens new avenues for embedding perceptual priors into vision model design.

Abstract: Contemporary deep learning models have achieved impressive performance in image classification by primarily leveraging statistical regularities within large datasets, but they rarely incorporate structured insights drawn directly from perceptual psychology. To explore the potential of perceptually motivated inductive biases, we propose integrating classic geometric visual illusions well-studied phenomena from human perception into standard image-classification training pipelines. Specifically, we introduce a synthetic, parametric geometric-illusion dataset and evaluate three multi-source learning strategies that combine illusion recognition tasks with ImageNet classification objectives. Our experiments reveal two key conceptual insights: (i) incorporating geometric illusions as auxiliary supervision systematically improves generalization, especially in visually challenging cases involving intricate contours and fine textures; and (ii) perceptually driven inductive biases, even when derived from synthetic stimuli traditionally considered unrelated to natural image recognition, can enhance the structural sensitivity of both CNN and transformer-based architectures. These results demonstrate a novel integration of perceptual science and machine learning and suggest new directions for embedding perceptual priors into vision model design.

</details>


### [6] [Sea-ing Through Scattered Rays: Revisiting the Image Formation Model for Realistic Underwater Image Generation](https://arxiv.org/abs/2509.15011)
*Vasiliki Ismiroglou, Malte Pedersen, Stefan H. Bengtson, Andreas Aakerberg, Thomas B. Moeslund*

Main category: cs.CV

TL;DR: This paper revisits the underwater image formation model (IFM) to improve synthetic underwater image generation, particularly in highly turbid environments, and introduces a new dataset (BUCKET) for realistic turbid underwater footage.


<details>
  <summary>Details</summary>
Motivation: Existing synthetic underwater image generation methods often overlook the complex distance-dependent visibility loss in turbid environments, leading to unrealistic results. Data scarcity for murky underwater environments hinders the development of robust computer vision algorithms.

Method: The authors propose an improved IFM that includes a forward scattering term and considers a non-uniform medium. They also collected the BUCKET dataset under controlled turbidity conditions, pairing real turbid footage with reference images.

Result: The proposed model demonstrates qualitative improvements over existing models, especially in high-turbidity conditions, with a selection rate of 82.5% by survey participants.

Conclusion: The improved IFM and the BUCKET dataset contribute to more realistic synthetic underwater image generation, addressing data scarcity and enabling better training of underwater computer vision algorithms, particularly for challenging, turbid environments.

Abstract: In recent years, the underwater image formation model has found extensive use in the generation of synthetic underwater data. Although many approaches focus on scenes primarily affected by discoloration, they often overlook the model's ability to capture the complex, distance-dependent visibility loss present in highly turbid environments. In this work, we propose an improved synthetic data generation pipeline that includes the commonly omitted forward scattering term, while also considering a nonuniform medium. Additionally, we collected the BUCKET dataset under controlled turbidity conditions to acquire real turbid footage with the corresponding reference images. Our results demonstrate qualitative improvements over the reference model, particularly under increasing turbidity, with a selection rate of 82. 5\% by survey participants. Data and code can be accessed on the project page: vap.aau.dk/sea-ing-through-scattered-rays.

</details>


### [7] [RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching](https://arxiv.org/abs/2509.14966)
*Xingwu Zhang, Guanxuan Li, Zhuocheng Zhang, Zijun Long*

Main category: cs.CV

TL;DR: RoboEye is a novel two-stage object identification framework that enhances 2D semantic features with dynamic 3D reasoning, improving accuracy in challenging warehouse environments with large product catalogs and diverse conditions.


<details>
  <summary>Details</summary>
Motivation: Existing object identification methods relying solely on 2D appearance features struggle with increasing product catalog sizes, viewpoint variations, occlusions, and packaging differences in warehouse settings, leading to performance degradation and financial losses.

Method: RoboEye uses a two-stage approach: first, a 2D vision model generates candidate rankings; second, a 3D-feature-aware module selectively applies a robot 3D retrieval transformer, which extracts geometry-aware dense features and uses keypoint matching for robust verification, avoiding unnecessary computation.

Result: RoboEye improves Recall@1 by 7.1% over the state-of-the-art (RoboLLM) while operating solely on RGB images, eliminating the need for explicit 3D inputs and reducing deployment costs.

Conclusion: RoboEye demonstrates the effectiveness of integrating 3D geometric reasoning into 2D object identification for warehouse automation, offering a cost-effective and robust solution for handling the complexities of large-scale e-commerce environments.

Abstract: The rapidly growing number of product categories in large-scale e-commerce makes accurate object identification for automated packing in warehouses substantially more difficult. As the catalog grows, intra-class variability and a long tail of rare or visually similar items increase, and when combined with diverse packaging, cluttered containers, frequent occlusion, and large viewpoint changes-these factors amplify discrepancies between query and reference images, causing sharp performance drops for methods that rely solely on 2D appearance features. Thus, we propose RoboEye, a two-stage identification framework that dynamically augments 2D semantic features with domain-adapted 3D reasoning and lightweight adapters to bridge training deployment gaps. In the first stage, we train a large vision model to extract 2D features for generating candidate rankings. A lightweight 3D-feature-awareness module then estimates 3D feature quality and predicts whether 3D re-ranking is necessary, preventing performance degradation and avoiding unnecessary computation. When invoked, the second stage uses our robot 3D retrieval transformer, comprising a 3D feature extractor that produces geometry-aware dense features and a keypoint-based matcher that computes keypoint-correspondence confidences between query and reference images instead of conventional cosine-similarity scoring. Experiments show that RoboEye improves Recall@1 by 7.1% over the prior state of the art (RoboLLM). Moreover, RoboEye operates using only RGB images, avoiding reliance on explicit 3D inputs and reducing deployment costs. The code used in this paper is publicly available at: https://github.com/longkukuhi/RoboEye.

</details>


### [8] [MARIC: Multi-Agent Reasoning for Image Classification](https://arxiv.org/abs/2509.14860)
*Wonduk Seo, Minhyeong Yu, Hyunjin An, Seunghyun Lee*

Main category: cs.CV

TL;DR: MARIC is a novel multi-agent framework for image classification that decomposes the task into collaborative reasoning among specialized agents, improving accuracy and interpretability without relying on extensive training data.


<details>
  <summary>Details</summary>
Motivation: Traditional image classification methods require large datasets and extensive fine-tuning, while existing vision-language models (VLMs) often lack robust reasoning capabilities and multi-perspective analysis.

Method: MARIC utilizes an Outliner Agent to generate prompts, three Aspect Agents to extract fine-grained descriptions, and a Reasoning Agent to synthesize these outputs through reflection, creating a unified representation for classification.

Result: Experiments on four diverse image classification benchmark datasets demonstrate that MARIC significantly outperforms baselines, showcasing the effectiveness of multi-agent visual reasoning.

Conclusion: MARIC presents a scalable and interpretable paradigm for image classification, moving beyond traditional training-heavy or single-pass inference approaches and offering improved robustness and interpretability.

Abstract: Image classification has traditionally relied on parameter-intensive model training, requiring large-scale annotated datasets and extensive fine tuning to achieve competitive performance. While recent vision language models (VLMs) alleviate some of these constraints, they remain limited by their reliance on single pass representations, often failing to capture complementary aspects of visual content. In this paper, we introduce Multi Agent based Reasoning for Image Classification (MARIC), a multi agent framework that reformulates image classification as a collaborative reasoning process. MARIC first utilizes an Outliner Agent to analyze the global theme of the image and generate targeted prompts. Based on these prompts, three Aspect Agents extract fine grained descriptions along distinct visual dimensions. Finally, a Reasoning Agent synthesizes these complementary outputs through integrated reflection step, producing a unified representation for classification. By explicitly decomposing the task into multiple perspectives and encouraging reflective synthesis, MARIC mitigates the shortcomings of both parameter-heavy training and monolithic VLM reasoning. Experiments on 4 diverse image classification benchmark datasets demonstrate that MARIC significantly outperforms baselines, highlighting the effectiveness of multi-agent visual reasoning for robust and interpretable image classification.

</details>


### [9] [[Re] Improving Interpretation Faithfulness for Vision Transformers](https://arxiv.org/abs/2509.14846)
*Izabela Kurek, Wojciech Trejter, Stipe Frkovic, Andro Erdelez*

Main category: cs.CV

TL;DR: This work reproduces and extends the findings of Hu et al. (2024) on Faithful Vision Transformers (FViTs), which utilize Diffusion Denoised Smoothing (DDS) to improve interpretability robustness. The study investigates the effectiveness of DDS in mitigating adversarial attacks on interpretability methods for Vision Transformers.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers (ViTs) offer interpretability through attention vectors, but these are vulnerable to adversarial attacks. The research aims to validate and expand upon the claims of FViTs, which propose DDS as a solution to enhance interpretability robustness.

Method: The researchers replicated the experiments from Hu et al. (2024) using DDS to convert standard ViTs into FViTs. They tested the robustness of FViTs and baseline interpretability methods (Raw Attention, GradCAM, Rollout, LRP, TA) against attacks in segmentation and classification tasks, also measuring the computational cost of DDS.

Result: The results largely agree with the original study, demonstrating improved interpretability robustness with DDS. However, minor discrepancies were observed and discussed. The study also quantified the computational overhead and environmental impact associated with using DDS.

Conclusion: FViTs, employing DDS, offer a promising approach to enhance the faithfulness and reliability of interpretability methods for Vision Transformers, particularly in the face of adversarial attacks. The study provides a valuable reproduction and extension of this work, highlighting both its benefits and associated computational costs.

Abstract: This work aims to reproduce the results of Faithful Vision Transformers (FViTs) proposed by arXiv:2311.17983 alongside interpretability methods for Vision Transformers from arXiv:2012.09838 and Xu (2022) et al. We investigate claims made by arXiv:2311.17983, namely that the usage of Diffusion Denoised Smoothing (DDS) improves interpretability robustness to (1) attacks in a segmentation task and (2) perturbation and attacks in a classification task. We also extend the original study by investigating the authors' claims that adding DDS to any interpretability method can improve its robustness under attack. This is tested on baseline methods and the recently proposed Attribution Rollout method. In addition, we measure the computational costs and environmental impact of obtaining an FViT through DDS. Our results broadly agree with the original study's findings, although minor discrepancies were found and discussed.

</details>


### [10] [Not All Degradations Are Equal: A Targeted Feature Denoising Framework for Generalizable Image Super-Resolution](https://arxiv.org/abs/2509.14841)
*Hongjun Wang, Jiyuan Chen, Zhengwei Yin, Xuan Song, Yinqiang Zheng*

Main category: cs.CV

TL;DR: This paper addresses the challenge of generalizable image super-resolution by identifying that models primarily overfit to noise degradation. It proposes a targeted feature denoising framework to mitigate this overfitting without architectural changes.


<details>
  <summary>Details</summary>
Motivation: Existing super-resolution models struggle to generalize to real-world images due to a domain gap between synthetic training data and complex, diverse real-world degradations. While regularization techniques exist, they treat all degradation types equally, overlooking the fact that models predominantly overfit to noise.

Method: The proposed framework consists of a noise detection module and a denoising module. It analyzes the frequency domain characteristics of different degradation types (blur, noise, JPEG) and specifically targets noise-related features for denoising, seamlessly integrating with existing super-resolution models.

Result: The targeted feature denoising framework outperforms previous regularization-based methods across five benchmark datasets, demonstrating improved generalization performance in both synthetic and real-world scenarios.

Conclusion: This work highlights the importance of targeted degradation mitigation in generalizable image super-resolution. By focusing specifically on noise overfitting, the proposed framework provides a simple and effective solution for improving model generalization without requiring significant architectural modifications.

Abstract: Generalizable Image Super-Resolution aims to enhance model generalization capabilities under unknown degradations. To achieve this goal, the models are expected to focus only on image content-related features instead of overfitting degradations. Recently, numerous approaches such as Dropout and Feature Alignment have been proposed to suppress models' natural tendency to overfit degradations and yield promising results. Nevertheless, these works have assumed that models overfit to all degradation types (e.g., blur, noise, JPEG), while through careful investigations in this paper, we discover that models predominantly overfit to noise, largely attributable to its distinct degradation pattern compared to other degradation types. In this paper, we propose a targeted feature denoising framework, comprising noise detection and denoising modules. Our approach presents a general solution that can be seamlessly integrated with existing super-resolution models without requiring architectural modifications. Our framework demonstrates superior performance compared to previous regularization-based methods across five traditional benchmarks and datasets, encompassing both synthetic and real-world scenarios.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [11] [Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning](https://arxiv.org/abs/2509.15188)
*Yeongbin Seo, Dongha Lee, Jaehyung Kim, Jinyoung Yeo*

Main category: cs.CL

TL;DR: This paper introduces Convolutional Decoding (Conv) and Rejecting Rule-based Fine-Tuning (R2FT) to address the 'long decoding-window' problem in diffusion language models, enabling faster and more fluent text generation without sacrificing bidirectionality.


<details>
  <summary>Details</summary>
Motivation: Diffusion language models offer potential speed and bidirectionality advantages over autoregressive models, but suffer from the 'long decoding-window' problem where distant tokens become irrelevant or repetitive. Existing solutions like semi-autoregressive approaches sacrifice speed and bidirectionality to mitigate this issue.

Method: The authors propose two methods: (1) Convolutional Decoding (Conv), a normalization-based technique that narrows the decoding window without hard segmentation; and (2) Rejecting Rule-based Fine-Tuning (R2FT), a post-hoc training scheme to better align distant tokens with the context.

Result: The proposed methods achieve state-of-the-art results on open-ended generation benchmarks (AlpacaEval) with significantly lower step sizes compared to previous diffusion LM baselines, demonstrating both speed and quality improvements.

Conclusion: The work demonstrates a significant advancement in diffusion language models, overcoming a key bottleneck to achieve faster, more fluent, and bidirectional text generation, making them a more competitive alternative to autoregressive models.

Abstract: Autoregressive (AR) language models generate text one token at a time, which limits their inference speed. Diffusion-based language models offer a promising alternative, as they can decode multiple tokens in parallel. However, we identify a key bottleneck in current diffusion LMs: the long decoding-window problem, where tokens generated far from the input context often become irrelevant or repetitive. Previous solutions like semi-autoregressive address this issue by splitting windows into blocks, but this sacrifices speed and bidirectionality, eliminating the main advantage of diffusion models. To overcome this, we propose Convolutional decoding (Conv), a normalization-based method that narrows the decoding window without hard segmentation, leading to better fluency and flexibility. Additionally, we introduce Rejecting Rule-based Fine-Tuning (R2FT), a post-hoc training scheme that better aligns tokens at positions far from context. Our methods achieve state-of-the-art results on open-ended generation benchmarks (e.g., AlpacaEval) among diffusion LM baselines, with significantly lower step size than previous works, demonstrating both speed and quality improvements.

</details>


### [12] [Patent Language Model Pretraining with ModernBERT](https://arxiv.org/abs/2509.14926)
*Amirhossein Yousefiramandi, Ciaran Cooney*

Main category: cs.CL

TL;DR: This paper introduces ModernBERT-base-PT, a domain-specific language model pretrained on a large corpus of patent records, demonstrating improved performance and significantly faster inference compared to existing patent NLP models.


<details>
  <summary>Details</summary>
Motivation: Existing language models, including BERT, struggle with specialized domains like patents due to their training on general-domain text. There's a need for domain-specific models that leverage recent architectural advancements for improved performance and efficiency in patent NLP tasks.

Method: The authors pretrained three domain-specific masked language models (ModernBERT-base-PT, ModernBERT-base-VX, and Mosaic-BERT-large) using the ModernBERT architecture and over 60 million patent records. They incorporated architectural optimizations like FlashAttention, rotary embeddings, and GLU feed-forward layers, and evaluated the models on four patent classification tasks.

Result: ModernBERT-base-PT consistently outperformed the general-purpose ModernBERT baseline on three out of four datasets and achieved competitive performance with PatentBERT. Scaling the model size and customizing the tokenizer further enhanced performance. Notably, all ModernBERT variants exhibited substantially faster inference (over 3×) than PatentBERT.

Conclusion: Domain-specific pretraining with architectural improvements like ModernBERT significantly benefits patent-focused NLP tasks, offering improved accuracy and faster inference, making it suitable for time-sensitive applications.

Abstract: Transformer-based language models such as BERT have become foundational in NLP, yet their performance degrades in specialized domains like patents, which contain long, technical, and legally structured text. Prior approaches to patent NLP have primarily relied on fine-tuning general-purpose models or domain-adapted variants pretrained with limited data. In this work, we pretrain 3 domain-specific masked language models for patents, using the ModernBERT architecture and a curated corpus of over 60 million patent records. Our approach incorporates architectural optimizations, including FlashAttention, rotary embeddings, and GLU feed-forward layers. We evaluate our models on four downstream patent classification tasks. Our model, ModernBERT-base-PT, consistently outperforms the general-purpose ModernBERT baseline on three out of four datasets and achieves competitive performance with a baseline PatentBERT. Additional experiments with ModernBERT-base-VX and Mosaic-BERT-large demonstrate that scaling the model size and customizing the tokenizer further enhance performance on selected tasks. Notably, all ModernBERT variants retain substantially faster inference over - 3x that of PatentBERT - underscoring their suitability for time-sensitive applications. These results underscore the benefits of domain-specific pretraining and architectural improvements for patent-focused NLP tasks.

</details>


### [13] [SMARTER: A Data-efficient Framework to Improve Toxicity Detection with Explanation via Self-augmenting Large Language Models](https://arxiv.org/abs/2509.15174)
*Huy Nghiem, Advik Sachdeva, Hal Daumé III*

Main category: cs.CL

TL;DR: SMARTER is a data-efficient framework that leverages Large Language Models (LLMs) to improve toxicity detection and provide explanations, even with limited training data. It uses self-augmentation and cross-model refinement to achieve strong performance and explainability.


<details>
  <summary>Details</summary>
Motivation: Content moderation is crucial for social media platforms, but traditional methods are inefficient and lack transparency. Existing machine learning models require substantial training data and often lack explainability, hindering user trust and understanding.

Method: SMARTER is a two-stage framework. Stage 1 uses LLMs to generate synthetic explanations for both correct and incorrect labels via preference optimization. Stage 2 refines explanation quality through cross-model training, aligning weaker models with stronger ones stylistically and semantically.

Result: Experiments on three benchmark datasets (HateXplain, Latent Hate, and Implicit Hate) showed that SMARTER achieved up to a 13.5% macro-F1 improvement over standard few-shot baselines, using a fraction of the full training data.

Conclusion: SMARTER offers a scalable and practical approach for low-resource settings to achieve both strong toxicity detection performance and human-understandable explanations, promoting transparency and trustworthiness in content moderation.

Abstract: WARNING: This paper contains examples of offensive materials. Toxic content has become pervasive on social media platforms. We introduce SMARTER, a data-efficient two-stage framework for explainable content moderation using Large Language Models (LLMs). In Stage 1, we leverage LLMs' own outputs to generate synthetic explanations for both correct and incorrect labels, enabling alignment via preference optimization with minimal human supervision. In Stage 2, we refine explanation quality through cross-model training, allowing weaker models to align stylistically and semantically with stronger ones. Experiments on three benchmark tasks -- HateXplain, Latent Hate, and Implicit Hate -- demonstrate that SMARTER enables LLMs to achieve up to a 13.5% macro-F1 improvement over standard few-shot baselines while using only a fraction of the full training data. Our framework offers a scalable strategy for low-resource settings by harnessing LLMs' self-improving capabilities for both classification and explanation.

</details>


### [14] [TextMine: LLM-Powered Knowledge Extraction for Humanitarian Mine Action](https://arxiv.org/abs/2509.15098)
*Chenyue Zhou, Gürkan Solmaz, Flavio Cirillo, Kiril Gashteovski, Jonathan Fürst*

Main category: cs.CL

TL;DR: TextMine is a novel LLM-powered pipeline for extracting structured knowledge from unstructured Humanitarian Mine Action (HMA) reports, creating a valuable resource for improving demining efforts. It introduces a dedicated HMA ontology and a curated dataset for evaluation.


<details>
  <summary>Details</summary>
Motivation: Despite extensive best-practice knowledge generated in Humanitarian Mine Action, much remains locked in unstructured reports, hindering knowledge sharing and operational efficiency. This work aims to transform this unstructured data into a structured knowledge base to improve decision-making and demining operations.

Method: TextMine utilizes a prompt-based pipeline integrating document chunking, domain-aware prompting guided by a newly created HMA ontology, triple extraction using LLMs, and both reference-based and LLM-as-a-Judge evaluation. It processes entire paragraphs to enable coreference resolution and multi-step inference.

Result: Experiments demonstrate that ontology-aligned prompts significantly improve extraction accuracy (44.2%), reduce hallucinations (22.5%), and enhance format conformance (20.9%) compared to baseline methods. The system was validated on Cambodian reports.

Conclusion: TextMine represents the first LLM application for knowledge extraction in the HMA domain, providing a scalable and adaptable solution for transforming unstructured data into structured knowledge. This can be applied globally to improve demining efforts and other domains.

Abstract: Humanitarian Mine Action has generated extensive best-practice knowledge, but much remains locked in unstructured reports. We introduce TextMine, an ontology-guided pipeline that uses Large Language Models to extract knowledge triples from HMA texts. TextMine integrates document chunking, domain-aware prompting, triple extraction, and both reference-based and LLM-as-a-Judge evaluation. We also create the first HMA ontology and a curated dataset of real-world demining reports. Experiments show ontology-aligned prompts boost extraction accuracy by 44.2%, cut hallucinations by 22.5%, and improve format conformance by 20.9% over baselines. While validated on Cambodian reports, TextMine can adapt to global demining efforts or other domains, transforming unstructured data into structured knowledge.

</details>


### [15] [CLEAR: A Comprehensive Linguistic Evaluation of Argument Rewriting by Large Language Models](https://arxiv.org/abs/2509.15027)
*Thomas Huber, Christina Niklaus*

Main category: cs.CL

TL;DR: This paper introduces CLEAR, a comprehensive evaluation pipeline, to analyze how Large Language Models (LLMs) rewrite argumentative texts (ArgImp). The study finds that LLMs primarily shorten texts, increase average word length, and merge sentences, leading to improvements in persuasion and coherence.


<details>
  <summary>Details</summary>
Motivation: While LLMs excel in general text generation, their behavior in text rewriting, particularly in improving argumentative texts (ArgImp), remains underexplored. This research aims to understand the linguistic transformations LLMs perform during ArgImp and identify potential biases.

Method: The researchers developed CLEAR, a pipeline with 57 metrics across four linguistic levels (lexical, syntactic, semantic, and pragmatic). This pipeline was applied to various argumentation corpora to examine the qualities of LLM-rewritten arguments and compare the behavior of different LLMs.

Result: The analysis revealed that LLMs tend to shorten texts while simultaneously increasing average word length and merging sentences.  Furthermore, the rewritten arguments showed improvements in persuasion and coherence dimensions.

Conclusion: CLEAR provides a valuable tool for evaluating LLMs in ArgImp, offering insights into their linguistic strategies and highlighting the potential for enhancing argumentative writing. The findings contribute to a better understanding of LLM biases in this specific task.

Abstract: While LLMs have been extensively studied on general text generation tasks, there is less research on text rewriting, a task related to general text generation, and particularly on the behavior of models on this task. In this paper we analyze what changes LLMs make in a text rewriting setting. We focus specifically on argumentative texts and their improvement, a task named Argument Improvement (ArgImp). We present CLEAR: an evaluation pipeline consisting of 57 metrics mapped to four linguistic levels: lexical, syntactic, semantic and pragmatic. This pipeline is used to examine the qualities of LLM-rewritten arguments on a broad set of argumentation corpora and compare the behavior of different LLMs on this task and analyze the behavior of different LLMs on this task in terms of linguistic levels. By taking all four linguistic levels into consideration, we find that the models perform ArgImp by shortening the texts while simultaneously increasing average word length and merging sentences. Overall we note an increase in the persuasion and coherence dimensions.

</details>


### [16] [Cross-Modal Knowledge Distillation for Speech Large Language Models](https://arxiv.org/abs/2509.14930)
*Enzhi Wang, Qicheng Li, Zhiyuan Tang, Yuhang Jia*

Main category: cs.CL

TL;DR: This paper addresses the performance degradation and knowledge loss in speech large language models (LLMs) when incorporating speech capabilities. It proposes a cross-modal knowledge distillation framework to transfer knowledge from a text-based teacher model to a speech LLM, preserving textual knowledge and improving cross-modal alignment.


<details>
  <summary>Details</summary>
Motivation: Introducing speech capabilities to LLMs often leads to catastrophic forgetting of previously learned knowledge and modality inequivalence, resulting in degraded performance, especially in speech-based interactions. Existing approaches like freezing the backbone often fail to fully restore reasoning capabilities.

Method: The authors propose a cross-modal knowledge distillation framework that leverages both text-to-text and speech-to-text channels. This framework transfers knowledge from a text-based teacher model to a speech LLM, aiming to mitigate catastrophic forgetting and improve cross-modal alignment.

Result: Extensive experiments on dialogue and audio understanding tasks demonstrate that the proposed approach effectively preserves textual knowledge, improves cross-modal alignment, and enhances reasoning in speech-based interactions, leading to improved performance compared to baseline models.

Conclusion: This work provides a systematic evaluation of challenges in speech LLMs and introduces a novel knowledge distillation framework to address them. The findings highlight the importance of cross-modal knowledge transfer for building robust and effective speech-enabled LLMs.

Abstract: In this work, we present the first systematic evaluation of catastrophic forgetting and modality inequivalence in speech large language models, showing that introducing speech capabilities can degrade knowledge and reasoning even when inputs remain textual, and performance further decreases with spoken queries. To address these challenges, we propose a cross-modal knowledge distillation framework that leverages both text-to-text and speech-to-text channels to transfer knowledge from a text-based teacher model to a speech LLM. Extensive experiments on dialogue and audio understanding tasks validate the effectiveness of our approach in preserving textual knowledge, improving cross-modal alignment, and enhancing reasoning in speech-based interactions.

</details>


### [17] [A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation](https://arxiv.org/abs/2509.14886)
*Ye Shen, Junying Wang, Farong Wen, Yijin Guo, Qi Jia, Zicheng Zhang, Guangtao Zhai*

Main category: cs.CL

TL;DR: This paper introduces a 'multi-to-one interview' paradigm for evaluating Multi-Modal Large Language Models (MLLMs) that is more efficient than traditional full-coverage question-answering benchmarks while maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: Conventional MLLM evaluation methods using full-coverage Question-Answering (Q&A) are inefficient due to redundancy. The research aims to develop a more efficient and reliable evaluation paradigm inspired by human interview processes.

Method: The proposed paradigm consists of a two-stage interview strategy (pre-interview and formal interview), dynamic adjustment of interviewer weights for fairness, and an adaptive mechanism for question difficulty selection. It leverages a 'multi-to-one' approach, using fewer questions to assess model capabilities.

Result: Experiments on MMT-Bench, ScienceQA, and SEED-Bench demonstrate that the proposed paradigm achieves significantly higher correlation (up to 17.6% PLCC and 16.7% SRCC improvements) with full-coverage results than random sampling, while requiring fewer questions.

Conclusion: The multi-to-one interview paradigm offers a reliable and efficient alternative for large-scale MLLM benchmarking, providing a comprehensive, accurate, fair, and efficient evaluation of these models.

Abstract: The rapid progress of Multi-Modal Large Language Models (MLLMs) has spurred the creation of numerous benchmarks. However, conventional full-coverage Question-Answering evaluations suffer from high redundancy and low efficiency. Inspired by human interview processes, we propose a multi-to-one interview paradigm for efficient MLLM evaluation. Our framework consists of (i) a two-stage interview strategy with pre-interview and formal interview phases, (ii) dynamic adjustment of interviewer weights to ensure fairness, and (iii) an adaptive mechanism for question difficulty-level chosen. Experiments on different benchmarks show that the proposed paradigm achieves significantly higher correlation with full-coverage results than random sampling, with improvements of up to 17.6% in PLCC and 16.7% in SRCC, while reducing the number of required questions. These findings demonstrate that the proposed paradigm provides a reliable and efficient alternative for large-scale MLLM benchmarking.

</details>


### [18] [Empathy-R1: A Chain-of-Empathy and Reinforcement Learning Framework for Long-Form Mental Health Support](https://arxiv.org/abs/2509.14851)
*Xianrong Yao, Dong She, Chenxu Zhang, Yimeng Zhang, Yueru Sun, Noman Ahmed, Yang Gao, Zhanpeng Jin*

Main category: cs.CL

TL;DR: Empathy-R1 is a novel framework that combines Chain-of-Empathy reasoning with Reinforcement Learning to improve the quality of AI-powered mental health support for long-form counseling texts (LCTs), particularly in Chinese.


<details>
  <summary>Details</summary>
Motivation: Existing Large Language Models (LLMs) often lack the structured reasoning and therapeutic depth needed to effectively respond to complex mental health needs expressed in long counseling texts, especially within a Chinese cultural context, highlighting a need for more nuanced and contextually appropriate AI support.

Method: Empathy-R1 utilizes a Chain-of-Empathy (CoE) paradigm, inspired by cognitive-behavioral therapy, to guide the model's reasoning about emotions, causes, and intentions. It employs a two-stage training process: Supervised Fine-Tuning to instill the CoE structure, followed by Reinforcement Learning with a dedicated reward model to refine therapeutic relevance and contextual appropriateness, and is trained on a new large-scale Chinese dataset, Empathy-QA.

Result: Experiments demonstrate that Empathy-R1 outperforms strong baselines on both automatic metrics and human evaluations, achieving a Win@1 rate of 44.30% on a new benchmark, indicating significantly improved response quality and user preference.

Conclusion: Empathy-R1 represents a significant advancement in developing responsible and genuinely beneficial AI for mental health support by enabling interpretable, contextually nuanced responses, addressing the treatment gap and providing accessible support for individuals struggling with mental health challenges.

Abstract: Empathy is critical for effective mental health support, especially when addressing Long Counseling Texts (LCTs). However, existing Large Language Models (LLMs) often generate replies that are semantically fluent but lack the structured reasoning necessary for genuine psychological support, particularly in a Chinese context. To bridge this gap, we introduce Empathy-R1, a novel framework that integrates a Chain-of-Empathy (CoE) reasoning process with Reinforcement Learning (RL) to enhance response quality for LCTs. Inspired by cognitive-behavioral therapy, our CoE paradigm guides the model to sequentially reason about a help-seeker's emotions, causes, and intentions, making its thinking process both transparent and interpretable. Our framework is empowered by a new large-scale Chinese dataset, Empathy-QA, and a two-stage training process. First, Supervised Fine-Tuning instills the CoE's reasoning structure. Subsequently, RL, guided by a dedicated reward model, refines the therapeutic relevance and contextual appropriateness of the final responses. Experiments show that Empathy-R1 achieves strong performance on key automatic metrics. More importantly, human evaluations confirm its superiority, showing a clear preference over strong baselines and achieving a Win@1 rate of 44.30% on our new benchmark. By enabling interpretable and contextually nuanced responses, Empathy-R1 represents a significant advancement in developing responsible and genuinely beneficial AI for mental health support.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [19] [WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance](https://arxiv.org/abs/2509.15130)
*Chenxi Song, Yanming Yang, Tong Zhao, Ruibo Li, Chi Zhang*

Main category: cs.GR

TL;DR: WorldForge is a training-free framework that leverages pre-trained video diffusion models for 3D/4D tasks like scene generation and re-rendering, enabling precise camera trajectory control without retraining.


<details>
  <summary>Details</summary>
Motivation: Existing video diffusion models have strong generative priors but lack controllability and geometric consistency for 3D/4D tasks, often requiring retraining which degrades performance and is computationally expensive.

Method: WorldForge introduces three modules: Intra-Step Recursive Refinement for trajectory injection, Flow-Gated Latent Fusion to decouple motion from appearance, and Dual-Path Self-Corrective Guidance to adaptively correct trajectory drift. These components work together to inject fine-grained, trajectory-aligned guidance during inference.

Result: Extensive experiments demonstrate WorldForge's superiority in realism, trajectory consistency, and visual fidelity across diverse benchmarks, achieving accurate motion control and photorealistic content generation.

Conclusion: WorldForge establishes a novel plug-and-play paradigm for controllable video synthesis, offering a new perspective on leveraging generative priors for spatial intelligence and enabling applications in novel view synthesis, 3D scene generation, and dynamic scene reconstruction.

Abstract: Recent video diffusion models demonstrate strong potential in spatial intelligence tasks due to their rich latent world priors. However, this potential is hindered by their limited controllability and geometric inconsistency, creating a gap between their strong priors and their practical use in 3D/4D tasks. As a result, current approaches often rely on retraining or fine-tuning, which risks degrading pretrained knowledge and incurs high computational costs. To address this, we propose WorldForge, a training-free, inference-time framework composed of three tightly coupled modules. Intra-Step Recursive Refinement introduces a recursive refinement mechanism during inference, which repeatedly optimizes network predictions within each denoising step to enable precise trajectory injection. Flow-Gated Latent Fusion leverages optical flow similarity to decouple motion from appearance in the latent space and selectively inject trajectory guidance into motion-related channels. Dual-Path Self-Corrective Guidance compares guided and unguided denoising paths to adaptively correct trajectory drift caused by noisy or misaligned structural signals. Together, these components inject fine-grained, trajectory-aligned guidance without training, achieving both accurate motion control and photorealistic content generation. Extensive experiments across diverse benchmarks validate our method's superiority in realism, trajectory consistency, and visual fidelity. This work introduces a novel plug-and-play paradigm for controllable video synthesis, offering a new perspective on leveraging generative priors for spatial intelligence.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [20] [Learning Mechanistic Subtypes of Neurodegeneration with a Physics-Informed Variational Autoencoder Mixture Model](https://arxiv.org/abs/2509.15124)
*Sanduni Pinnawala, Annabelle Hartanto, Ivor J. A. Simpson, Peter A. Wijeratne*

Main category: eess.IV

TL;DR: This paper introduces BrainPhys, a novel physics-informed variational autoencoder (VAE) mixture model, to learn mixtures of latent dynamic models governed by physics-based partial differential equations (PDEs) for neurodegenerative diseases, addressing limitations of single-PDE approaches.


<details>
  <summary>Details</summary>
Motivation: Current physics-integrated machine learning methods for neurodegenerative diseases are limited by their reliance on a single PDE, failing to account for disease heterogeneity and leading to model misspecification and degeneracy.  Neurodegenerative diseases like Alzheimer's are complex and likely driven by multiple mechanisms and subtypes.

Method: The authors developed BrainPhys, a deep generative model that integrates reaction-diffusion PDEs within a VAE mixture model framework. This allows for the inference of subtypes of interpretable latent variables (e.g., diffusivity and reaction rates) from neuroimaging data, effectively modeling multiple PDEs simultaneously.

Result: The model successfully recovered clusters of mechanistic models and their parameters using synthetic data.  Application to Alzheimer's Disease Neuroimaging Initiative (ADNI) data revealed evidence supporting a two-component mixture model.

Conclusion: BrainPhys offers a powerful new approach for uncovering mechanistic subtypes of neurodegenerative diseases from neuroimaging data, providing enhanced interpretability and utility compared to traditional methods and addressing critical challenges in biophysical model inference.

Abstract: Modelling the underlying mechanisms of neurodegenerative diseases demands methods that capture heterogeneous and spatially varying dynamics from sparse, high-dimensional neuroimaging data. Integrating partial differential equation (PDE) based physics knowledge with machine learning provides enhanced interpretability and utility over classic numerical methods. However, current physics-integrated machine learning methods are limited to considering a single PDE, severely limiting their application to diseases where multiple mechanisms are responsible for different groups (i.e., subtypes) and aggravating problems with model misspecification and degeneracy. Here, we present a deep generative model for learning mixtures of latent dynamic models governed by physics-based PDEs, going beyond traditional approaches that assume a single PDE structure. Our method integrates reaction-diffusion PDEs within a variational autoencoder (VAE) mixture model framework, supporting inference of subtypes of interpretable latent variables (e.g. diffusivity and reaction rates) from neuroimaging data. We evaluate our method on synthetic benchmarks and demonstrate its potential for uncovering mechanistic subtypes of Alzheimer's disease progression from positron emission tomography (PET) data.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [21] [Generalizable Geometric Image Caption Synthesis](https://arxiv.org/abs/2509.15217)
*Yue Xin, Wenyuan Wang, Rui Pan, Ruida Wang, Howard Meng, Renjie Pi, Shizhe Diao, Tong Zhang*

Main category: cs.AI

TL;DR: This paper introduces a novel data synthesis pipeline using Reinforcement Learning with Verifiable Rewards (RLVR) to generate high-quality image-text pairs for geometric reasoning. The generated dataset significantly improves the general reasoning capabilities of multimodal large language models (MLLMs) across various tasks.


<details>
  <summary>Details</summary>
Motivation: Existing datasets for geometric reasoning lack high-quality image-text alignment, hindering the development of strong cross-modal reasoning capabilities in MLLMs. Template-based data synthesis pipelines often fail to generalize beyond predefined templates.

Method: The proposed pipeline synthesizes geometric images from 50 basic geometric relations and refines captions using RLVR. A reward function is designed to incorporate both reasoning and caption quality, derived from mathematical problem-solving tasks. The RAFT method is employed for reinforcement learning.

Result: The generated dataset enhances the general reasoning capabilities of MLLMs, leading to accuracy improvements of 2.8%-4.8% in statistics, arithmetic, algebraic, and numerical tasks with non-geometric input images, and 2.4%-3.9% improvements in Art & Design and Tech & Engineering tasks.

Conclusion: This work demonstrates the effectiveness of RLVR in generating datasets that improve the general reasoning abilities of MLLMs, particularly in geometric and mathematical reasoning, and shows promising generalization to non-geometric tasks.

Abstract: Multimodal large language models have various practical applications that demand strong reasoning abilities. Despite recent advancements, these models still struggle to solve complex geometric problems. A key challenge stems from the lack of high-quality image-text pair datasets for understanding geometric images. Furthermore, most template-based data synthesis pipelines typically fail to generalize to questions beyond their predefined templates. In this paper, we bridge this gap by introducing a complementary process of Reinforcement Learning with Verifiable Rewards (RLVR) into the data generation pipeline. By adopting RLVR to refine captions for geometric images synthesized from 50 basic geometric relations and using reward signals derived from mathematical problem-solving tasks, our pipeline successfully captures the key features of geometry problem-solving. This enables better task generalization and yields non-trivial improvements. Furthermore, even in out-of-distribution scenarios, the generated dataset enhances the general reasoning capabilities of multimodal large language models, yielding accuracy improvements of $2.8\%\text{-}4.8\%$ in statistics, arithmetic, algebraic, and numerical tasks with non-geometric input images of MathVista and MathVerse, along with $2.4\%\text{-}3.9\%$ improvements in Art, Design, Tech, and Engineering tasks in MMMU.

</details>


### [22] [A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical Decision-making](https://arxiv.org/abs/2509.14998)
*Xiao Wu, Ting-Zhu Huang, Liang-Jian Deng, Yanyuan Qiao, Imran Razzak, Yutong Xie*

Main category: cs.AI

TL;DR: This paper introduces KAMAC, a novel knowledge-driven adaptive multi-agent collaboration framework for LLMs, designed to enhance medical decision-making by dynamically forming expert teams based on evolving diagnostic context.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent LLM approaches for medical decision-making are limited by static, pre-assigned roles, hindering adaptability and dynamic knowledge integration. The paper aims to address this limitation by mimicking the flexible teamwork of multidisciplinary medical teams.

Method: KAMAC begins with initial expert agents and uses knowledge-driven discussions to identify and recruit additional specialists as needed. This dynamic team formation allows for flexible and scalable collaboration, with decisions finalized through reviewing updated agent comments.

Result: Experiments on two real-world medical benchmarks demonstrate that KAMAC significantly outperforms both single-agent and advanced multi-agent methods, particularly in complex clinical scenarios like cancer prognosis.

Conclusion: KAMAC offers a promising approach to improve medical decision-making by enabling adaptive and dynamic collaboration among LLM agents, ultimately leading to more accurate and comprehensive diagnoses and treatment plans.

Abstract: Medical decision-making often involves integrating knowledge from multiple clinical specialties, typically achieved through multidisciplinary teams. Inspired by this collaborative process, recent work has leveraged large language models (LLMs) in multi-agent collaboration frameworks to emulate expert teamwork. While these approaches improve reasoning through agent interaction, they are limited by static, pre-assigned roles, which hinder adaptability and dynamic knowledge integration. To address these limitations, we propose KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration framework that enables LLM agents to dynamically form and expand expert teams based on the evolving diagnostic context. KAMAC begins with one or more expert agents and then conducts a knowledge-driven discussion to identify and fill knowledge gaps by recruiting additional specialists as needed. This supports flexible, scalable collaboration in complex clinical scenarios, with decisions finalized through reviewing updated agent comments. Experiments on two real-world medical benchmarks demonstrate that KAMAC significantly outperforms both single-agent and advanced multi-agent methods, particularly in complex clinical scenarios (i.e., cancer prognosis) requiring dynamic, cross-specialty expertise. Our code is publicly available at: https://github.com/XiaoXiao-Woo/KAMAC.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [23] [Forecasting and Visualizing Air Quality from Sky Images with Vision-Language Models](https://arxiv.org/abs/2509.15076)
*Mohammad Saleh Vahdatpour, Maryam Eyvazi, Yanqing Zhang*

Main category: cs.LG

TL;DR: This paper proposes an AI agent that predicts air pollution levels and generates realistic visualizations from sky images using vision-language models, addressing limitations of traditional monitoring systems.


<details>
  <summary>Details</summary>
Motivation: Conventional air quality monitoring systems often have limited spatial coverage and accessibility. There's a need for more scalable, cost-effective, and interpretable methods for air quality forecasting, especially in low-resource settings.

Method: The approach combines statistical texture analysis with supervised learning for pollution classification and leverages vision-language models (VLMs) to generate interpretable visualizations of air quality conditions. It integrates sky images with generative modeling to simulate varying pollution degrees.

Result: The method effectively estimates pollution levels and synthesizes semantically consistent visuals from urban sky images. The system demonstrates improved accuracy and robustness compared to previous methods.

Conclusion: This work provides a foundation for user-facing interfaces that improve transparency and support informed environmental decision-making, potentially enhancing situational awareness and encouraging behavioral responses based on real-time forecasts. Future work will focus on energy-efficient deployment using green CNN architectures and FPGA-based incremental learning.

Abstract: Air pollution remains a critical threat to public health and environmental sustainability, yet conventional monitoring systems are often constrained by limited spatial coverage and accessibility. This paper proposes an AI-driven agent that predicts ambient air pollution levels from sky images and synthesizes realistic visualizations of pollution scenarios using generative modeling. Our approach combines statistical texture analysis with supervised learning for pollution classification, and leverages vision-language model (VLM)-guided image generation to produce interpretable representations of air quality conditions. The generated visuals simulate varying degrees of pollution, offering a foundation for user-facing interfaces that improve transparency and support informed environmental decision-making. These outputs can be seamlessly integrated into intelligent applications aimed at enhancing situational awareness and encouraging behavioral responses based on real-time forecasts. We validate our method using a dataset of urban sky images and demonstrate its effectiveness in both pollution level estimation and semantically consistent visual synthesis. The system design further incorporates human-centered user experience principles to ensure accessibility, clarity, and public engagement in air quality forecasting. To support scalable and energy-efficient deployment, future iterations will incorporate a green CNN architecture enhanced with FPGA-based incremental learning, enabling real-time inference on edge platforms.

</details>


### [24] [Communication Efficient Split Learning of ViTs with Attention-based Double Compression](https://arxiv.org/abs/2509.15058)
*Federico Alvetreti, Jary Pomponi, Paolo Di Lorenzo, Simone Scardapane*

Main category: cs.LG

TL;DR: This paper introduces Attention-based Double Compression (ADC), a novel Split Learning (SL) framework that significantly reduces communication overhead in Vision Transformer (ViT) training by merging similar activations and discarding less important tokens.


<details>
  <summary>Details</summary>
Motivation: Traditional deep learning training requires substantial computational resources and data transmission, raising privacy concerns. Split Learning addresses this by partitioning the model between edge devices and a cloud server, but communication bottlenecks remain a challenge.

Method: ADC employs two parallel compression strategies: (1) merging activations of similar samples based on attention scores, and (2) discarding the least meaningful tokens. This approach compresses both activations and gradients without requiring additional tuning or approximations.

Result: Experimental results on CIFAR100 demonstrate that ADC outperforms state-of-the-art SL frameworks across various compression ratios, maintaining high accuracy even under extreme communication constraints.

Conclusion: ADC provides a communication-efficient solution for Split Learning with ViTs, enabling reliable training even with limited bandwidth and paving the way for wider adoption of federated learning in resource-constrained environments.

Abstract: This paper proposes a novel communication-efficient Split Learning (SL) framework, named Attention-based Double Compression (ADC), which reduces the communication overhead required for transmitting intermediate Vision Transformers activations during the SL training process. ADC incorporates two parallel compression strategies. The first one merges samples' activations that are similar, based on the average attention score calculated in the last client layer; this strategy is class-agnostic, meaning that it can also merge samples having different classes, without losing generalization ability nor decreasing final results. The second strategy follows the first and discards the least meaningful tokens, further reducing the communication cost. Combining these strategies not only allows for sending less during the forward pass, but also the gradients are naturally compressed, allowing the whole model to be trained without additional tuning or approximations of the gradients. Simulation results demonstrate that Attention-based Double Compression outperforms state-of-the-art SL frameworks by significantly reducing communication overheads while maintaining high accuracy.

</details>


### [25] [FlowRL: Matching Reward Distributions for LLM Reasoning](https://arxiv.org/abs/2509.15207)
*Xuekai Zhu, Daixuan Cheng, Dinghuai Zhang, Hengli Li, Kaiyan Zhang, Che Jiang, Youbang Sun, Ermo Hua, Yuxin Zuo, Xingtai Lv, Qizheng Zhang, Lin Chen, Fanghao Shao, Bo Xue, Yunchong Song, Zhenjie Yang, Ganqu Cui, Ning Ding, Jianfeng Gao, Xiaodong Liu, Bowen Zhou, Hongyuan Mei, Zhouhan Lin*

Main category: cs.LG

TL;DR: FlowRL is a new reinforcement learning algorithm for LLMs that matches the full reward distribution instead of maximizing rewards, leading to more diverse and generalizable reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing reward-maximizing RL methods for LLMs (like PPO and GRPO) tend to over-optimize dominant reward signals, neglecting less frequent but valid reasoning paths and reducing diversity.

Method: FlowRL transforms scalar rewards into a normalized target distribution using a learnable partition function and minimizes the reverse KL divergence between the policy and this target distribution, effectively balancing the reward distribution.

Result: FlowRL achieves a significant average improvement of 10.0% over GRPO and 5.1% over PPO on math benchmarks, and consistently performs better on code reasoning tasks.

Conclusion: Reward distribution-matching with FlowRL is a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning, addressing the mode-collapse limitations of traditional reward-maximizing approaches.

Abstract: We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning (RL). Recent advanced reasoning models adopt reward-maximizing methods (\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, thus reducing diversity. In contrast, we transform scalar rewards into a normalized target distribution using a learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution. We implement this idea as a flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories. We conduct experiments on math and code reasoning tasks: FlowRL achieves a significant average improvement of $10.0\%$ over GRPO and $5.1\%$ over PPO on math benchmarks, and performs consistently better on code reasoning tasks. These results highlight reward distribution-matching as a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning.

</details>


### [26] [Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation](https://arxiv.org/abs/2509.15194)
*Yujun Zhou, Zhenwen Liang, Haolin Liu, Wenhao Yu, Kishan Panaganti, Linfeng Song, Dian Yu, Xiangliang Zhang, Haitao Mi, Dong Yu*

Main category: cs.LG

TL;DR: This paper introduces EVOL-RL, a novel label-free reinforcement learning method that enables language models to continuously improve without collapsing into repetitive and brittle responses. It balances stability (selection) with variation (novelty) to maintain exploration and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing label-free methods for language model improvement often lead to 'entropy collapse,' where models produce shorter, less diverse, and less robust outputs. The goal is to develop models that can autonomously evolve – continuously learning and improving without labels or external judges, while preserving exploration and generalization ability.

Method: EVOL-RL combines majority voting for stable answers (selection) with a novelty-aware reward that encourages responses differing from previously generated ones (variation), measured in semantic space. It utilizes GRPO, asymmetric clipping, and an entropy regularizer to prevent collapse and sustain search.

Result: EVOL-RL significantly outperforms the majority-only TTRL baseline, achieving substantial improvements in pass@1 and pass@16 on AIME24 and demonstrating enhanced generalization across domains like GPQA. It also boosts performance in the RLVR setting.

Conclusion: EVOL-RL provides a practical and effective approach for label-free language model evolution, enabling continuous self-improvement, preventing diversity collapse, and unlocking stronger generalization capabilities, paving the way for more autonomous and adaptable AI systems.

Abstract: Large language models (LLMs) are increasingly trained with reinforcement learning from verifiable rewards (RLVR), yet real-world deployment demands models that can self-improve without labels or external judges. Existing label-free methods, confidence minimization, self-consistency, or majority-vote objectives, stabilize learning but steadily shrink exploration, causing an entropy collapse: generations become shorter, less diverse, and brittle. Unlike prior approaches such as Test-Time Reinforcement Learning (TTRL), which primarily adapt models to the immediate unlabeled dataset at hand, our goal is broader: to enable general improvements without sacrificing the model's inherent exploration capacity and generalization ability, i.e., evolving. We formalize this issue and propose EVolution-Oriented and Label-free Reinforcement Learning (EVOL-RL), a simple rule that couples stability with variation under a label-free setting. EVOL-RL keeps the majority-voted answer as a stable anchor (selection) while adding a novelty-aware reward that favors responses whose reasoning differs from what has already been produced (variation), measured in semantic space. Implemented with GRPO, EVOL-RL also uses asymmetric clipping to preserve strong signals and an entropy regularizer to sustain search. This majority-for-selection + novelty-for-variation design prevents collapse, maintains longer and more informative chains of thought, and improves both pass@1 and pass@n. EVOL-RL consistently outperforms the majority-only TTRL baseline; e.g., training on label-free AIME24 lifts Qwen3-4B-Base AIME25 pass@1 from TTRL's 4.6% to 16.4%, and pass@16 from 18.5% to 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks stronger generalization across domains (e.g., GPQA). Furthermore, we demonstrate that EVOL-RL also boosts performance in the RLVR setting, highlighting its broad applicability.

</details>


### [27] [Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning](https://arxiv.org/abs/2509.15157)
*Shiwan Zhao, Xuyang Zhao, Jiaming Zhou, Aobo Kong, Qicheng Li, Yong Qin*

Main category: cs.LG

TL;DR: This paper introduces a data rewriting framework for stable off-policy supervised fine-tuning (SFT) of large language models, proactively shrinking the policy gap by rewriting incorrect solutions and leveraging expert demonstrations only when necessary.


<details>
  <summary>Details</summary>
Motivation: Standard SFT suffers from training instability and high variance due to the policy gap between expert demonstrations and the evolving model policy. Existing methods passively constrain updates, but don't actively reduce this gap.

Method: The proposed framework rewrites SFT data by sampling multiple responses. Correct solutions are kept as on-policy data. Incorrect solutions are re-solved using the ground truth as a reference (digest-and-retell), and expert demonstrations are used as a fallback when self-solve and re-solve fail.

Result: Experiments on five mathematical reasoning benchmarks demonstrate consistent and significant gains over vanilla SFT and the state-of-the-art Dynamic Fine-Tuning (DFT) approach.

Conclusion: This data rewriting technique effectively aligns the training distribution with the target policy, reducing importance sampling variance and stabilizing off-policy fine-tuning, providing a more robust foundation for SFT and future RL-based approaches.

Abstract: Supervised fine-tuning (SFT) of large language models can be viewed as an off-policy learning problem, where expert demonstrations come from a fixed behavior policy while training aims to optimize a target policy. Importance sampling is the standard tool for correcting this distribution mismatch, but large policy gaps lead to high variance and training instability. Existing approaches mitigate this issue using KL penalties or clipping, which passively constrain updates rather than actively reducing the gap. We propose a simple yet effective data rewriting framework that proactively shrinks the policy gap by keeping correct solutions as on-policy data and rewriting incorrect ones with guided re-solving, falling back to expert demonstrations only when needed. This aligns the training distribution with the target policy before optimization, reducing importance sampling variance and stabilizing off-policy fine-tuning. Experiments on five mathematical reasoning benchmarks demonstrate consistent and significant gains over both vanilla SFT and the state-of-the-art Dynamic Fine-Tuning (DFT) approach. The data and code will be released at https://github.com/NKU-HLT/Off-Policy-SFT.

</details>


### [28] [TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference](https://arxiv.org/abs/2509.15110)
*Dan Zhang, Min Cai, Jonathan Li, Ziniu Hu, Yisong Yue, Yuxiao Dong, Jie Tang*

Main category: cs.LG

TL;DR: TDRM introduces a temporal difference regularization method to train smoother and more reliable reward models for language model reinforcement learning and inference. It improves alignment with long-term objectives and leads to significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Existing reward models often lack temporal consistency, leading to unstable training and ineffective policy updates in language model reinforcement learning. This inconsistency makes it difficult to distinguish beneficial reasoning steps.

Method: TDRM minimizes temporal differences during reward model training using n-step temporal difference learning. It can be used as a supplement to verifiable reward methods and integrated into actor-critic style online RL loops.

Result: TDRM improves performance in both Best-of-N and tree-search settings. When combined with RLVR, it achieves comparable performance with significantly less data (2.5k vs. 50.1k) and yields higher-quality language model policies across multiple models.

Conclusion: TDRM provides a simple yet effective way to improve the quality and efficiency of reward models, leading to more stable and data-efficient reinforcement learning for language models and better overall performance.

Abstract: Reward models are central to both reinforcement learning (RL) with language models and inference-time verification. However, existing reward models often lack temporal consistency, leading to ineffective policy updates and unstable RL training. We introduce TDRM, a method for learning smoother and more reliable reward models by minimizing temporal differences during training. This temporal-difference (TD) regularization produces smooth rewards and improves alignment with long-term objectives. Incorporating TDRM into the actor-critic style online RL loop yields consistent empirical gains. It is worth noting that TDRM is a supplement to verifiable reward methods, and both can be used in series. Experiments show that TD-trained process reward models (PRMs) improve performance across Best-of-N (up to 6.6%) and tree-search (up to 23.7%) settings. When combined with Reinforcement Learning with Verifiable Rewards (RLVR), TD-trained PRMs lead to more data-efficient RL -- achieving comparable performance with just 2.5k data to what baseline methods require 50.1k data to attain -- and yield higher-quality language model policies on 8 model variants (5 series), e.g., Qwen2.5-(0.5B, 1,5B), GLM4-9B-0414, GLM-Z1-9B-0414, Qwen2.5-Math-(1.5B, 7B), and DeepSeek-R1-Distill-Qwen-(1.5B, 7B). We release all code at https://github.com/THUDM/TDRM.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [29] [M4Diffuser: Multi-View Diffusion Policy with Manipulability-Aware Control for Robust Mobile Manipulation](https://arxiv.org/abs/2509.14980)
*Ju Dong, Lei Zhang, Liding Zhang, Yao Ling, Yu Fu, Kaixin Bai, Zoltán-Csaba Márton, Zhenshan Bing, Zhaopeng Chen, Alois Christian Knoll, Jianwei Zhang*

Main category: cs.RO

TL;DR: M4Diffuser is a novel hybrid framework for robust mobile manipulation that combines a multi-view diffusion policy for high-level goal generation with a manipulability-aware QP controller for efficient and stable execution.


<details>
  <summary>Details</summary>
Motivation: Existing mobile manipulation approaches struggle with limited fields of view, generalization, computational inefficiency, and robustness near singularities. Classical controllers lack efficiency and manipulability, while learning-based methods often lack stability and are susceptible to occlusion.

Method: M4Diffuser integrates a Multi-View Diffusion Transformer Policy leveraging proprioceptive states and multiple camera views to generate end-effector goals. These goals are then executed by a Reduced and Manipulability-aware QP (ReM-QP) controller, which eliminates slack variables and incorporates manipulability preferences.

Result: Experiments in simulation and real-world environments demonstrate that M4Diffuser achieves 7%–56% higher success rates and reduces collisions by 3%–31% compared to baselines. It also exhibits strong generalization to unseen tasks.

Conclusion: M4Diffuser paves the way for reliable mobile manipulation in unstructured environments by combining the strengths of diffusion policies and optimized control, enabling robust whole-body coordination and improved generalization capabilities.

Abstract: Mobile manipulation requires the coordinated control of a mobile base and a robotic arm while simultaneously perceiving both global scene context and fine-grained object details. Existing single-view approaches often fail in unstructured environments due to limited fields of view, exploration, and generalization abilities. Moreover, classical controllers, although stable, struggle with efficiency and manipulability near singularities. To address these challenges, we propose M4Diffuser, a hybrid framework that integrates a Multi-View Diffusion Policy with a novel Reduced and Manipulability-aware QP (ReM-QP) controller for mobile manipulation. The diffusion policy leverages proprioceptive states and complementary camera perspectives with both close-range object details and global scene context to generate task-relevant end-effector goals in the world frame. These high-level goals are then executed by the ReM-QP controller, which eliminates slack variables for computational efficiency and incorporates manipulability-aware preferences for robustness near singularities. Comprehensive experiments in simulation and real-world environments show that M4Diffuser achieves 7 to 56 percent higher success rates and reduces collisions by 3 to 31 percent over baselines. Our approach demonstrates robust performance for smooth whole-body coordination, and strong generalization to unseen tasks, paving the way for reliable mobile manipulation in unstructured environments. Details of the demo and supplemental material are available on our project website https://sites.google.com/view/m4diffuser.

</details>
