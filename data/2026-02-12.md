<div id=toc></div>

# Table of Contents

- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 5]
- [stat.ML](#stat.ML) [Total: 1]


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [1] [NMRTrans: Structure Elucidation from Experimental NMR Spectra via Set Transformers](https://arxiv.org/abs/2602.10158)
*Liujia Yang, Zhuo Yang, Jiaqing Xie, Yubin Wang, Ben Gao, Tianfan Fu, Xingjian Wei, Jiaxing Sun, Jiang Wu, Conghui He, Yuqiang Li, Qinying Gu*

Main category: physics.chem-ph

TL;DR: 本文提出了NMRTrans，一种基于集合Transformer的NMR谱结构解析模型，该模型仅在实验NMR谱数据集上训练，并在实验基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统NMR谱结构解析耗时且依赖专家经验，现有方法依赖于计算谱数据，在实验数据上表现不佳。因此，需要一种能够有效处理实验数据的结构感知的模型。

Method: 构建了大规模实验NMR谱数据集NMRSpec，并提出了NMRTrans模型，该模型将谱线表示为无序的峰集，并利用集合Transformer进行结构解析。

Result: NMRTrans在实验基准测试中实现了最先进的性能，Top-10准确率比最强的基线提高了17.82个百分点（从43.33%提升至61.15%）。

Conclusion: 实验数据和结构感知的架构对于可靠的NMR结构解析至关重要，NMRTrans的成功表明了该方法在自动化化学结构解析方面的潜力。

Abstract: Nuclear Magnetic Resonance (NMR) spectroscopy is fundamental for molecular structure elucidation, yet interpreting spectra at scale remains time-consuming and highly expertise-dependent. While recent spectrum-as-language modeling and retrieval-based methods have shown promise, they rely heavily on large corpora of computed spectra and exhibit notable performance drops when applied to experimental measurements. To address these issues, we build NMRSpec, a large-scale corpus of experimental $^1$H and $^{13}$C spectra mined from chemical literature, and propose NMRTrans, which models spectra as unordered peak sets and aligns the model's inductive bias with the physical nature of NMR. To our best knowledge, NMRTrans is the first NMR Transformer trained solely on large-scale experimental spectra and achieves state-of-the-art performance on experimental benchmarks, improving Top-10 Accuracy over the strongest baseline by +17.82 points (61.15% vs. 43.33%), and underscoring the importance of experimental data and structure-aware architectures for reliable NMR structure elucidation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [2] [Statistical Learning Analysis of Physics-Informed Neural Networks](https://arxiv.org/abs/2602.11097)
*David A. Barajas-Solano*

Main category: cs.LG

TL;DR: 本文从统计学习的角度分析了物理信息神经网络 (PINNs)，将物理惩罚项视为无限的间接数据来源，并利用奇异学习理论工具分析了 PINNs 的参数估计。


<details>
  <summary>Details</summary>
Motivation: 传统统计工具难以分析深度学习，而物理信息学习中的 PINNs 也具有奇异特性。现有研究对 PINNs 的损失景观、泛化能力等理解仍不完整，因此需要新的分析方法。

Method: 将 PINNs 的学习问题重新表述为统计学习问题，将物理惩罚项视为拟合 PINNs 残差分布和真实数据生成分布的 Kullback-Leibler 散度最小化问题。并运用奇异学习理论中的局部学习系数 (LLC) 来研究热方程 IBVP 的 PINNs 损失特征。

Result: 分析表明，物理信息学习是奇异学习问题，物理惩罚项可以被理解为无限的间接数据来源，而非简单的正则化项。通过计算 LLC，可以研究 PINNs 损失的特征。

Conclusion: 该分析为量化 PINNs 的预测不确定性和外推能力提供了新的视角，并为理解 PINNs 的训练和性能提供了更深入的理论基础。

Abstract: We study the training and performance of physics-informed learning for initial and boundary value problems (IBVP) with physics-informed neural networks (PINNs) from a statistical learning perspective. Specifically, we restrict ourselves to parameterizations with hard initial and boundary condition constraints and reformulate the problem of estimating PINN parameters as a statistical learning problem. From this perspective, the physics penalty on the IBVP residuals can be better understood not as a regularizing term bus as an infinite source of indirect data, and the learning process as fitting the PINN distribution of residuals $p(y \mid x, t, w) q(x, t) $ to the true data-generating distribution $δ(0) q(x, t)$ by minimizing the Kullback-Leibler divergence between the true and PINN distributions. Furthermore, this analysis show that physics-informed learning with PINNs is a singular learning problem, and we employ singular learning theory tools, namely the so-called Local Learning Coefficient (Lau et al., 2025) to analyze the estimates of PINN parameters obtained via stochastic optimization for a heat equation IBVP. Finally, we discuss implications of this analysis on the quantification of predictive uncertainty of PINNs and the extrapolation capacity of PINNs.

</details>


### [3] [Coarse-Grained Boltzmann Generators](https://arxiv.org/abs/2602.10637)
*Weilong Chen, Bojun Zhao, Jan Eckwert, Julija Zavadlav*

Main category: cs.LG

TL;DR: 本文提出了一种名为粗粒化玻尔兹曼生成器（CG-BG）的新框架，它将粗粒化建模与玻尔兹曼生成器的精确重要性采样相结合，以实现对更大分子系统的无偏采样。


<details>
  <summary>Details</summary>
Motivation: 传统的玻尔兹曼生成器（BG）在处理高维分子系统时存在可扩展性问题，而粗粒化方法虽然可以降低维度，但通常缺乏确保统计正确性的重加权过程。因此，需要一种能够结合两者优势的方法。

Method: CG-BG在粗粒化坐标空间中运行，使用学习的平均力势（PMF）对基于流的模型生成的样本进行重加权。PMF通过力匹配从快速收敛的数据中高效学习，并结合了学习的势能和流模型。

Result: 实验结果表明，CG-BG能够忠实地捕捉到显式溶剂介导的复杂相互作用，即使在高度降低的表示形式下，也能够实现对更大分子系统的无偏采样。

Conclusion: CG-BG为对更大分子系统进行无偏采样提供了一条可扩展的途径，有望解决统计物理中长期存在的平衡采样难题。

Abstract: Sampling equilibrium molecular configurations from the Boltzmann distribution is a longstanding challenge. Boltzmann Generators (BGs) address this by combining exact-likelihood generative models with importance sampling, but their practical scalability is limited. Meanwhile, coarse-grained surrogates enable the modeling of larger systems by reducing effective dimensionality, yet often lack the reweighting process required to ensure asymptotically correct statistics. In this work, we propose Coarse-Grained Boltzmann Generators (CG-BGs), a principled framework that unifies scalable reduced-order modeling with the exactness of importance sampling. CG-BGs act in a coarse-grained coordinate space, using a learned potential of mean force (PMF) to reweight samples generated by a flow-based model. Crucially, we show that this PMF can be efficiently learned from rapidly converged data via force matching. Our results demonstrate that CG-BGs faithfully capture complex interactions mediated by explicit solvent within highly reduced representations, establishing a scalable pathway for the unbiased sampling of larger molecular systems.

</details>


### [4] [On the Role of Consistency Between Physics and Data in Physics-Informed Neural Networks](https://arxiv.org/abs/2602.10611)
*Nicolás Becerra-Zuniga, Lucas Lacasa, Eusebio Valero, Gonzalo Rubio*

Main category: cs.LG

TL;DR: 本文研究了数据与物理方程不一致性对物理信息神经网络（PINNs）准确性和收敛性的影响，提出了“一致性壁垒”的概念，并分析了数据质量对PINNs性能的限制。


<details>
  <summary>Details</summary>
Motivation: 实际应用中，PINNs训练数据通常与控制方程不完全一致，例如由于测量噪声、离散化误差或建模假设等。本文旨在理解这种数据-PDE不一致性对PINNs准确性和收敛性的影响。

Method: 研究人员使用一维粘性Burgers方程，通过控制数据保真度和残差误差，系统地分析了数据不一致性对PINNs的限制。他们训练了使用不同精度数值数据和完美一致的解析数据的PINNs，并观察了训练过程中的误差变化。

Result: 研究结果表明，PINNs可以通过包含PDE残差来部分缓解低保真度数据的影响，但训练过程最终会达到由数据不一致性决定的误差水平。当使用高保真度数值数据时，PINNs的解与使用解析数据训练的解变得无法区分，表明一致性壁垒被有效移除。

Conclusion: 本文阐明了数据质量和物理约束在PINNs中的相互作用，并为构建和解释物理信息代理模型提供了实践指导，强调了高质量数据对PINNs性能的重要性。

Abstract: Physics-informed neural networks (PINNs) have gained significant attention as a surrogate modeling strategy for partial differential equations (PDEs), particularly in regimes where labeled data are scarce and physical constraints can be leveraged to regularize the learning process. In practice, however, PINNs are frequently trained using experimental or numerical data that are not fully consistent with the governing equations due to measurement noise, discretization errors, or modeling assumptions. The implications of such data-to-PDE inconsistencies on the accuracy and convergence of PINNs remain insufficiently understood. In this work, we systematically analyze how data inconsistency fundamentally limits the attainable accuracy of PINNs. We introduce the concept of a consistency barrier, defined as an intrinsic lower bound on the error that arises from mismatches between the fidelity of the data and the exact enforcement of the PDE residual. To isolate and quantify this effect, we consider the 1D viscous Burgers equation with a manufactured analytical solution, which enables full control over data fidelity and residual errors. PINNs are trained using datasets of progressively increasing numerical accuracy, as well as perfectly consistent analytical data. Results show that while the inclusion of the PDE residual allows PINNs to partially mitigate low-fidelity data and recover the dominant physical structure, the training process ultimately saturates at an error level dictated by the data inconsistency. When high-fidelity numerical data are employed, PINN solutions become indistinguishable from those trained on analytical data, indicating that the consistency barrier is effectively removed. These findings clarify the interplay between data quality and physics enforcement in PINNs providing practical guidance for the construction and interpretation of physics-informed surrogate models.

</details>


### [5] [A Multimodal Conditional Mixture Model with Distribution-Level Physics Priors](https://arxiv.org/abs/2602.10451)
*Jinkyo Han, Bahador Bahmani*

Main category: cs.LG

TL;DR: 本文提出了一种基于混合密度网络（MDNs）的物理信息多模态建模框架，通过组件特定的正则化项将物理知识融入模型，以在科学和工程系统中有效预测多模态行为。


<details>
  <summary>Details</summary>
Motivation: 许多科学和工程系统表现出内在的多模态行为，传统建模方法通常依赖于单峰不确定性表示。现有机器学习方法在物理约束科学问题上的应用面临挑战，尤其是在保留物理结构和数据有限的情况下。

Method: 该框架基于混合密度网络（MDNs），通过组件特定的正则化项将物理知识嵌入到模型中，从而对多模态条件分布进行显式和可解释的参数化。该方法适用于非唯一性和随机性，同时保持计算效率和对上下文输入的条件能力。

Result: 在非线性动力系统分叉现象、随机偏微分方程和原子级冲击动力学等科学问题上进行了评估，结果表明MDNs可以实现与先进的条件流匹配（CFM）模型相媲美的性能，同时提供更简单和更易于解释的公式。

Conclusion: 该研究提供了一种有效且可解释的多模态建模方法，特别适用于物理约束的科学问题，为科学建模领域提供了一种有价值的补充。

Abstract: Many scientific and engineering systems exhibit intrinsically multimodal behavior arising from latent regime switching and non-unique physical mechanisms. In such settings, learning the full conditional distribution of admissible outcomes in a physically consistent and interpretable manner remains a challenge. While recent advances in machine learning have enabled powerful multimodal generative modeling, their integration with physics-constrained scientific modeling remains nontrivial, particularly when physical structure must be preserved or data are limited. This work develops a physics-informed multimodal conditional modeling framework based on mixture density representations. Mixture density networks (MDNs) provide an explicit and interpretable parameterization of multimodal conditional distributions. Physical knowledge is embedded through component-specific regularization terms that penalize violations of governing equations or physical laws. This formulation naturally accommodates non-uniqueness and stochasticity while remaining computationally efficient and amenable to conditioning on contextual inputs. The proposed framework is evaluated across a range of scientific problems in which multimodality arises from intrinsic physical mechanisms rather than observational noise, including bifurcation phenomena in nonlinear dynamical systems, stochastic partial differential equations, and atomistic-scale shock dynamics. In addition, the proposed method is compared with a conditional flow matching (CFM) model, a representative state-of-the-art generative modeling approach, demonstrating that MDNs can achieve competitive performance while offering a simpler and more interpretable formulation.

</details>


### [6] [Neural Network Quantum Field Theory from Transformer Architectures](https://arxiv.org/abs/2602.10209)
*Dmitry S. Ageev, Yulia A. Ageeva*

Main category: cs.LG

TL;DR: 本文提出了一种基于Transformer架构的神经网络量子场论(NN-QFT)构建方法，通过平均随机网络参数来定义n点相关函数。研究表明，单个注意力头由于共享随机softmax权重，即使在无限宽度极限下也能产生非高斯场统计。


<details>
  <summary>Details</summary>
Motivation: 传统的NN-QFT通常在无限宽度极限下产生高斯理论。本文旨在探索如何利用Transformer架构，特别是单个注意力头，构建具有非高斯相互作用的NN-QFT，从而扩展NNGP/QFT对应关系。

Method: 研究人员使用单个注意力头Transformer构建标量场，并定义相关函数通过平均随机网络参数。他们计算了二点函数和四点函数，并分析了共享注意力权重引起的“独立性破坏”效应，并通过随机特征token嵌入工程化欧几里得不变核。

Result: 研究结果表明，单个注意力头由于共享softmax权重，会导致非高斯场统计，即使在无限宽度极限下仍然存在。四点函数中存在一个与query-key权重协方差相关的“独立性破坏”贡献。通过增加独立头的数量并使用标准1/N_h归一化，可以抑制非高斯相关函数。

Conclusion: 本文提供了一种利用Transformer架构构建非高斯NN-QFT的新途径，为研究具有相互作用的量子场论提供了新的视角。结果表明，即使在无限宽度极限下，神经网络架构也可以产生非高斯行为，为构建更丰富的量子场论模型提供了可能性。

Abstract: We propose a neural-network construction of Euclidean scalar quantum field theories from transformer attention heads, defining $n$-point correlators by averaging over random network parameters in the NN-QFT framework. For a single attention head, shared random softmax weights couple different width coordinates and induce non-Gaussian field statistics that persist in the infinite-width limit $d_k\to\infty$. We compute the two-point function in an attention-weight representation and show how Euclidean-invariant kernels can be engineered via random-feature token embeddings. We then analyze the connected four-point function and identify an "independence-breaking" contribution, expressible as a covariance over query-key weights, which remains finite at infinite width. Finally, we show that summing many independent heads with standard $1/N_h$ normalization suppresses connected non-Gaussian correlators as $1/N_h$, yielding a Gaussian NN-QFT in the large-head limit.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [7] [A solvable high-dimensional model where nonlinear autoencoders learn structure invisible to PCA while test loss misaligns with generalization](https://arxiv.org/abs/2602.10680)
*Vicente Conde Mendes, Lorenzo Bardone, Cédric Koller, Jorge Medina Moreira, Vittorio Erba, Emanuele Troiani, Lenka Zdeborová*

Main category: stat.ML

TL;DR: 本文构建了一个高维模型，其中包含不可通过主成分分析（PCA）检测到的隐藏结构，并证明了非线性自编码器能够有效地提取这些结构，同时揭示了自监督测试损失与表示质量之间的错位现象。


<details>
  <summary>Details</summary>
Motivation: 现有理论模型难以解释非线性神经网络在提取隐藏结构方面的优势，尤其是在线性方法失效的情况下。本文旨在建立一个可分析的高维数据模型，证明非线性表示学习器优于线性基线。

Method: 引入了一个具有两个潜在因素的可解高维“ spiked” 模型：一个因素可被协方差检测到，另一个因素统计上相关但互不相关，仅存在于高阶矩中。通过分析群体风险和经验风险最小化来研究模型。

Result: PCA和线性自编码器无法恢复后者，而一个最小的非线性自编码器可以证明地提取这两个因素。研究还表明，在本文的模型中，自监督测试损失与表示质量存在错位，即非线性自编码器可以提取线性方法无法捕捉到的隐藏结构，即使其重建损失更高。

Conclusion: 该模型提供了一个可处理的例子，展示了非线性方法在提取隐藏结构方面的优势，并为理解非线性神经网络在高维无监督学习中的作用提供了理论基础。该模型可作为未来研究的基准，用于分析非线性表示学习的算法和信息论限度。

Abstract: Many real-world datasets contain hidden structure that cannot be detected by simple linear correlations between input features. For example, latent factors may influence the data in a coordinated way, even though their effect is invisible to covariance-based methods such as PCA. In practice, nonlinear neural networks often succeed in extracting such hidden structure in unsupervised and self-supervised learning. However, constructing a minimal high-dimensional model where this advantage can be rigorously analyzed has remained an open theoretical challenge. We introduce a tractable high-dimensional spiked model with two latent factors: one visible to covariance, and one statistically dependent yet uncorrelated, appearing only in higher-order moments. PCA and linear autoencoders fail to recover the latter, while a minimal nonlinear autoencoder provably extracts both. We analyze both the population risk, and empirical risk minimization. Our model also provides a tractable example where self-supervised test loss is poorly aligned with representation quality: nonlinear autoencoders recover latent structure that linear methods miss, even though their reconstruction loss is higher.

</details>
