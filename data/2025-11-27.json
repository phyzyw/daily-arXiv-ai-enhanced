{"id": "2511.21369", "title": "Differentiable Physics-Neural Models enable Learning of Non-Markovian Closures for Accelerated Coarse-Grained Physics Simulations", "authors": ["Tingkai Xue", "Chin Chun Ooi", "Zhengwei Ge", "Fong Yew Leong", "Hongying Li", "Chang Wei Kang"], "summary": "Numerical simulations provide key insights into many physical, real-world problems. However, while these simulations are solved on a full 3D domain, most analysis only require a reduced set of metrics (e.g. plane-level concentrations). This work presents a hybrid physics-neural model that predicts scalar transport in a complex domain orders of magnitude faster than the 3D simulation (from hours to less than 1 min). This end-to-end differentiable framework jointly learns the physical model parameterization (i.e. orthotropic diffusivity) and a non-Markovian neural closure model to capture unresolved, 'coarse-grained' effects, thereby enabling stable, long time horizon rollouts. This proposed model is data-efficient (learning with 26 training data), and can be flexibly extended to an out-of-distribution scenario (with a moving source), achieving a Spearman correlation coefficient of 0.96 at the final simulation time. Overall results show that this differentiable physics-neural framework enables fast, accurate, and generalizable coarse-grained surrogates for physical phenomena.", "published": "2025-11-26", "categories": ["physics.comp-ph", "cs.LG", "physics.flu-dyn"], "pdf_url": "https://arxiv.org/pdf/2511.21369v1", "primary_category": "physics.comp-ph"}
{"id": "2511.20798", "title": "Physics Steering: Causal Control of Cross-Domain Concepts in a Physics Foundation Model", "authors": ["Rio Alexa Fear", "Payel Mukhopadhyay", "Michael McCabe", "Alberto Bietti", "Miles Cranmer"], "summary": "Recent advances in mechanistic interpretability have revealed that large language models (LLMs) develop internal representations corresponding not only to concrete entities but also distinct, human-understandable abstract concepts and behaviour. Moreover, these hidden features can be directly manipulated to steer model behaviour. However, it remains an open question whether this phenomenon is unique to models trained on inherently structured data (ie. language, images) or if it is a general property of foundation models. In this work, we investigate the internal representations of a large physics-focused foundation model. Inspired by recent work identifying single directions in activation space for complex behaviours in LLMs, we extract activation vectors from the model during forward passes over simulation datasets for different physical regimes. We then compute \"delta\" representations between the two regimes. These delta tensors act as concept directions in activation space, encoding specific physical features. By injecting these concept directions back into the model during inference, we can steer its predictions, demonstrating causal control over physical behaviours, such as inducing or removing some particular physical feature from a simulation. These results suggest that scientific foundation models learn generalised representations of physical principles. They do not merely rely on superficial correlations and patterns in the simulations. Our findings open new avenues for understanding and controlling scientific foundation models and has implications for AI-enabled scientific discovery.", "published": "2025-11-25", "categories": ["cs.LG", "cs.AI", "physics.comp-ph"], "pdf_url": "https://arxiv.org/pdf/2511.20798v1", "primary_category": "cs.LG"}
{"id": "2511.20976", "title": "AI4X Roadmap: Artificial Intelligence for the advancement of scientific pursuit and its future directions", "authors": ["Stephen G. Dale", "Nikita Kazeev", "Alastair J. A. Price", "Victor Posligua", "Stephan Roche", "O. Anatole von Lilienfeld", "Konstantin S. Novoselov", "Xavier Bresson", "Gianmarco Mengaldo", "Xudong Chen", "Terence J. O'Kane", "Emily R. Lines", "Matthew J. Allen", "Amandine E. Debus", "Clayton Miller", "Jiayu Zhou", "Hiroko H. Dodge", "David Rousseau", "Andrey Ustyuzhanin", "Ziyun Yan", "Mario Lanza", "Fabio Sciarrino", "Ryo Yoshida", "Zhidong Leong", "Teck Leong Tan", "Qianxiao Li", "Adil Kabylda", "Igor Poltavsky", "Alexandre Tkatchenko", "Sherif Abdulkader Tawfik", "Prathami Divakar Kamath", "Theo Jaffrelot Inizan", "Kristin A. Persson", "Bryant Y. Li", "Vir Karan", "Chenru Duan", "Haojun Jia", "Qiyuan Zhao", "Hiroyuki Hayashi", "Atsuto Seko", "Isao Tanaka", "Omar M. Yaghi", "Tim Gould", "Bun Chan", "Stefan Vuckovic", "Tianbo Li", "Min Lin", "Zehcen Tang", "Yang Li", "Yong Xu", "Amrita Joshi", "Xiaonan Wang", "Leonard W. T. Ng", "Sergei V. Kalinin", "Mahshid Ahmadi", "Jiyizhe Zhang", "Shuyuan Zhang", "Alexei Lapkin", "Ming Xiao", "Zhe Wu", "Kedar Hippalgaonkar", "Limsoon Wong", "Lorenzo Bastonero", "Nicola Marzari", "Dorye Luis Esteras Cordoba", "Andrei Tomut", "Alba Quinones Andrade", "Jose-Hugo Garcia"], "summary": "Artificial intelligence and machine learning are reshaping how we approach scientific discovery, not by replacing established methods but by extending what researchers can probe, predict, and design. In this roadmap we provide a forward-looking view of AI-enabled science across biology, chemistry, climate science, mathematics, materials science, physics, self-driving laboratories and unconventional computing. Several shared themes emerge: the need for diverse and trustworthy data, transferable electronic-structure and interatomic models, AI systems integrated into end-to-end scientific workflows that connect simulations to experiments and generative systems grounded in synthesisability rather than purely idealised phases. Across domains, we highlight how large foundation models, active learning and self-driving laboratories can close loops between prediction and validation while maintaining reproducibility and physical interpretability. Taken together, these perspectives outline where AI-enabled science stands today, identify bottlenecks in data, methods and infrastructure, and chart concrete directions for building AI systems that are not only more powerful but also more transparent and capable of accelerating discovery in complex real-world environments.", "published": "2025-11-26", "categories": ["physics.soc-ph", "cs.AI", "physics.ao-ph", "physics.atm-clus", "physics.chem-ph", "physics.comp-ph"], "pdf_url": "https://arxiv.org/pdf/2511.20976v1", "primary_category": "physics.soc-ph"}
