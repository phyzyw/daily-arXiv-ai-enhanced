<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [High-Dimensional Limit of Stochastic Gradient Flow via Dynamical Mean-Field Theory](https://arxiv.org/abs/2602.06320)
*Sota Nishiyama, Masaaki Imaizumi*

Main category: stat.ML

TL;DR: 本文研究了小批量随机梯度下降（SGD）在高维情况下的动力学行为，并基于动力学平均场理论（DMFT）推导出一组低维连续时间方程，用于描述SGF参数的渐近分布。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习模型通常使用小批量SGD进行训练，但缺乏一个分析框架来描述其在高维情况下的渐近行为。

Method: 分析了随机梯度流（SGF）的动力学，SGF是小批量SGD的近似。在数据样本数和维度共同增长的极限情况下，利用DMFT推导出一组低维连续时间方程。

Result: 推导出的DMFT方程能够表征SGF参数的渐近分布，并且可以作为在线SGD和高维线性回归等现有框架的特例。

Conclusion: 本文提供了一个统一的视角来理解SGD的动力学，并为高维机器学习模型的理论分析奠定了基础。

Abstract: Modern machine learning models are typically trained via multi-pass stochastic gradient descent (SGD) with small batch sizes, and understanding their dynamics in high dimensions is of great interest. However, an analytical framework for describing the high-dimensional asymptotic behavior of multi-pass SGD with small batch sizes for nonlinear models is currently missing. In this study, we address this gap by analyzing the high-dimensional dynamics of a stochastic differential equation called a \emph{stochastic gradient flow} (SGF), which approximates multi-pass SGD in this regime. In the limit where the number of data samples $n$ and the dimension $d$ grow proportionally, we derive a closed system of low-dimensional and continuous-time equations and prove that it characterizes the asymptotic distribution of the SGF parameters. Our theory is based on the dynamical mean-field theory (DMFT) and is applicable to a wide range of models encompassing generalized linear models and two-layer neural networks. We further show that the resulting DMFT equations recover several existing high-dimensional descriptions of SGD dynamics as special cases, thereby providing a unifying perspective on prior frameworks such as online SGD and high-dimensional linear regression. Our proof builds on the existing DMFT technique for gradient flow and extends it to handle the stochasticity in SGF using tools from stochastic calculus.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [2] [Rare Event Analysis of Large Language Models](https://arxiv.org/abs/2602.06791)
*Jake McAllister Dorman, Edward Gillman, Dominic C. Rose, Jamie F. Mair, Juan P. Garrahan*

Main category: cs.LG

TL;DR: 本文提出了一种系统分析大型语言模型（LLMs）中罕见事件的端到端框架，旨在理解和控制这些事件，尤其是在模型部署后。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的广泛应用，其输出中的罕见但重要的事件对实际影响日益增加。然而，针对LLMs的罕见事件分析仍处于起步阶段，需要一种实用的方法来识别、评估和控制这些事件。

Method: 该框架包括四个关键阶段：将LLMs建模为随机过程，设置数值实验以研究特定类型的罕见事件，估计罕见事件的期望值和概率，以及探索罕见事件的性质和结构。作者以TinyStories模型为例，研究了文本完成的自动可读性指数（ARI）和完成概率的罕见值。

Result: 通过对TinyStories模型的分析，作者提供了一种理论、采样策略、概率估计和误差分析的实用细节，展示了该框架在识别和评估LLMs中罕见事件方面的有效性。

Conclusion: 该研究为LLMs的罕见事件分析提供了一个实用的起点，有助于开发者、研究人员和工程师更好地理解和控制LLMs的行为，尤其是在关键领域应用中，从而提高模型的安全性和可靠性。

Abstract: Being probabilistic models, during inference large language models (LLMs) display rare events: behaviour that is far from typical but highly significant. By definition all rare events are hard to see, but the enormous scale of LLM usage means that events completely unobserved during development are likely to become prominent in deployment. Here we present an end-to-end framework for the systematic analysis of rare events in LLMs. We provide a practical implementation spanning theory, efficient generation strategies, probability estimation and error analysis, which we illustrate with concrete examples. We outline extensions and applications to other models and contexts, highlighting the generality of the concepts and techniques presented here.

</details>


### [3] [Adaptive Protein Tokenization](https://arxiv.org/abs/2602.06418)
*Rohit Dilip, Ayush Varshney, David Van Valen*

Main category: cs.LG

TL;DR: 本文提出了一种自适应蛋白质分词方法（APT），该方法通过全局分词方式，使每个token提供递增的细节信息，从而改善蛋白质结构生成和表示学习任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于局部邻域信息的蛋白质结构分词方法在生成模型中表现不佳，存在误差累积问题，且难以有效处理大型蛋白质，缺乏任务特定信息的适应性。

Method: APT模型采用全局分词策略，每个token贡献越来越多的全局信息，类似于傅里叶和小波变换，将信号分解为粗略和精细的成分。该方法允许对分词序列的信息内容进行任务特定的适应。

Result: 在重建、生成和表示学习任务中，APT方法优于或与基于局部蛋白质结构分词器的现有模型相当。通过非线性探测验证的表示在CATH分类任务中表现优异，并且支持零样本蛋白质收缩和亲和力成熟。

Conclusion: APT方法提供了一种更有效、可扩展的蛋白质结构分词方法，能够生成高质量的蛋白质表示，并支持各种下游应用，为蛋白质设计和理解提供了新的可能性。

Abstract: Tokenization is a promising path to multi-modal models capable of jointly understanding protein sequences, structure, and function. Existing protein structure tokenizers create tokens by pooling information from local neighborhoods, an approach that limits their performance on generative and representation tasks. In this work, we present a method for global tokenization of protein structures in which successive tokens contribute increasing levels of detail to a global representation. This change resolves several issues with generative models based on local protein tokenization: it mitigates error accumulation, provides embeddings without sequence-reduction operations, and allows task-specific adaptation of a tokenized sequence's information content. We validate our method on reconstruction, generative, and representation tasks and demonstrate that it matches or outperforms existing models based on local protein structure tokenizers. We show how adaptive tokens enable inference criteria based on information content, which boosts designability. We validate representations generated from our tokenizer on CATH classification tasks and demonstrate that non-linear probing on our tokenized sequences outperforms equivalent probing on representations from other tokenizers. Finally, we demonstrate how our method supports zero-shot protein shrinking and affinity maturation.

</details>
