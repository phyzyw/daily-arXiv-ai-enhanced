{"id": "2601.04176", "title": "Robust Physics Discovery from Highly Corrupted Data: A PINN Framework Applied to the Nonlinear Schrödinger Equation", "authors": ["Pietro de Oliveira Esteves"], "summary": "We demonstrate a deep learning framework capable of recovering physical parameters from the Nonlinear Schrodinger Equation (NLSE) under severe noise conditions. By integrating Physics-Informed Neural Networks (PINNs) with automatic differentiation, we achieve reconstruction of the nonlinear coefficient beta with less than 0.2 percent relative error using only 500 sparse, randomly sampled data points corrupted by 20 percent additive Gaussian noise, a regime where traditional finite difference methods typically fail due to noise amplification in numerical derivatives. We validate the method's generalization capabilities across different physical regimes (beta between 0.5 and 2.0) and varying data availability (between 100 and 1000 training points), demonstrating consistent sub-1 percent accuracy. Statistical analysis over multiple independent runs confirms robustness (standard deviation less than 0.15 percent for beta equals 1.0). The complete pipeline executes in approximately 80 minutes on modest cloud GPU resources (NVIDIA Tesla T4), making the approach accessible for widespread adoption. Our results indicate that physics-based regularization acts as an effective filter against high measurement uncertainty, positioning PINNs as a viable alternative to traditional optimization methods for inverse problems in spatiotemporal dynamics where experimental data is scarce and noisy. All code is made publicly available to facilitate reproducibility.", "abs": "", "categories": ["cs.LG", "physics.comp-ph"], "AI": {"tldr": "本文提出了一种基于物理信息神经网络 (PINN) 的框架，即使在存在严重噪声的情况下，也能从稀疏数据中准确恢复非线性薛定谔方程 (NLSE) 的物理参数。", "motivation": "传统数值方法在处理实验数据时容易受到噪声放大影响，导致参数估计不稳定。因此，需要一种更鲁棒的方法来解决 NLSE 的逆问题，尤其是在数据稀疏且噪声较大的情况下。", "method": "该研究将 PINN 与自动微分相结合，构建了一个深度学习框架，将 NLSE 的偏微分方程直接嵌入到神经网络的损失函数中，作为一种软约束机制。通过调整物理正则化权重，实现了对非线性系数 β 的精确重建。", "result": "实验结果表明，即使在存在 20% 高斯噪声的情况下，仅使用 500 个稀疏数据点，也能以 <0.2% 的相对误差重建 β。该方法在不同物理状态和数据可用性下都表现出一致的亚 1% 准确率，且具有良好的鲁棒性。", "conclusion": "PINN 能够有效充当物理导向的滤波器，将信号与随机噪声分离，为在实验数据稀疏且噪声较大的时空动力学逆问题中，提供了一种可行的替代传统优化方法的方式。"}}
{"id": "2601.04104", "title": "Equivariant Neural Networks for Force-Field Models of Lattice Systems", "authors": ["Yunhao Fan", "Gia-Wei Chern"], "summary": "Machine-learning (ML) force fields enable large-scale simulations with near-first-principles accuracy at substantially reduced computational cost. Recent work has extended ML force-field approaches to adiabatic dynamical simulations of condensed-matter lattice models with coupled electronic and structural or magnetic degrees of freedom. However, most existing formulations rely on hand-crafted, symmetry-aware descriptors, whose construction is often system-specific and can hinder generality and transferability across different lattice Hamiltonians. Here we introduce a symmetry-preserving framework based on equivariant neural networks (ENNs) that provides a general, data-driven mapping from local configurations of dynamical variables to the associated on-site forces in a lattice Hamiltonian. In contrast to ENN architectures developed for molecular systems -- where continuous Euclidean symmetries dominate -- our approach aims to embed the discrete point-group and internal symmetries intrinsic to lattice models directly into the neural-network representation of the force field. As a proof of principle, we construct an ENN-based force-field model for the adiabatic dynamics of the Holstein Hamiltonian on a square lattice, a canonical system for electron-lattice physics. The resulting ML-enabled large-scale dynamical simulations faithfully capture mesoscale evolution of the symmetry-breaking phase, illustrating the utility of lattice-equivariant architectures for linking microscopic electronic processes to emergent dynamical behavior in condensed-matter lattice systems.", "abs": "", "categories": ["cond-mat.str-el", "cs.LG", "physics.comp-ph"], "AI": {"tldr": "本文提出了一种基于等变神经网络 (ENN) 的新框架，用于构建晶格系统的力场模型，该框架直接将离散对称性嵌入到神经网络表示中，无需手工设计的对称性感知描述符。", "motivation": "现有机器学习力场模型通常依赖于手工设计的对称性感知描述符，这限制了其通用性和跨不同晶格哈密顿量的可传递性。本文旨在提供一种更通用、数据驱动的方法。", "method": "研究人员构建了一种基于等变神经网络 (ENN) 的力场模型，该模型直接将晶格模型的离散点群和内部对称性嵌入到神经网络表示中。他们以 Holstein 哈密顿量在方形晶格上的绝热动力学为例进行了验证。", "result": "实验结果表明，基于 ENN 的力场模型能够忠实地捕捉到对称性破缺相的中间尺度演化，证明了晶格等变架构在将微观电子过程与凝聚态晶格系统中的涌现动力学行为联系起来方面的实用性。", "conclusion": "该研究提供了一种通用的、数据驱动的晶格力场模型构建方法，避免了手工设计描述符的局限性，并有望促进对复杂凝聚态系统的大规模模拟。"}}
{"id": "2601.03689", "title": "A Pre-trained Reaction Embedding Descriptor Capturing Bond Transformation Patterns", "authors": ["Weiqi Liu", "Fenglei Cao", "Yuan Qi", "Li-Cheng Xu"], "summary": "With the rise of data-driven reaction prediction models, effective reaction descriptors are crucial for bridging the gap between real-world chemistry and digital representations. However, general-purpose, reaction-wise descriptors remain scarce. This study introduces RXNEmb, a novel reaction-level descriptor derived from RXNGraphormer, a model pre-trained to distinguish real reactions from fictitious ones with erroneous bond changes, thereby learning intrinsic bond formation and cleavage patterns. We demonstrate its utility by data-driven re-clustering of the USPTO-50k dataset, yielding a classification that more directly reflects bond-change similarities than rule-based categories. Combined with dimensionality reduction, RXNEmb enables visualization of reaction space diversity. Furthermore, attention weight analysis reveals the model's focus on chemically critical sites, providing mechanistic insight. RXNEmb serves as a powerful, interpretable tool for reaction fingerprinting and analysis, paving the way for more data-centric approaches in reaction analysis and discovery.", "abs": "", "categories": ["cs.LG", "cs.AI", "physics.chem-ph"], "AI": {"tldr": "本文提出了一种新的反应级描述符RXNEmb，它基于预训练模型RXNGraphormer，能够捕捉反应中键的形成和断裂模式，并为反应指纹识别和分析提供了一种强大的工具。", "motivation": "现有反应描述符要么依赖于分子级别的描述符，要么依赖于预定义的专家规则，缺乏直接反映反应变化的反应级描述符。数据驱动的反应预测模型需要有效的反应描述符来弥合现实化学和数字表示之间的差距。", "method": "研究人员利用RXNGraphormer模型，通过预训练使其能够区分真实反应和虚构反应，从而学习到内在的键形成和断裂模式。基于此，提出了RXNEmb反应级描述符，并将其应用于USPTO-50k数据集的重新聚类，以及反应空间的降维可视化。", "result": "RXNEmb能够进行反应重新聚类，更好地反映键变化的相似性，并实现反应空间的降维可视化。注意力权重分析表明，该模型关注化学上关键的位点，提供了机制洞察。", "conclusion": "RXNEmb作为一种强大的、可解释的反应指纹识别和分析工具，为数据驱动的反应分析和发现开辟了道路，有望促进有机合成方法学的优化和新方法的发展。"}}
{"id": "2601.03774", "title": "Scalable Machine Learning Force Fields for Macromolecular Systems Through Long-Range Aware Message Passing", "authors": ["Chu Wang", "Lin Huang", "Xinran Wei", "Tao Qin", "Arthur Jiang", "Lixue Cheng", "Jia Zhang"], "summary": "Machine learning force fields (MLFFs) have revolutionized molecular simulations by providing quantum mechanical accuracy at the speed of molecular mechanical computations. However, a fundamental reliance of these models on fixed-cutoff architectures limits their applicability to macromolecular systems where long-range interactions dominate. We demonstrate that this locality constraint causes force prediction errors to scale monotonically with system size, revealing a critical architectural bottleneck. To overcome this, we establish the systematically designed MolLR25 ({Mol}ecules with {L}ong-{R}ange effect) benchmark up to 1200 atoms, generated using high-fidelity DFT, and introduce E2Former-LSR, an equivariant transformer that explicitly integrates long-range attention blocks. E2Former-LSR exhibits stable error scaling, achieves superior fidelity in capturing non-covalent decay, and maintains precision on complex protein conformations. Crucially, its efficient design provides up to 30% speedup compared to purely local models. This work validates the necessity of non-local architectures for generalizable MLFFs, enabling high-fidelity molecular dynamics for large-scale chemical and biological systems.", "abs": "", "categories": ["physics.chem-ph", "cs.AI", "physics.bio-ph"], "AI": {"tldr": "本文提出了一种新的机器学习力场模型E2Former-LSR，通过引入长程注意力机制，解决了传统模型在模拟大分子系统时因长程相互作用而产生的误差问题，实现了更准确、更高效的分子动力学模拟。", "motivation": "现有机器学习力场模型普遍依赖于固定截断半径的局部架构，导致其在模拟大分子系统时，由于长程相互作用的影响，预测误差随系统规模单调增加，限制了其适用性。", "method": "研究人员构建了包含高达1200个原子的MolLR25基准数据集，并设计了E2Former-LSR模型，该模型是一种等变Transformer，显式地整合了长程注意力模块。", "result": "E2Former-LSR模型在长程相互作用的捕获方面表现出更稳定的误差缩放，在非共价键衰减的精确性方面优于传统模型，并且在复杂蛋白质构象的模拟中保持了精度，同时实现了高达30%的加速。", "conclusion": "该研究验证了非局部架构对于通用机器学习力场的必要性，为大规模化学和生物系统的精确分子动力学模拟提供了可能。"}}
{"id": "2601.03704", "title": "Investigating Knowledge Distillation Through Neural Networks for Protein Binding Affinity Prediction", "authors": ["Wajid Arshad Abbasi", "Syed Ali Abbas", "Maryum Bibi", "Saiqa Andleeb", "Muhammad Naveed Akhtar"], "summary": "The trade-off between predictive accuracy and data availability makes it difficult to predict protein--protein binding affinity accurately. The lack of experimentally resolved protein structures limits the performance of structure-based machine learning models, which generally outperform sequence-based methods. In order to overcome this constraint, we suggest a regression framework based on knowledge distillation that uses protein structural data during training and only needs sequence data during inference. The suggested method uses binding affinity labels and intermediate feature representations to jointly supervise the training of a sequence-based student network under the guidance of a structure-informed teacher network. Leave-One-Complex-Out (LOCO) cross-validation was used to assess the framework on a non-redundant protein--protein binding affinity benchmark dataset. A maximum Pearson correlation coefficient (P_r) of 0.375 and an RMSE of 2.712 kcal/mol were obtained by sequence-only baseline models, whereas a P_r of 0.512 and an RMSE of 2.445 kcal/mol were obtained by structure-based models. With a P_r of 0.481 and an RMSE of 2.488 kcal/mol, the distillation-based student model greatly enhanced sequence-only performance. Improved agreement and decreased bias were further confirmed by thorough error analyses. With the potential to close the performance gap between sequence-based and structure-based models as larger datasets become available, these findings show that knowledge distillation is an efficient method for transferring structural knowledge to sequence-based predictors. The source code for running inference with the proposed distillation-based binding affinity predictor can be accessed at https://github.com/wajidarshad/ProteinAffinityKD.", "abs": "", "categories": ["cs.LG", "cs.AI", "q-bio.BM", "q-bio.MN", "q-bio.QM"], "AI": {"tldr": "本文提出了一种基于知识蒸馏的框架，利用结构信息指导序列数据训练，以提高蛋白质结合亲和力预测的准确性，尤其是在结构数据有限的情况下。", "motivation": "蛋白质-蛋白质结合亲和力的准确预测受到实验数据可用性的限制。结构化机器学习模型通常优于基于序列的方法，但结构数据稀缺是其瓶颈。因此，需要一种方法来利用结构信息，同时仅依赖于序列数据进行推理。", "method": "该方法采用知识蒸馏技术，利用结构信息指导训练一个基于序列的“学生”网络。一个结构信息丰富的“教师”网络提供中间特征表示，用于联合监督学生网络的训练。使用Leave-One-Complex-Out (LOCO)交叉验证评估模型性能。", "result": "基于蒸馏的学生模型在Pearson相关系数 (0.481) 和均方根误差 (2.488 kcal/mol) 方面显著优于仅基于序列的基线模型 (0.375 和 2.712 kcal/mol)，并接近基于结构的模型的性能 (0.512 和 2.445 kcal/mol)。", "conclusion": "知识蒸馏是一种有效的方法，可以将结构知识转移到基于序列的预测器中，有望缩小基于序列和结构的模型之间的性能差距，尤其是在数据集不断扩大的情况下。"}}
{"id": "2601.03764", "title": "Learning Shrinks the Hard Tail: Training-Dependent Inference Scaling in a Solvable Linear Model", "authors": ["Noam Levi"], "summary": "We analyze neural scaling laws in a solvable model of last-layer fine-tuning where targets have intrinsic, instance-heterogeneous difficulty. In our Latent Instance Difficulty (LID) model, each input's target variance is governed by a latent ``precision'' drawn from a heavy-tailed distribution. While generalization loss recovers standard scaling laws, our main contribution connects this to inference. The pass@$k$ failure rate exhibits a power-law decay, $k^{-β_\\text{eff}}$, but the observed exponent $β_\\text{eff}$ is training-dependent. It grows with sample size $N$ before saturating at an intrinsic limit $β$ set by the difficulty distribution's tail. This coupling reveals that learning shrinks the ``hard tail'' of the error distribution: improvements in the model's generalization error steepen the pass@$k$ curve until irreducible target variance dominates. The LID model yields testable, closed-form predictions for this behavior, including a compute-allocation rule that favors training before saturation and inference attempts after. We validate these predictions in simulations and in two real-data proxies: CIFAR-10H (human-label variance) and a maths teacher-student distillation task.", "abs": "", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.AI", "stat.ML"], "AI": {"tldr": "本文提出了一个Latent Instance Difficulty (LID) 模型，分析了在可解线性模型中，训练如何影响推断的scaling规律。研究发现，训练会“收缩”误差分布的“硬尾”，使得pass@k曲线变陡峭。", "motivation": "现有研究主要关注训练阶段的scaling规律，而忽略了训练进度如何影响推断阶段的性能。本文旨在填补这一空白，研究训练和推断之间的耦合关系，尤其是在存在实例难度异构性时。", "method": "采用一个可分析的线性模型，其中每个实例都具有一个控制目标方差的潜在精度τx（“难度”）。通过在训练阶段观察单个实例，并进行线性回归，研究了随着样本量N的增加，pass@k失败率的变化。", "result": "研究发现，pass@k失败率呈现幂律衰减k−βeff，但βeff是训练相关的，随着样本量N的增加而增大，直到达到由难度分布尾部决定的内在极限βset。βeff的值取决于内在难度分布的尾部指数β和模型泛化误差。", "conclusion": "本文揭示了训练如何收缩误差分布的“硬尾”，并提出了可验证的闭式预测，包括一个计算分配规则，优先进行训练，并在饱和后进行推断尝试。研究结果对理解和优化大规模机器学习模型的训练和推断过程具有重要意义。"}}
