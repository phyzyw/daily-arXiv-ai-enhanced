{"id": "2512.04954", "title": "Amortized Inference of Multi-Modal Posteriors using Likelihood-Weighted Normalizing Flows", "authors": ["Rajneil Baruah"], "summary": "We present a novel technique for amortized posterior estimation using Normalizing Flows trained with likelihood-weighted importance sampling. This approach allows for the efficient inference of theoretical parameters in high-dimensional inverse problems without the need for posterior training samples. We implement the method on multi-modal benchmark tasks in 2D and 3D to check for the efficacy. A critical observation of our study is the impact of the topology of the base distributions on the modelled posteriors. We find that standard unimodal base distributions fail to capture disconnected support, resulting in spurious probability bridges between modes. We demonstrate that initializing the flow with a Gaussian Mixture Model that matches the cardinality of the target modes significantly improves reconstruction fidelity, as measured by some distance and divergence metrics.", "published": "2025-12-04", "categories": ["cs.LG", "hep-ex", "hep-ph", "physics.comp-ph", "physics.data-an"], "pdf_url": "https://arxiv.org/pdf/2512.04954v1", "primary_category": "cs.LG"}
{"id": "2512.04663", "title": "Fermionic neural Gibbs states", "authors": ["Jannes Nys", "Juan Carrasquilla"], "summary": "We introduce fermionic neural Gibbs states (fNGS), a variational framework for modeling finite-temperature properties of strongly interacting fermions. fNGS starts from a reference mean-field thermofield-double state and uses neural-network transformations together with imaginary-time evolution to systematically build strong correlations. Applied to the doped Fermi-Hubbard model, a minimal lattice model capturing essential features of strong electronic correlations, fNGS accurately reproduces thermal energies over a broad range of temperatures, interaction strengths, even at large dopings, for system sizes beyond the reach of exact methods. These results demonstrate a scalable route to studying finite-temperature properties of strongly correlated fermionic systems beyond one dimension with neural-network representations of quantum states.", "published": "2025-12-04", "categories": ["quant-ph", "cond-mat.str-el", "cs.LG", "physics.comp-ph"], "pdf_url": "https://arxiv.org/pdf/2512.04663v1", "primary_category": "quant-ph"}
{"id": "2512.04452", "title": "NORi: An ML-Augmented Ocean Boundary Layer Parameterization", "authors": ["Xin Kai Lee", "Ali Ramadhan", "Andre Souza", "Gregory LeClaire Wagner", "Simone Silvestri", "John Marshall", "Raffaele Ferrari"], "summary": "NORi is a machine-learned (ML) parameterization of ocean boundary layer turbulence that is physics-based and augmented with neural networks. NORi stands for neural ordinary differential equations (NODEs) Richardson number (Ri) closure. The physical parameterization is controlled by a Richardson number-dependent diffusivity and viscosity. The NODEs are trained to capture the entrainment through the base of the boundary layer, which cannot be represented with a local diffusive closure. The parameterization is trained using large-eddy simulations in an \"a posteriori\" fashion, where parameters are calibrated with a loss function that explicitly depends on the actual time-integrated variables of interest rather than the instantaneous subgrid fluxes, which are inherently noisy. NORi is designed for the realistic nonlinear equation of state of seawater and demonstrates excellent prediction and generalization capabilities in capturing entrainment dynamics under different convective strengths, oceanic background stratifications, rotation strengths, and surface wind forcings. NORi is numerically stable for at least 100 years of integration time in large-scale simulations, despite only being trained on 2-day horizons, and can be run with time steps as long as one hour. The highly expressive neural networks, combined with a physically-rigorous base closure, prove to be a robust paradigm for designing parameterizations for climate models where data requirements are drastically reduced, inference performance can be directly targeted and optimized, and numerical stability is implicitly encouraged during training.", "published": "2025-12-04", "categories": ["physics.ao-ph", "cs.AI", "cs.LG", "physics.comp-ph", "physics.flu-dyn"], "pdf_url": "https://arxiv.org/pdf/2512.04452v1", "primary_category": "physics.ao-ph"}
