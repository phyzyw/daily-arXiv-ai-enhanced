{"id": "2510.06174", "title": "Thermodynamic Performance Limits for Score-Based Diffusion Models", "authors": ["Nathan X. Kodama", "Michael Hinczewski"], "summary": "We establish a fundamental connection between score-based diffusion models and non-equilibrium thermodynamics by deriving performance limits based on entropy rates. Our main theoretical contribution is a lower bound on the negative log-likelihood of the data that relates model performance to entropy rates of diffusion processes. We numerically validate this bound on a synthetic dataset and investigate its tightness. By building a bridge to entropy rates - system, intrinsic, and exchange entropy - we provide new insights into the thermodynamic operation of these models, drawing parallels to Maxwell's demon and implications for thermodynamic computing hardware. Our framework connects generative modeling performance to fundamental physical principles through stochastic thermodynamics.", "published": "2025-10-07", "categories": ["cs.LG", "cond-mat.stat-mech"], "pdf_url": "http://arxiv.org/pdf/2510.06174v1", "primary_category": "cs.LG"}
{"id": "2510.06106", "title": "The Physics of Data and Tasks: Theories of Locality and Compositionality in Deep Learning", "authors": ["Alessandro Favero"], "summary": "Deep neural networks have achieved remarkable success, yet our understanding of how they learn remains limited. These models can learn high-dimensional tasks, which is generally statistically intractable due to the curse of dimensionality. This apparent paradox suggests that learnable data must have an underlying latent structure. What is the nature of this structure? How do neural networks encode and exploit it, and how does it quantitatively impact performance - for instance, how does generalization improve with the number of training examples? This thesis addresses these questions by studying the roles of locality and compositionality in data, tasks, and deep learning representations.", "published": "2025-10-07", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.stat-mech", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2510.06106v1", "primary_category": "cs.LG"}
{"id": "2510.06030", "title": "Adaptive Pruning for Increased Robustness and Reduced Computational Overhead in Gaussian Process Accelerated Saddle Point Searches", "authors": ["Rohit Goswami", "Hannes JÃ³nsson"], "summary": "Gaussian process (GP) regression provides a strategy for accelerating saddle point searches on high-dimensional energy surfaces by reducing the number of times the energy and its derivatives with respect to atomic coordinates need to be evaluated. The computational overhead in the hyperparameter optimization can, however, be large and make the approach inefficient. Failures can also occur if the search ventures too far into regions that are not represented well enough by the GP model. Here, these challenges are resolved by using geometry-aware optimal transport measures and an active pruning strategy using a summation over Wasserstein-1 distances for each atom-type in farthest-point sampling, selecting a fixed-size subset of geometrically diverse configurations to avoid rapidly increasing cost of GP updates as more observations are made. Stability is enhanced by permutation-invariant metric that provides a reliable trust radius for early-stopping and a logarithmic barrier penalty for the growth of the signal variance. These physically motivated algorithmic changes prove their efficacy by reducing to less than a half the mean computational time on a set of 238 challenging configurations from a previously published data set of chemical reactions. With these improvements, the GP approach is established as, a robust and scalable algorithm for accelerating saddle point searches when the evaluation of the energy and atomic forces requires significant computational effort.", "published": "2025-10-07", "categories": ["physics.chem-ph", "cond-mat.mtrl-sci", "cs.LG", "physics.comp-ph"], "pdf_url": "http://arxiv.org/pdf/2510.06030v1", "primary_category": "physics.chem-ph"}
{"id": "2510.05568", "title": "Bilevel optimization for learning hyperparameters: Application to solving PDEs and inverse problems with Gaussian processes", "authors": ["Nicholas H. Nelsen", "Houman Owhadi", "Andrew M. Stuart", "Xianjin Yang", "Zongren Zou"], "summary": "Methods for solving scientific computing and inference problems, such as kernel- and neural network-based approaches for partial differential equations (PDEs), inverse problems, and supervised learning tasks, depend crucially on the choice of hyperparameters. Specifically, the efficacy of such methods, and in particular their accuracy, stability, and generalization properties, strongly depends on the choice of hyperparameters. While bilevel optimization offers a principled framework for hyperparameter tuning, its nested optimization structure can be computationally demanding, especially in PDE-constrained contexts. In this paper, we propose an efficient strategy for hyperparameter optimization within the bilevel framework by employing a Gauss-Newton linearization of the inner optimization step. Our approach provides closed-form updates, eliminating the need for repeated costly PDE solves. As a result, each iteration of the outer loop reduces to a single linearized PDE solve, followed by explicit gradient-based hyperparameter updates. We demonstrate the effectiveness of the proposed method through Gaussian process models applied to nonlinear PDEs and to PDE inverse problems. Extensive numerical experiments highlight substantial improvements in accuracy and robustness compared to conventional random hyperparameter initialization. In particular, experiments with additive kernels and neural network-parameterized deep kernels demonstrate the method's scalability and effectiveness for high-dimensional hyperparameter optimization.", "published": "2025-10-07", "categories": ["stat.ML", "cs.LG", "physics.comp-ph"], "pdf_url": "http://arxiv.org/pdf/2510.05568v1", "primary_category": "stat.ML"}
{"id": "2510.05385", "title": "Physics-Informed Neural Networks with Fourier Features and Attention-Driven Decoding", "authors": ["Rohan Arni", "Carlos Blanco"], "summary": "Physics-Informed Neural Networks (PINNs) are a useful framework for approximating partial differential equation solutions using deep learning methods. In this paper, we propose a principled redesign of the PINNsformer, a Transformer-based PINN architecture. We present the Spectral PINNSformer (S-Pformer), a refinement of encoder-decoder PINNSformers that addresses two key issues; 1. the redundancy (i.e. increased parameter count) of the encoder, and 2. the mitigation of spectral bias. We find that the encoder is unnecessary for capturing spatiotemporal correlations when relying solely on self-attention, thereby reducing parameter count. Further, we integrate Fourier feature embeddings to explicitly mitigate spectral bias, enabling adaptive encoding of multiscale behaviors in the frequency domain. Our model outperforms encoder-decoder PINNSformer architectures across all benchmarks, achieving or outperforming MLP performance while reducing parameter count significantly.", "published": "2025-10-06", "categories": ["cs.LG", "physics.comp-ph"], "pdf_url": "http://arxiv.org/pdf/2510.05385v1", "primary_category": "cs.LG"}
{"id": "2510.05286", "title": "Computing frustration and near-monotonicity in deep neural networks", "authors": ["Joel Wendin", "Erik G. Larsson", "Claudio Altafini"], "summary": "For the signed graph associated to a deep neural network, one can compute the frustration level, i.e., test how close or distant the graph is to structural balance. For all the pretrained deep convolutional neural networks we consider, we find that the frustration is always less than expected from null models. From a statistical physics point of view, and in particular in reference to an Ising spin glass model, the reduced frustration indicates that the amount of disorder encoded in the network is less than in the null models. From a functional point of view, low frustration (i.e., proximity to structural balance) means that the function representing the network behaves near-monotonically, i.e., more similarly to a monotone function than in the null models. Evidence of near-monotonic behavior along the partial order determined by frustration is observed for all networks we consider. This confirms that the class of deep convolutional neural networks tends to have a more ordered behavior than expected from null models, and suggests a novel form of implicit regularization.", "published": "2025-10-06", "categories": ["cs.LG", "cond-mat.dis-nn", "stat.ML"], "pdf_url": "http://arxiv.org/pdf/2510.05286v1", "primary_category": "cs.LG"}
