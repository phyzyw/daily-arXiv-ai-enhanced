<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Semantic Chunking and the Entropy of Natural Language](https://arxiv.org/abs/2602.13194)
*Weishun Zhong, Doron Sivan, Tankut Can, Mikhail Katkov, Misha Tsodyks*

Main category: cs.CL

TL;DR: 本文提出了一种基于语义分块的统计模型，旨在从第一原理上解释自然语言的冗余程度和熵率。该模型通过自相似地将文本分割成语义连贯的块，并与现代LLM和开放数据集的实验结果一致。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对自然语言熵率的从第一原理的解释，而自然语言的冗余性暗示着可以通过上下文预测即将出现的词语，这种预测过程涉及抽象推理的层级结构。

Method: 研究人员构建了一个统计模型，该模型描述了将文本自相似地分割成语义连贯的块的过程，形成语义树的层级结构。他们利用大型语言模型（LLMs）对多个文本进行分块，并与开放数据集进行比较。

Result: 模型成功地量化地捕捉了不同语义层级上真实文本的结构，预测的熵率与印刷英语的估计熵率一致。研究还表明，自然语言的熵率并非固定，而是随着语料库的语义复杂性而系统性地增加。

Conclusion: 该研究提供了一种新的视角来理解自然语言的熵率，并揭示了熵率与语义复杂性之间的关系，为进一步研究自然语言的统计特性和理解提供了理论基础。

Abstract: The entropy rate of printed English is famously estimated to be about one bit per character, a benchmark that modern large language models (LLMs) have only recently approached. This entropy rate implies that English contains nearly 80 percent redundancy relative to the five bits per character expected for random text. We introduce a statistical model that attempts to capture the intricate multi-scale structure of natural language, providing a first-principles account of this redundancy level. Our model describes a procedure of self-similarly segmenting text into semantically coherent chunks down to the single-word level. The semantic structure of the text can then be hierarchically decomposed, allowing for analytical treatment. Numerical experiments with modern LLMs and open datasets suggest that our model quantitatively captures the structure of real texts at different levels of the semantic hierarchy. The entropy rate predicted by our model agrees with the estimated entropy rate of printed English. Moreover, our theory further reveals that the entropy rate of natural language is not fixed but should increase systematically with the semantic complexity of corpora, which are captured by the only free parameter in our model.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [2] [Computationally sufficient statistics for Ising models](https://arxiv.org/abs/2602.12449)
*Abhijith Jayakumar, Shreya Shukla, Marc Vuffray, Andrey Y. Lokhov, Sidhant Misra*

Main category: cs.LG

TL;DR: 本文研究了在仅观察有限统计信息的情况下，如何高效地学习伊辛模型参数的问题。研究表明，当已知模型ℓ1-宽度时，可以通过观察到与该宽度相关的统计信息来重建模型参数。


<details>
  <summary>Details</summary>
Motivation: 传统的伊辛模型学习方法需要完整的样本配置，但在实际应用中难以获得。本文旨在探索在观测能力受限的情况下，如何实现计算上可行的学习算法，填补现有研究的空白。

Method: 本文使用Interaction Screening Estimator (ISE) 的改进版本，通过最小化凸损失函数来估计每个变量相关的能量函数参数，并对模型ℓ1-宽度进行约束。

Result: 研究结果表明，在已知模型ℓ1-宽度γ的情况下，可以通过观察到O(γ)阶的统计信息来重建模型参数，并学习模型的结构和耦合。

Conclusion: 本文提供了一种在有限观测信息下高效学习伊辛模型参数的方法，为学习离散图模型提供了一种新的途径，并为物理系统中的参数估计提供了理论基础。

Abstract: Learning Gibbs distributions using only sufficient statistics has long been recognized as a computationally hard problem. On the other hand, computationally efficient algorithms for learning Gibbs distributions rely on access to full sample configurations generated from the model. For many systems of interest that arise in physical contexts, expecting a full sample to be observed is not practical, and hence it is important to look for computationally efficient methods that solve the learning problem with access to only a limited set of statistics. We examine the trade-offs between the power of computation and observation within this scenario, employing the Ising model as a paradigmatic example. We demonstrate that it is feasible to reconstruct the model parameters for a model with $\ell_1$ width $γ$ by observing statistics up to an order of $O(γ)$. This approach allows us to infer the model's structure and also learn its couplings and magnetic fields. We also discuss a setting where prior information about structure of the model is available and show that the learning problem can be solved efficiently with even more limited observational power.

</details>
