{"id": "2509.20191", "title": "Examining the robustness of Physics-Informed Neural Networks to noise for Inverse Problems", "authors": ["Aleksandra Jekic", "Afroditi Natsaridou", "Signe Riemer-Sørensen", "Helge Langseth", "Odd Erik Gundersen"], "summary": "Approximating solutions to partial differential equations (PDEs) is fundamental for the modeling of dynamical systems in science and engineering. Physics-informed neural networks (PINNs) are a recent machine learning-based approach, for which many properties and limitations remain unknown. PINNs are widely accepted as inferior to traditional methods for solving PDEs, such as the finite element method, both with regard to computation time and accuracy. However, PINNs are commonly claimed to show promise in solving inverse problems and handling noisy or incomplete data. We compare the performance of PINNs in solving inverse problems with that of a traditional approach using the finite element method combined with a numerical optimizer. The models are tested on a series of increasingly difficult fluid mechanics problems, with and without noise. We find that while PINNs may require less human effort and specialized knowledge, they are outperformed by the traditional approach. However, the difference appears to decrease with higher dimensions and more data. We identify common failures during training to be addressed if the performance of PINNs on noisy inverse problems is to become more competitive.", "published": "2025-09-24", "categories": ["physics.comp-ph", "cs.LG", "cs.NA", "math.NA"], "pdf_url": "http://arxiv.org/pdf/2509.20191v1", "primary_category": "physics.comp-ph"}
{"id": "2509.19929", "title": "Geometric Autoencoder Priors for Bayesian Inversion: Learn First Observe Later", "authors": ["Arnaud Vadeboncoeur", "Gregory Duthé", "Mark Girolami", "Eleni Chatzi"], "summary": "Uncertainty Quantification (UQ) is paramount for inference in engineering applications. A common inference task is to recover full-field information of physical systems from a small number of noisy observations, a usually highly ill-posed problem. Critically, engineering systems often have complicated and variable geometries prohibiting the use of standard Bayesian UQ. In this work, we introduce Geometric Autoencoders for Bayesian Inversion (GABI), a framework for learning geometry-aware generative models of physical responses that serve as highly informative geometry-conditioned priors for Bayesian inversion. Following a ''learn first, observe later'' paradigm, GABI distills information from large datasets of systems with varying geometries, without requiring knowledge of governing PDEs, boundary conditions, or observation processes, into a rich latent prior. At inference time, this prior is seamlessly combined with the likelihood of the specific observation process, yielding a geometry-adapted posterior distribution. Our proposed framework is architecture agnostic. A creative use of Approximate Bayesian Computation (ABC) sampling yields an efficient implementation that utilizes modern GPU hardware. We test our method on: steady-state heat over rectangular domains; Reynold-Averaged Navier-Stokes (RANS) flow around airfoils; Helmholtz resonance and source localization on 3D car bodies; RANS airflow over terrain. We find: the predictive accuracy to be comparable to deterministic supervised learning approaches in the restricted setting where supervised learning is applicable; UQ to be well calibrated and robust on challenging problems with complex geometries. The method provides a flexible geometry-aware train-once-use-anywhere foundation model which is independent of any particular observation process.", "published": "2025-09-24", "categories": ["stat.ML", "cs.LG", "physics.comp-ph", "physics.data-an"], "pdf_url": "http://arxiv.org/pdf/2509.19929v1", "primary_category": "stat.ML"}
{"id": "2509.19877", "title": "Advancing Universal Deep Learning for Electronic-Structure Hamiltonian Prediction of Materials", "authors": ["Shi Yin", "Zujian Dai", "Xinyang Pan", "Lixin He"], "summary": "Deep learning methods for electronic-structure Hamiltonian prediction has offered significant computational efficiency advantages over traditional DFT methods, yet the diversity of atomic types, structural patterns, and the high-dimensional complexity of Hamiltonians pose substantial challenges to the generalization performance. In this work, we contribute on both the methodology and dataset sides to advance universal deep learning paradigm for Hamiltonian prediction. On the method side, we propose NextHAM, a neural E(3)-symmetry and expressive correction method for efficient and generalizable materials electronic-structure Hamiltonian prediction. First, we introduce the zeroth-step Hamiltonians, which can be efficiently constructed by the initial charge density of DFT, as informative descriptors of neural regression model in the input level and initial estimates of the target Hamiltonian in the output level, so that the regression model directly predicts the correction terms to the target ground truths, thereby significantly simplifying the input-output mapping for learning. Second, we present a neural Transformer architecture with strict E(3)-Symmetry and high non-linear expressiveness for Hamiltonian prediction. Third, we propose a novel training objective to ensure the accuracy performance of Hamiltonians in both real space and reciprocal space, preventing error amplification and the occurrence of \"ghost states\" caused by the large condition number of the overlap matrix. On the dataset side, we curate a high-quality broad-coverage large benchmark, namely Materials-HAM-SOC, comprising 17,000 material structures spanning 68 elements from six rows of the periodic table and explicitly incorporating SOC effects. Experimental results on Materials-HAM-SOC demonstrate that NextHAM achieves excellent accuracy and efficiency in predicting Hamiltonians and band structures.", "published": "2025-09-24", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI", "physics.comp-ph"], "pdf_url": "http://arxiv.org/pdf/2509.19877v1", "primary_category": "cs.LG"}
{"id": "2509.19588", "title": "Discovery of Sustainable Refrigerants through Physics-Informed RL Fine-Tuning of Sequence Models", "authors": ["Adrien Goldszal", "Diego Calanzone", "Vincent Taboga", "Pierre-Luc Bacon"], "summary": "Most refrigerants currently used in air-conditioning systems, such as hydrofluorocarbons, are potent greenhouse gases and are being phased down. Large-scale molecular screening has been applied to the search for alternatives, but in practice only about 300 refrigerants are known, and only a few additional candidates have been suggested without experimental validation. This scarcity of reliable data limits the effectiveness of purely data-driven methods. We present Refgen, a generative pipeline that integrates machine learning with physics-grounded inductive biases. Alongside fine-tuning for valid molecular generation, Refgen incorporates predictive models for critical properties, equations of state, thermochemical polynomials, and full vapor compression cycle simulations. These models enable reinforcement learning fine-tuning under thermodynamic constraints, enforcing consistency and guiding discovery toward molecules that balance efficiency, safety, and environmental impact. By embedding physics into the learning process, Refgen leverages scarce data effectively and enables de novo refrigerant discovery beyond the known set of compounds.", "published": "2025-09-23", "categories": ["physics.chem-ph", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/2509.19588v1", "primary_category": "physics.chem-ph"}
{"id": "2509.17018", "title": "DeepEOSNet: Capturing the dependency on thermodynamic state in property prediction tasks", "authors": ["Jan Pavšek", "Alexander Mitsos", "Manuel Dahmen", "Tai Xuan Tan", "Jan G. Rittig"], "summary": "We propose a machine learning (ML) architecture to better capture the dependency of thermodynamic properties on the independent states. When predicting state-dependent thermodynamic properties, ML models need to account for both molecular structure and the thermodynamic state, described by independent variables, typically temperature, pressure, and composition. Modern molecular ML models typically include state information by adding it to molecular fingerprint vectors or by embedding explicit (semi-empirical) thermodynamic relations. Here, we propose to rather split the information processing on the molecular structure and the dependency on states into two separate network channels: a graph neural network and a multilayer perceptron, whose output is combined by a dot product. We refer to our approach as DeepEOSNet, as this idea is based on the DeepONet architecture [Lu et al. (2021), Nat. Mach. Intell.]: instead of operators, we learn state dependencies, with the possibility to predict equation of states (EOS). We investigate the predictive performance of DeepEOSNet by means of three case studies, which include the prediction of vapor pressure as a function of temperature, and mixture molar volume as a function of composition, temperature, and pressure. Our results show superior performance of DeepEOSNet for predicting vapor pressure and comparable performance for predicting mixture molar volume compared to state-of-research graph-based thermodynamic prediction models from our earlier works. In fact, we see large potential of DeepEOSNet in cases where data is sparse in the state domain and the output function is structurally similar across different molecules. The concept of DeepEOSNet can easily be transferred to other ML architectures in molecular context, and thus provides a viable option for property prediction.", "published": "2025-09-21", "categories": ["physics.chem-ph", "cs.LG"], "pdf_url": "http://arxiv.org/pdf/2509.17018v1", "primary_category": "physics.chem-ph"}
{"id": "2509.19715", "title": "SMILES-Inspired Transfer Learning for Quantum Operators in Generative Quantum Eigensolver", "authors": ["Zhi Yin", "Xiaoran Li", "Shengyu Zhang", "Xin Li", "Xiaojin Zhang"], "summary": "Given the inherent limitations of traditional Variational Quantum Eigensolver(VQE) algorithms, the integration of deep generative models into hybrid quantum-classical frameworks, specifically the Generative Quantum Eigensolver(GQE), represents a promising innovative approach. However, taking the Unitary Coupled Cluster with Singles and Doubles(UCCSD) ansatz which is widely used in quantum chemistry as an example, different molecular systems require constructions of distinct quantum operators. Considering the similarity of different molecules, the construction of quantum operators utilizing the similarity can reduce the computational cost significantly. Inspired by the SMILES representation method in computational chemistry, we developed a text-based representation approach for UCCSD quantum operators by leveraging the inherent representational similarities between different molecular systems. This framework explores text pattern similarities in quantum operators and employs text similarity metrics to establish a transfer learning framework. Our approach with a naive baseline setting demonstrates knowledge transfer between different molecular systems for ground-state energy calculations within the GQE paradigm. This discovery offers significant benefits for hybrid quantum-classical computation of molecular ground-state energies, substantially reducing computational resource requirements.", "published": "2025-09-24", "categories": ["physics.chem-ph", "cs.AI"], "pdf_url": "http://arxiv.org/pdf/2509.19715v1", "primary_category": "physics.chem-ph"}
{"id": "2509.17224", "title": "AI-based Methods for Simulating, Sampling, and Predicting Protein Ensembles", "authors": ["Bowen Jing", "Bonnie Berger", "Tommi Jaakkola"], "summary": "Advances in deep learning have opened an era of abundant and accurate predicted protein structures; however, similar progress in protein ensembles has remained elusive. This review highlights several recent research directions towards AI-based predictions of protein ensembles, including coarse-grained force fields, generative models, multiple sequence alignment perturbation methods, and modeling of ensemble descriptors. An emphasis is placed on realistic assessments of the technological maturity of current methods, the strengths and weaknesses of broad families of techniques, and promising machine learning frameworks at an early stage of development. We advocate for \"closing the loop\" between model training, simulation, and inference to overcome challenges in training data availability and to enable the next generation of models.", "published": "2025-09-21", "categories": ["q-bio.BM", "cs.LG", "physics.bio-ph"], "pdf_url": "http://arxiv.org/pdf/2509.17224v1", "primary_category": "q-bio.BM"}
{"id": "2509.19766", "title": "Dynamicasome: a molecular dynamics-guided and AI-driven pathogenicity prediction catalogue for all genetic mutations", "authors": ["Naeyma N Islam", "Mathew A Coban", "Jessica M Fuller", "Caleb Weber", "Rohit Chitale", "Benjamin Jussila", "Trisha J. Brock", "Cui Tao", "Thomas R Caulfield"], "summary": "Advances in genomic medicine accelerate the identi cation of mutations in disease-associated genes, but the pathogenicity of many mutations remains unknown, hindering their use in diagnostics and clinical decision-making. Predictive AI models are generated to combat this issue, but current tools display low accuracy when tested against functionally validated datasets. We show that integrating detailed conformational data extracted from molecular dynamics simulations (MDS) into advanced AI-based models increases their predictive power. We carry out an exhaustive mutational analysis of the disease gene PMM2 and subject structural models of each variant to MDS. AI models trained on this dataset outperform existing tools when predicting the known pathogenicity of mutations. Our best performing model, a neuronal networks model, also predicts the pathogenicity of several PMM2 mutations currently considered of unknown signi cance. We believe this model helps alleviate the burden of unknown variants in genomic medicine.", "published": "2025-09-23", "categories": ["q-bio.QM", "cs.AI", "physics.bio-ph", "q-bio.MN"], "pdf_url": "http://arxiv.org/pdf/2509.19766v1", "primary_category": "q-bio.QM"}
{"id": "2509.18758", "title": "Complexity of Activity Patterns in a Bio-Inspired Hopfield-Type Network in Different Topologies", "authors": ["Marco Cafiso", "Paolo Paradisi"], "summary": "Neural network models capable of storing memory have been extensively studied in computer science and computational neuroscience. The Hopfield network is a prototypical example of a model designed for associative, or content-addressable, memory and has been analyzed in many forms. Further, ideas and methods from complex network theory have been incorporated into artificial neural networks and learning, emphasizing their structural properties. Nevertheless, the temporal dynamics also play a vital role in biological neural networks, whose temporal structure is a crucial feature to examine. Biological neural networks display complex intermittency and, thus, can be studied through the lens of the temporal complexity (TC) theory. The TC approach look at the metastability of self-organized states, characterized by a power-law decay in the inter-event time distribution and in the total activity distribution or a scaling behavior in the corresponding event-driven diffusion processes. In this study, we present a temporal complexity (TC) analysis of a biologically-inspired Hopfield-type neural network model. We conducted a comparative assessment between scale-free and random network topologies, with particular emphasis on their global activation patterns. Our parametric analysis revealed comparable dynamical behaviors across both neural network architectures. Furthermore, our investigation into temporal complexity characteristics uncovered that seemingly distinct dynamical patterns exhibit similar temporal complexity behaviors. In particular, similar power-law decay in the activity distribution and similar complexity levels are observed in both topologies, but with a much reduced noise in the scale-free topology. Notably, most of the complex dynamical profiles were consistently observed in scale-free network configurations, thus confirming the crucial role of hubs in neural network dynamics.", "published": "2025-09-23", "categories": ["q-bio.NC", "cs.AI", "nlin.AO", "physics.bio-ph"], "pdf_url": "http://arxiv.org/pdf/2509.18758v1", "primary_category": "q-bio.NC"}
{"id": "2509.19586", "title": "A Foundation Chemical Language Model for Comprehensive Fragment-Based Drug Discovery", "authors": ["Alexander Ho", "Sukyeong Lee", "Francis T. F. Tsai"], "summary": "We introduce FragAtlas-62M, a specialized foundation model trained on the largest fragment dataset to date. Built on the complete ZINC-22 fragment subset comprising over 62 million molecules, it achieves unprecedented coverage of fragment chemical space. Our GPT-2 based model (42.7M parameters) generates 99.90% chemically valid fragments. Validation across 12 descriptors and three fingerprint methods shows generated fragments closely match the training distribution (all effect sizes < 0.4). The model retains 53.6% of known ZINC fragments while producing 22% novel structures with practical relevance. We release FragAtlas-62M with training code, preprocessed data, documentation, and model weights to accelerate adoption.", "published": "2025-09-23", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "pdf_url": "http://arxiv.org/pdf/2509.19586v1", "primary_category": "cs.LG"}
