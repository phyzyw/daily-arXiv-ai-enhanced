import os
import logging
import json
import re
import time
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
import arxiv
from arxiv import SortCriterion, SortOrder

class ArxivAPISpider:
    def __init__(self, categories=None, days=3):
        """åˆå§‹åŒ– arXiv çˆ¬è™«ï¼Œæœç´¢æœ€è¿‘å‡ å¤©çš„æ–‡ç« """
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)

        # è®¾ç½®ç±»åˆ«
        if categories is None:
            categories = os.environ.get("CATEGORIES", "")
            self.categories = [cat.strip() for cat in categories.split(",")] if categories else []
        else:
            self.categories = categories

        if not self.categories:
            raise ValueError("è‡³å°‘éœ€è¦æŒ‡å®šä¸€ä¸ªç±»åˆ«")

        # è®¾ç½®æœç´¢å¤©æ•°
        self.days = days
        self.end_date = datetime.now(ZoneInfo("UTC"))
        self.start_date = self.end_date - timedelta(days=days)
        
        self.logger.info(f"æœç´¢æ—¶é—´èŒƒå›´: {self.start_date.strftime('%Y-%m-%d')} åˆ° {self.end_date.strftime('%Y-%m-%d')}")

        # ç”Ÿæˆäº¤å‰å­¦ç§‘ç»„åˆ
        cross_categories = ["cs.LG", "cs.AI"]
        self.target_category_pairs = [
            (cat, cross_cat) for cat in self.categories for cross_cat in cross_categories
        ]

        self.logger.info(f"ç›®æ ‡ç±»åˆ«å¯¹: {self.target_category_pairs}")

    def construct_query(self):
        """æ„é€ æ­£ç¡®çš„æŸ¥è¯¢å­—ç¬¦ä¸² - ä¿®å¤é€»è¾‘é”™è¯¯"""
        # æ­£ç¡®çš„æŸ¥è¯¢é€»è¾‘: (cs.CV AND (cs.LG OR cs.AI))
        base_queries = []
        for target_cat in self.categories:
            # å¯¹äºæ¯ä¸ªä¸»ç±»åˆ«ï¼Œæ„é€  (cs.LG OR cs.AI) çš„å­æŸ¥è¯¢
            cross_query = " OR ".join([f"cat:{cross_cat}" for cross_cat in ["cs.LG", "cs.AI"]])
            base_queries.append(f"cat:{target_cat} AND ({cross_query})")
        
        return " OR ".join(base_queries)

    def search_articles_with_pagination(self, max_results=300):
        """å¸¦åˆ†é¡µæ§åˆ¶çš„æœç´¢ï¼Œé¿å…APIé™åˆ¶"""
        query = self.construct_query()
        self.logger.info(f"æ‰§è¡ŒæŸ¥è¯¢: {query}")

        all_results = []
        max_retries = 3
        batch_size = 100  # æ¯æ¬¡è·å–100æ¡
        
        try:
            client = arxiv.Client()
            
            # åˆ†æ‰¹è·å–ç»“æœï¼Œé¿å…åˆ†é¡µé”™è¯¯
            for start in range(0, max_results, batch_size):
                remaining = max_results - start
                current_batch_size = min(batch_size, remaining)
                
                if current_batch_size <= 0:
                    break
                
                self.logger.info(f"è·å–ç¬¬ {start} åˆ° {start + current_batch_size - 1} æ¡ç»“æœ")
                
                search = arxiv.Search(
                    query=query,
                    max_results=current_batch_size,
                    start=start,
                    sort_by=SortCriterion.SubmittedDate,
                    sort_order=SortOrder.Descending
                )
                
                # å¸¦é‡è¯•æœºåˆ¶çš„è·å–
                for attempt in range(max_retries):
                    try:
                        batch_results = list(client.results(search))
                        if not batch_results:
                            self.logger.info(f"ç¬¬ {start} æ‰¹æ²¡æœ‰æ›´å¤šç»“æœï¼Œåœæ­¢æœç´¢")
                            return all_results
                        
                        for result in batch_results:
                            result_dict = {
                                'id': result.entry_id,
                                'title': result.title,
                                'authors': [{'name': author.name} for author in result.authors],
                                'summary': result.summary,
                                'published': result.published.isoformat(),
                                'categories': [str(cat) for cat in result.categories],
                                'pdf_url': result.pdf_url,
                                'primary_category': str(result.primary_category) if result.primary_category else ""
                            }
                            all_results.append(result_dict)
                        
                        self.logger.info(f"æˆåŠŸè·å–ç¬¬ {start} æ‰¹çš„ {len(batch_results)} æ¡ç»“æœ")
                        break
                        
                    except Exception as e:
                        if attempt < max_retries - 1:
                            wait_time = 2 ** attempt
                            self.logger.warning(f"ç¬¬ {start} æ‰¹è·å–å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
                            self.logger.info(f"{wait_time}ç§’åé‡è¯•...")
                            time.sleep(wait_time)
                        else:
                            self.logger.error(f"ç¬¬ {start} æ‰¹æ‰€æœ‰é‡è¯•å‡å¤±è´¥: {str(e)}")
                            return all_results
                
                # æ£€æŸ¥æ˜¯å¦å·²ç»è·å–åˆ°è¶³å¤Ÿæ—§çš„ç»“æœ
                if len(all_results) > 0:
                    oldest_result_date = datetime.fromisoformat(all_results[-1]['published'].replace('Z', '+00:00'))
                    if oldest_result_date < self.start_date:
                        self.logger.info(f"å·²è·å–åˆ°è¶³å¤Ÿæ—§çš„ç»“æœ ({oldest_result_date.strftime('%Y-%m-%d')})ï¼Œåœæ­¢æœç´¢")
                        break
                
                # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                time.sleep(1)
                
            return all_results

        except Exception as e:
            self.logger.error(f"æœç´¢æ–‡ç« æ—¶å‡ºé”™: {str(e)}")
            return all_results

    def filter_articles_by_date_range(self, results):
        """æŒ‰æ—¥æœŸèŒƒå›´ç­›é€‰ç»“æœ"""
        filtered_results = []
        
        for result in results:
            # æå–å‘å¸ƒæ—¶é—´
            published_str = result.get('published', '')
            try:
                published_date = datetime.fromisoformat(published_str.replace('Z', '+00:00'))
            except (ValueError, TypeError):
                continue
                
            # æ£€æŸ¥æ˜¯å¦åœ¨ç›®æ ‡æ—¥æœŸèŒƒå›´å†…
            if self.start_date <= published_date <= self.end_date:
                # éªŒè¯ç±»åˆ«
                categories = result.get('categories', [])
                found_pair = False
                for target_cat, cross_cat in self.target_category_pairs:
                    if target_cat in categories and cross_cat in categories:
                        found_pair = True
                        break
                
                if found_pair:
                    # æå–arXiv ID
                    paper_id = re.sub(r'v\d+$', '', result.get('id', '').split('/')[-1])
                    
                    filtered_results.append({
                        "id": paper_id,
                        "title": result.get('title', '').replace('\n', ''),
                        "authors": [author.get('name', '') for author in result.get('authors', [])],
                        "summary": result.get('summary', '').replace('\n', ' '),
                        "published": published_date.strftime("%Y-%m-%d"),
                        "categories": categories,
                        "pdf_url": result.get('pdf_url', ''),
                        "primary_category": categories[0] if categories else ""
                    })
        
        return filtered_results

    def group_results_by_date(self, results):
        """æŒ‰æ—¥æœŸåˆ†ç»„ç»“æœ"""
        grouped = {}
        for result in results:
            date = result['published']
            if date not in grouped:
                grouped[date] = []
            grouped[date].append(result)
        return grouped

    def run(self, output_file=None):
        """è¿è¡Œçˆ¬è™«"""
        self.logger.info(f"å¼€å§‹æœç´¢æœ€è¿‘ {self.days} å¤©çš„æ–‡ç« ...")

        try:
            results = self.search_articles_with_pagination(max_results=500)
            self.logger.info(f"æ€»å…±è·å–åˆ° {len(results)} æ¡åŸå§‹ç»“æœ")
            
            filtered_results = self.filter_articles_by_date_range(results)
            
            # æŒ‰æ—¥æœŸåˆ†ç»„
            grouped_results = self.group_results_by_date(filtered_results)
            
            self.logger.info(f"æ‰¾åˆ° {len(filtered_results)} ç¯‡åŒ¹é…çš„æ–‡ç« ")
            
            # æŒ‰æ—¥æœŸæ‰“å°ç»Ÿè®¡ä¿¡æ¯
            for date, articles in grouped_results.items():
                self.logger.info(f"æ—¥æœŸ {date}: {len(articles)} ç¯‡æ–‡ç« ")
            
            for result in filtered_results:
                self.logger.info(f"æ‰¾åˆ°æ–‡ç« : {result['id']}, æ—¥æœŸ: {result['published']}, æ ‡é¢˜: {result['title'][:50]}...")

            if output_file:
                os.makedirs(os.path.dirname(output_file), exist_ok=True)
                with open(output_file, 'w', encoding='utf-8') as f:
                    for result in filtered_results:
                        json.dump(result, f, ensure_ascii=False)
                        f.write('\n')
                self.logger.info(f"ç»“æœå·²ä¿å­˜åˆ° {output_file}")

            return filtered_results

        except Exception as e:
            self.logger.error(f"æœç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            return []

if __name__ == "__main__":
    # ä»ç¯å¢ƒå˜é‡è·å–ç±»åˆ«ï¼Œæˆ–ä½¿ç”¨é»˜è®¤å€¼
    categories = os.environ.get("CATEGORIES", "cs.CV,cs.CL")
    
    # ä»ç¯å¢ƒå˜é‡è·å–å¤©æ•°ï¼Œé»˜è®¤ä¸º3å¤©
    days = int(os.environ.get("DAYS", "5"))
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶åï¼ŒåŒ…å«æ—¥æœŸèŒƒå›´
    end_date = datetime.now(ZoneInfo("UTC"))
    start_date = end_date - timedelta(days=days)
    date_range_str = f"{start_date.strftime('%Y%m%d')}-{end_date.strftime('%Y%m%d')}"
    output_file = os.environ.get("OUTPUT_FILE", f"data/last_{days}_days_{date_range_str}.jsonl")

    # åˆ›å»ºå¹¶è¿è¡Œçˆ¬è™«
    spider = ArxivAPISpider(
        categories=categories.split(","),
        days=days
    )
    
    results = spider.run(output_file=output_file)

    # æ‰“å°ç»“æœæ‘˜è¦
    print(f"\næ‰¾åˆ° {len(results)} ç¯‡æ–‡ç«  (æœ€è¿‘ {days} å¤©):")
    
    # æŒ‰æ—¥æœŸåˆ†ç»„æ˜¾ç¤º
    grouped_results = {}
    for result in results:
        date = result['published']
        if date not in grouped_results:
            grouped_results[date] = []
        grouped_results[date].append(result)
    
    for date, articles in sorted(grouped_results.items(), reverse=True):
        print(f"\nğŸ“… {date} ({len(articles)} ç¯‡):")
        for result in articles:
            print(f"  - {result['id']}: {result['title'][:60]}...")
            print(f"    ç±»åˆ«: {result['categories']}")
    
    if not results:
        print("æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡ç« ã€‚")
        print("å»ºè®®:")
        print("1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("2. ç­‰å¾…å‡ åˆ†é’Ÿåé‡è¯•ï¼ˆarXiv APIå¯èƒ½æœ‰é€Ÿç‡é™åˆ¶ï¼‰")
        print("3. æ‰‹åŠ¨éªŒè¯æŸ¥è¯¢: https://arxiv.org/search/?query=cat%3Acs.CL+AND+(cat%3Acs.LG+OR+cat%3Acs.AI)&searchtype=all&abstracts=show&order=-submitted_date&size=50")
